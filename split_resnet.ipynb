{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    Load RESNET model and split it\\n        1. layer by layer\\n        2. [TODO] vertically \\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    Load RESNET model and split it\n",
    "        1. layer by layer\n",
    "        2. [TODO] vertically \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from source.core.engine import MoP\n",
    "import source.core.run_partition as run_p\n",
    "from os import environ\n",
    "from source.utils.dataset import *\n",
    "from source.utils.misc import *\n",
    "from split_network import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "from source.models import resnet\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from source.utils import io\n",
    "from source.utils import testers\n",
    "from source.core import engine\n",
    "import json\n",
    "import itertools\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device :  \n",
      "model :  resnet18\n",
      "data_code :  cifar10\n",
      "num_classes :  10\n",
      "model_file :  test.pt\n",
      "epochs :  0\n",
      "batch_size :  128\n",
      "optimizer :  sgd\n",
      "lr_scheduler :  default\n",
      "learning_rate :  0.01\n",
      "seed :  1234\n",
      "sparsity_type :  kernel\n",
      "prune_ratio :  1\n",
      "admm :  True\n",
      "admm_epochs :  3\n",
      "rho :  0.0001\n",
      "multi_rho :  True\n",
      "retrain_bs :  128\n",
      "retrain_lr :  0.005\n",
      "retrain_ep :  50\n",
      "retrain_opt :  default\n",
      "xentropy_weight :  1.0\n",
      "warmup :  False\n",
      "warmup_lr :  0.001\n",
      "warmup_epochs :  10\n",
      "mix_up :  True\n",
      "alpha :  0.3\n",
      "smooth :  False\n",
      "smooth_eps :  0\n",
      "save_last_model_only :  False\n",
      "num_partition :  1\n",
      "layer_type :  regular\n",
      "bn_type :  masked\n",
      "par_first_layer :  False\n",
      "comm_outsize :  False\n",
      "lambda_comm :  0\n",
      "lambda_comp :  0\n",
      "distill_model :  \n",
      "distill_loss :  kl\n",
      "distill_temp :  30\n",
      "distill_alpha :  1\n"
     ]
    }
   ],
   "source": [
    "# setup config\n",
    "dataset='cifar10'\n",
    "environ[\"config\"] = f\"config/{dataset}.yaml\"\n",
    "\n",
    "configs = run_p.main()\n",
    "\n",
    "configs[\"device\"] = \"cpu\"\n",
    "configs['load_model'] = \"cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001.pt\"\n",
    "configs[\"num_partition\"] = '4' #'resnet18-v2.yaml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (linear): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# load data and load or train model\n",
    "model = get_model_from_code(configs).to(configs['device']) # grabs model architecture from ./source/models/escnet.py\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "   1st section of resnet model \n",
    "'''\n",
    "class ResnetBlockOne(nn.Module):\n",
    "    def __init__(self, block, num_blocks, conv_layer, bn_layer, num_classes=10, num_filters=512, bn_partition=[1]*9):\n",
    "        super(ResnetBlockOne, self).__init__()\n",
    "\n",
    "        self.in_planes = 64\n",
    "        self.conv_layer = conv_layer\n",
    "        self.bn_layer = bn_layer\n",
    "        self.shrink = num_filters/512\n",
    "        self.bn_partition = bn_partition\n",
    "        \n",
    "        self.conv1 = conv_layer(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        num_bn = self.bn_partition.pop(0)\n",
    "        self.bn1 = bn_layer(64) if num_bn==1 else bn_layer(64, num_bn)\n",
    "\n",
    "        self.layer1 = self._make_layer(block, int(64*self.shrink),  num_blocks[0], stride=1)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        # TODO: find better way to implement this method using inheretence and getting from ResNet class\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, self.conv_layer, self.bn_layer, stride, self.bn_partition.pop(0)))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # override the the foward pass to only include the first modules \n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "'''\n",
    "   2nd section of resnet model \n",
    "'''\n",
    "class ResnetBlockTwo(nn.Module):\n",
    "    def __init__(self, block, num_blocks, conv_layer, bn_layer, num_classes=10, num_filters=512, bn_partition=[1]*9):\n",
    "        super(ResnetBlockTwo, self).__init__()\n",
    "\n",
    "        self.in_planes = 64\n",
    "        self.conv_layer = conv_layer\n",
    "        self.bn_layer = bn_layer\n",
    "        self.shrink = num_filters/512\n",
    "        self.bn_partition = bn_partition\n",
    "\n",
    "        self.layer2 = self._make_layer(block, int(128*self.shrink), num_blocks[1], stride=2)\n",
    "    \n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        # TODO: find better way to implement this method using inheretence and getting from ResNet class\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, self.conv_layer, self.bn_layer, stride, self.bn_partition.pop(0)))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer2(x)\n",
    "        return out\n",
    "\n",
    "'''\n",
    "    3rd section of resenet model \n",
    "'''\n",
    "class ResnetBlockThree(nn.Module):\n",
    "    def __init__(self, block, num_blocks, conv_layer, bn_layer, num_classes=10, num_filters=512, bn_partition=[1]*9):\n",
    "        super(ResnetBlockThree, self).__init__()\n",
    "\n",
    "        self.in_planes = 128\n",
    "        self.conv_layer = conv_layer\n",
    "        self.bn_layer = bn_layer\n",
    "        self.shrink = num_filters/512\n",
    "        self.bn_partition = bn_partition\n",
    "\n",
    "        self.layer3 = self._make_layer(block, int(256*self.shrink), num_blocks[2], stride=2)\n",
    "    \n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        # TODO: find better way to implement this method using inheretence and getting from ResNet class\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, self.conv_layer, self.bn_layer, stride, self.bn_partition.pop(0)))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer3(x)\n",
    "        return out\n",
    "\n",
    "'''\n",
    "    4-th section of resenet model \n",
    "'''\n",
    "class ResnetBlockFour(nn.Module):\n",
    "    def __init__(self, block, num_blocks, conv_layer, bn_layer, num_classes=10, num_filters=512, bn_partition=[1]*9):\n",
    "        super(ResnetBlockFour, self).__init__()\n",
    "\n",
    "        self.in_planes = 256\n",
    "        self.conv_layer = conv_layer\n",
    "        self.bn_layer = bn_layer\n",
    "        self.shrink = num_filters/512\n",
    "        self.bn_partition = bn_partition\n",
    "\n",
    "        self.layer4 = self._make_layer(block, num_filters, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(num_filters*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        # TODO: find better way to implement this method using inheretence and getting from ResNet class\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, self.conv_layer, self.bn_layer, stride, self.bn_partition.pop(0)))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer4(x)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResnetBlockFour(\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (linear): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# get framework for each layer of resnet\n",
    "\n",
    "# inputs can be found from looking at ./source/util/misc/get_model_from_code\n",
    "num_classes=configs['num_classes']\n",
    "bn_layers = get_bn_layers('regular') # basic block layer\n",
    "conv_layers = get_layers(configs['layer_type'])\n",
    "\n",
    "# resnet18 inputs from resnet.py\n",
    "layer_1 =  ResnetBlockOne(resnet.BasicBlock, [2,2,2,2],conv_layers, bn_layers, num_classes=num_classes) # also includes bn1 and conv1 \n",
    "layer_2 =  ResnetBlockTwo(resnet.BasicBlock, [2,2,2,2],conv_layers, bn_layers, num_classes=num_classes) # also includes bn1 and conv1 \n",
    "layer_3 =  ResnetBlockThree(resnet.BasicBlock, [2,2,2,2],conv_layers, bn_layers, num_classes=num_classes) # also includes bn1 and conv1 \n",
    "layer_4 =  ResnetBlockFour(resnet.BasicBlock, [2,2,2,2],conv_layers, bn_layers, num_classes=num_classes) # also includes bn1 and conv1 \n",
    "\n",
    "print(layer_4)\n",
    "#block2 = |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not found:  layer2.0.conv1.weight\n",
      "not found:  layer2.0.conv2.weight\n",
      "not found:  layer2.0.bn1.weight\n",
      "not found:  layer2.0.bn1.bias\n",
      "not found:  layer2.0.bn1.running_mean\n",
      "not found:  layer2.0.bn1.running_var\n",
      "not found:  layer2.0.bn1.num_batches_tracked\n",
      "not found:  layer2.0.bn2.weight\n",
      "not found:  layer2.0.bn2.bias\n",
      "not found:  layer2.0.bn2.running_mean\n",
      "not found:  layer2.0.bn2.running_var\n",
      "not found:  layer2.0.bn2.num_batches_tracked\n",
      "not found:  layer2.0.shortcut.0.weight\n",
      "not found:  layer2.0.shortcut.1.weight\n",
      "not found:  layer2.0.shortcut.1.bias\n",
      "not found:  layer2.0.shortcut.1.running_mean\n",
      "not found:  layer2.0.shortcut.1.running_var\n",
      "not found:  layer2.0.shortcut.1.num_batches_tracked\n",
      "not found:  layer2.1.conv1.weight\n",
      "not found:  layer2.1.conv2.weight\n",
      "not found:  layer2.1.bn1.weight\n",
      "not found:  layer2.1.bn1.bias\n",
      "not found:  layer2.1.bn1.running_mean\n",
      "not found:  layer2.1.bn1.running_var\n",
      "not found:  layer2.1.bn1.num_batches_tracked\n",
      "not found:  layer2.1.bn2.weight\n",
      "not found:  layer2.1.bn2.bias\n",
      "not found:  layer2.1.bn2.running_mean\n",
      "not found:  layer2.1.bn2.running_var\n",
      "not found:  layer2.1.bn2.num_batches_tracked\n",
      "not found:  layer3.0.conv1.weight\n",
      "not found:  layer3.0.conv2.weight\n",
      "not found:  layer3.0.bn1.weight\n",
      "not found:  layer3.0.bn1.bias\n",
      "not found:  layer3.0.bn1.running_mean\n",
      "not found:  layer3.0.bn1.running_var\n",
      "not found:  layer3.0.bn1.num_batches_tracked\n",
      "not found:  layer3.0.bn2.weight\n",
      "not found:  layer3.0.bn2.bias\n",
      "not found:  layer3.0.bn2.running_mean\n",
      "not found:  layer3.0.bn2.running_var\n",
      "not found:  layer3.0.bn2.num_batches_tracked\n",
      "not found:  layer3.0.shortcut.0.weight\n",
      "not found:  layer3.0.shortcut.1.weight\n",
      "not found:  layer3.0.shortcut.1.bias\n",
      "not found:  layer3.0.shortcut.1.running_mean\n",
      "not found:  layer3.0.shortcut.1.running_var\n",
      "not found:  layer3.0.shortcut.1.num_batches_tracked\n",
      "not found:  layer3.1.conv1.weight\n",
      "not found:  layer3.1.conv2.weight\n",
      "not found:  layer3.1.bn1.weight\n",
      "not found:  layer3.1.bn1.bias\n",
      "not found:  layer3.1.bn1.running_mean\n",
      "not found:  layer3.1.bn1.running_var\n",
      "not found:  layer3.1.bn1.num_batches_tracked\n",
      "not found:  layer3.1.bn2.weight\n",
      "not found:  layer3.1.bn2.bias\n",
      "not found:  layer3.1.bn2.running_mean\n",
      "not found:  layer3.1.bn2.running_var\n",
      "not found:  layer3.1.bn2.num_batches_tracked\n",
      "not found:  layer4.0.conv1.weight\n",
      "not found:  layer4.0.conv2.weight\n",
      "not found:  layer4.0.bn1.weight\n",
      "not found:  layer4.0.bn1.bias\n",
      "not found:  layer4.0.bn1.running_mean\n",
      "not found:  layer4.0.bn1.running_var\n",
      "not found:  layer4.0.bn1.num_batches_tracked\n",
      "not found:  layer4.0.bn2.weight\n",
      "not found:  layer4.0.bn2.bias\n",
      "not found:  layer4.0.bn2.running_mean\n",
      "not found:  layer4.0.bn2.running_var\n",
      "not found:  layer4.0.bn2.num_batches_tracked\n",
      "not found:  layer4.0.shortcut.0.weight\n",
      "not found:  layer4.0.shortcut.1.weight\n",
      "not found:  layer4.0.shortcut.1.bias\n",
      "not found:  layer4.0.shortcut.1.running_mean\n",
      "not found:  layer4.0.shortcut.1.running_var\n",
      "not found:  layer4.0.shortcut.1.num_batches_tracked\n",
      "not found:  layer4.1.conv1.weight\n",
      "not found:  layer4.1.conv2.weight\n",
      "not found:  layer4.1.bn1.weight\n",
      "not found:  layer4.1.bn1.bias\n",
      "not found:  layer4.1.bn1.running_mean\n",
      "not found:  layer4.1.bn1.running_var\n",
      "not found:  layer4.1.bn1.num_batches_tracked\n",
      "not found:  layer4.1.bn2.weight\n",
      "not found:  layer4.1.bn2.bias\n",
      "not found:  layer4.1.bn2.running_mean\n",
      "not found:  layer4.1.bn2.running_var\n",
      "not found:  layer4.1.bn2.num_batches_tracked\n",
      "not found:  linear.weight\n",
      "not found:  linear.bias\n",
      "not found:  conv1.weight\n",
      "not found:  bn1.weight\n",
      "not found:  bn1.bias\n",
      "not found:  bn1.running_mean\n",
      "not found:  bn1.running_var\n",
      "not found:  bn1.num_batches_tracked\n",
      "not found:  layer1.0.conv1.weight\n",
      "not found:  layer1.0.conv2.weight\n",
      "not found:  layer1.0.bn1.weight\n",
      "not found:  layer1.0.bn1.bias\n",
      "not found:  layer1.0.bn1.running_mean\n",
      "not found:  layer1.0.bn1.running_var\n",
      "not found:  layer1.0.bn1.num_batches_tracked\n",
      "not found:  layer1.0.bn2.weight\n",
      "not found:  layer1.0.bn2.bias\n",
      "not found:  layer1.0.bn2.running_mean\n",
      "not found:  layer1.0.bn2.running_var\n",
      "not found:  layer1.0.bn2.num_batches_tracked\n",
      "not found:  layer1.1.conv1.weight\n",
      "not found:  layer1.1.conv2.weight\n",
      "not found:  layer1.1.bn1.weight\n",
      "not found:  layer1.1.bn1.bias\n",
      "not found:  layer1.1.bn1.running_mean\n",
      "not found:  layer1.1.bn1.running_var\n",
      "not found:  layer1.1.bn1.num_batches_tracked\n",
      "not found:  layer1.1.bn2.weight\n",
      "not found:  layer1.1.bn2.bias\n",
      "not found:  layer1.1.bn2.running_mean\n",
      "not found:  layer1.1.bn2.running_var\n",
      "not found:  layer1.1.bn2.num_batches_tracked\n",
      "not found:  layer3.0.conv1.weight\n",
      "not found:  layer3.0.conv2.weight\n",
      "not found:  layer3.0.bn1.weight\n",
      "not found:  layer3.0.bn1.bias\n",
      "not found:  layer3.0.bn1.running_mean\n",
      "not found:  layer3.0.bn1.running_var\n",
      "not found:  layer3.0.bn1.num_batches_tracked\n",
      "not found:  layer3.0.bn2.weight\n",
      "not found:  layer3.0.bn2.bias\n",
      "not found:  layer3.0.bn2.running_mean\n",
      "not found:  layer3.0.bn2.running_var\n",
      "not found:  layer3.0.bn2.num_batches_tracked\n",
      "not found:  layer3.0.shortcut.0.weight\n",
      "not found:  layer3.0.shortcut.1.weight\n",
      "not found:  layer3.0.shortcut.1.bias\n",
      "not found:  layer3.0.shortcut.1.running_mean\n",
      "not found:  layer3.0.shortcut.1.running_var\n",
      "not found:  layer3.0.shortcut.1.num_batches_tracked\n",
      "not found:  layer3.1.conv1.weight\n",
      "not found:  layer3.1.conv2.weight\n",
      "not found:  layer3.1.bn1.weight\n",
      "not found:  layer3.1.bn1.bias\n",
      "not found:  layer3.1.bn1.running_mean\n",
      "not found:  layer3.1.bn1.running_var\n",
      "not found:  layer3.1.bn1.num_batches_tracked\n",
      "not found:  layer3.1.bn2.weight\n",
      "not found:  layer3.1.bn2.bias\n",
      "not found:  layer3.1.bn2.running_mean\n",
      "not found:  layer3.1.bn2.running_var\n",
      "not found:  layer3.1.bn2.num_batches_tracked\n",
      "not found:  layer4.0.conv1.weight\n",
      "not found:  layer4.0.conv2.weight\n",
      "not found:  layer4.0.bn1.weight\n",
      "not found:  layer4.0.bn1.bias\n",
      "not found:  layer4.0.bn1.running_mean\n",
      "not found:  layer4.0.bn1.running_var\n",
      "not found:  layer4.0.bn1.num_batches_tracked\n",
      "not found:  layer4.0.bn2.weight\n",
      "not found:  layer4.0.bn2.bias\n",
      "not found:  layer4.0.bn2.running_mean\n",
      "not found:  layer4.0.bn2.running_var\n",
      "not found:  layer4.0.bn2.num_batches_tracked\n",
      "not found:  layer4.0.shortcut.0.weight\n",
      "not found:  layer4.0.shortcut.1.weight\n",
      "not found:  layer4.0.shortcut.1.bias\n",
      "not found:  layer4.0.shortcut.1.running_mean\n",
      "not found:  layer4.0.shortcut.1.running_var\n",
      "not found:  layer4.0.shortcut.1.num_batches_tracked\n",
      "not found:  layer4.1.conv1.weight\n",
      "not found:  layer4.1.conv2.weight\n",
      "not found:  layer4.1.bn1.weight\n",
      "not found:  layer4.1.bn1.bias\n",
      "not found:  layer4.1.bn1.running_mean\n",
      "not found:  layer4.1.bn1.running_var\n",
      "not found:  layer4.1.bn1.num_batches_tracked\n",
      "not found:  layer4.1.bn2.weight\n",
      "not found:  layer4.1.bn2.bias\n",
      "not found:  layer4.1.bn2.running_mean\n",
      "not found:  layer4.1.bn2.running_var\n",
      "not found:  layer4.1.bn2.num_batches_tracked\n",
      "not found:  linear.weight\n",
      "not found:  linear.bias\n",
      "not found:  conv1.weight\n",
      "not found:  bn1.weight\n",
      "not found:  bn1.bias\n",
      "not found:  bn1.running_mean\n",
      "not found:  bn1.running_var\n",
      "not found:  bn1.num_batches_tracked\n",
      "not found:  layer1.0.conv1.weight\n",
      "not found:  layer1.0.conv2.weight\n",
      "not found:  layer1.0.bn1.weight\n",
      "not found:  layer1.0.bn1.bias\n",
      "not found:  layer1.0.bn1.running_mean\n",
      "not found:  layer1.0.bn1.running_var\n",
      "not found:  layer1.0.bn1.num_batches_tracked\n",
      "not found:  layer1.0.bn2.weight\n",
      "not found:  layer1.0.bn2.bias\n",
      "not found:  layer1.0.bn2.running_mean\n",
      "not found:  layer1.0.bn2.running_var\n",
      "not found:  layer1.0.bn2.num_batches_tracked\n",
      "not found:  layer1.1.conv1.weight\n",
      "not found:  layer1.1.conv2.weight\n",
      "not found:  layer1.1.bn1.weight\n",
      "not found:  layer1.1.bn1.bias\n",
      "not found:  layer1.1.bn1.running_mean\n",
      "not found:  layer1.1.bn1.running_var\n",
      "not found:  layer1.1.bn1.num_batches_tracked\n",
      "not found:  layer1.1.bn2.weight\n",
      "not found:  layer1.1.bn2.bias\n",
      "not found:  layer1.1.bn2.running_mean\n",
      "not found:  layer1.1.bn2.running_var\n",
      "not found:  layer1.1.bn2.num_batches_tracked\n",
      "not found:  layer2.0.conv1.weight\n",
      "not found:  layer2.0.conv2.weight\n",
      "not found:  layer2.0.bn1.weight\n",
      "not found:  layer2.0.bn1.bias\n",
      "not found:  layer2.0.bn1.running_mean\n",
      "not found:  layer2.0.bn1.running_var\n",
      "not found:  layer2.0.bn1.num_batches_tracked\n",
      "not found:  layer2.0.bn2.weight\n",
      "not found:  layer2.0.bn2.bias\n",
      "not found:  layer2.0.bn2.running_mean\n",
      "not found:  layer2.0.bn2.running_var\n",
      "not found:  layer2.0.bn2.num_batches_tracked\n",
      "not found:  layer2.0.shortcut.0.weight\n",
      "not found:  layer2.0.shortcut.1.weight\n",
      "not found:  layer2.0.shortcut.1.bias\n",
      "not found:  layer2.0.shortcut.1.running_mean\n",
      "not found:  layer2.0.shortcut.1.running_var\n",
      "not found:  layer2.0.shortcut.1.num_batches_tracked\n",
      "not found:  layer2.1.conv1.weight\n",
      "not found:  layer2.1.conv2.weight\n",
      "not found:  layer2.1.bn1.weight\n",
      "not found:  layer2.1.bn1.bias\n",
      "not found:  layer2.1.bn1.running_mean\n",
      "not found:  layer2.1.bn1.running_var\n",
      "not found:  layer2.1.bn1.num_batches_tracked\n",
      "not found:  layer2.1.bn2.weight\n",
      "not found:  layer2.1.bn2.bias\n",
      "not found:  layer2.1.bn2.running_mean\n",
      "not found:  layer2.1.bn2.running_var\n",
      "not found:  layer2.1.bn2.num_batches_tracked\n",
      "not found:  layer4.0.conv1.weight\n",
      "not found:  layer4.0.conv2.weight\n",
      "not found:  layer4.0.bn1.weight\n",
      "not found:  layer4.0.bn1.bias\n",
      "not found:  layer4.0.bn1.running_mean\n",
      "not found:  layer4.0.bn1.running_var\n",
      "not found:  layer4.0.bn1.num_batches_tracked\n",
      "not found:  layer4.0.bn2.weight\n",
      "not found:  layer4.0.bn2.bias\n",
      "not found:  layer4.0.bn2.running_mean\n",
      "not found:  layer4.0.bn2.running_var\n",
      "not found:  layer4.0.bn2.num_batches_tracked\n",
      "not found:  layer4.0.shortcut.0.weight\n",
      "not found:  layer4.0.shortcut.1.weight\n",
      "not found:  layer4.0.shortcut.1.bias\n",
      "not found:  layer4.0.shortcut.1.running_mean\n",
      "not found:  layer4.0.shortcut.1.running_var\n",
      "not found:  layer4.0.shortcut.1.num_batches_tracked\n",
      "not found:  layer4.1.conv1.weight\n",
      "not found:  layer4.1.conv2.weight\n",
      "not found:  layer4.1.bn1.weight\n",
      "not found:  layer4.1.bn1.bias\n",
      "not found:  layer4.1.bn1.running_mean\n",
      "not found:  layer4.1.bn1.running_var\n",
      "not found:  layer4.1.bn1.num_batches_tracked\n",
      "not found:  layer4.1.bn2.weight\n",
      "not found:  layer4.1.bn2.bias\n",
      "not found:  layer4.1.bn2.running_mean\n",
      "not found:  layer4.1.bn2.running_var\n",
      "not found:  layer4.1.bn2.num_batches_tracked\n",
      "not found:  linear.weight\n",
      "not found:  linear.bias\n",
      "not found:  conv1.weight\n",
      "not found:  bn1.weight\n",
      "not found:  bn1.bias\n",
      "not found:  bn1.running_mean\n",
      "not found:  bn1.running_var\n",
      "not found:  bn1.num_batches_tracked\n",
      "not found:  layer1.0.conv1.weight\n",
      "not found:  layer1.0.conv2.weight\n",
      "not found:  layer1.0.bn1.weight\n",
      "not found:  layer1.0.bn1.bias\n",
      "not found:  layer1.0.bn1.running_mean\n",
      "not found:  layer1.0.bn1.running_var\n",
      "not found:  layer1.0.bn1.num_batches_tracked\n",
      "not found:  layer1.0.bn2.weight\n",
      "not found:  layer1.0.bn2.bias\n",
      "not found:  layer1.0.bn2.running_mean\n",
      "not found:  layer1.0.bn2.running_var\n",
      "not found:  layer1.0.bn2.num_batches_tracked\n",
      "not found:  layer1.1.conv1.weight\n",
      "not found:  layer1.1.conv2.weight\n",
      "not found:  layer1.1.bn1.weight\n",
      "not found:  layer1.1.bn1.bias\n",
      "not found:  layer1.1.bn1.running_mean\n",
      "not found:  layer1.1.bn1.running_var\n",
      "not found:  layer1.1.bn1.num_batches_tracked\n",
      "not found:  layer1.1.bn2.weight\n",
      "not found:  layer1.1.bn2.bias\n",
      "not found:  layer1.1.bn2.running_mean\n",
      "not found:  layer1.1.bn2.running_var\n",
      "not found:  layer1.1.bn2.num_batches_tracked\n",
      "not found:  layer2.0.conv1.weight\n",
      "not found:  layer2.0.conv2.weight\n",
      "not found:  layer2.0.bn1.weight\n",
      "not found:  layer2.0.bn1.bias\n",
      "not found:  layer2.0.bn1.running_mean\n",
      "not found:  layer2.0.bn1.running_var\n",
      "not found:  layer2.0.bn1.num_batches_tracked\n",
      "not found:  layer2.0.bn2.weight\n",
      "not found:  layer2.0.bn2.bias\n",
      "not found:  layer2.0.bn2.running_mean\n",
      "not found:  layer2.0.bn2.running_var\n",
      "not found:  layer2.0.bn2.num_batches_tracked\n",
      "not found:  layer2.0.shortcut.0.weight\n",
      "not found:  layer2.0.shortcut.1.weight\n",
      "not found:  layer2.0.shortcut.1.bias\n",
      "not found:  layer2.0.shortcut.1.running_mean\n",
      "not found:  layer2.0.shortcut.1.running_var\n",
      "not found:  layer2.0.shortcut.1.num_batches_tracked\n",
      "not found:  layer2.1.conv1.weight\n",
      "not found:  layer2.1.conv2.weight\n",
      "not found:  layer2.1.bn1.weight\n",
      "not found:  layer2.1.bn1.bias\n",
      "not found:  layer2.1.bn1.running_mean\n",
      "not found:  layer2.1.bn1.running_var\n",
      "not found:  layer2.1.bn1.num_batches_tracked\n",
      "not found:  layer2.1.bn2.weight\n",
      "not found:  layer2.1.bn2.bias\n",
      "not found:  layer2.1.bn2.running_mean\n",
      "not found:  layer2.1.bn2.running_var\n",
      "not found:  layer2.1.bn2.num_batches_tracked\n",
      "not found:  layer3.0.conv1.weight\n",
      "not found:  layer3.0.conv2.weight\n",
      "not found:  layer3.0.bn1.weight\n",
      "not found:  layer3.0.bn1.bias\n",
      "not found:  layer3.0.bn1.running_mean\n",
      "not found:  layer3.0.bn1.running_var\n",
      "not found:  layer3.0.bn1.num_batches_tracked\n",
      "not found:  layer3.0.bn2.weight\n",
      "not found:  layer3.0.bn2.bias\n",
      "not found:  layer3.0.bn2.running_mean\n",
      "not found:  layer3.0.bn2.running_var\n",
      "not found:  layer3.0.bn2.num_batches_tracked\n",
      "not found:  layer3.0.shortcut.0.weight\n",
      "not found:  layer3.0.shortcut.1.weight\n",
      "not found:  layer3.0.shortcut.1.bias\n",
      "not found:  layer3.0.shortcut.1.running_mean\n",
      "not found:  layer3.0.shortcut.1.running_var\n",
      "not found:  layer3.0.shortcut.1.num_batches_tracked\n",
      "not found:  layer3.1.conv1.weight\n",
      "not found:  layer3.1.conv2.weight\n",
      "not found:  layer3.1.bn1.weight\n",
      "not found:  layer3.1.bn1.bias\n",
      "not found:  layer3.1.bn1.running_mean\n",
      "not found:  layer3.1.bn1.running_var\n",
      "not found:  layer3.1.bn1.num_batches_tracked\n",
      "not found:  layer3.1.bn2.weight\n",
      "not found:  layer3.1.bn2.bias\n",
      "not found:  layer3.1.bn2.running_mean\n",
      "not found:  layer3.1.bn2.running_var\n",
      "not found:  layer3.1.bn2.num_batches_tracked\n"
     ]
    }
   ],
   "source": [
    "split_model = [layer_1, layer_2, layer_3, layer_4]\n",
    "\n",
    "# load model params into dictionary\n",
    "state_dict = torch.load(io.get_model_path(\"{}\".format(configs[\"load_model\"])), map_location=configs['device'])\n",
    "\n",
    "# add params to split\n",
    "for l in split_model:\n",
    "    l = io.load_state_dict(l, \n",
    "                    state_dict['model_state_dict'] if 'model_state_dict' in state_dict \n",
    "                    else state_dict['state_dict'] if 'state_dict' in state_dict else state_dict,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['conv1.weight', 'bn1.weight', 'bn1.bias', 'bn1.running_mean', 'bn1.running_var', 'bn1.num_batches_tracked', 'layer1.0.conv1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn1.weight', 'layer1.0.bn1.bias', 'layer1.0.bn1.running_mean', 'layer1.0.bn1.running_var', 'layer1.0.bn1.num_batches_tracked', 'layer1.0.bn2.weight', 'layer1.0.bn2.bias', 'layer1.0.bn2.running_mean', 'layer1.0.bn2.running_var', 'layer1.0.bn2.num_batches_tracked', 'layer1.1.conv1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn1.weight', 'layer1.1.bn1.bias', 'layer1.1.bn1.running_mean', 'layer1.1.bn1.running_var', 'layer1.1.bn1.num_batches_tracked', 'layer1.1.bn2.weight', 'layer1.1.bn2.bias', 'layer1.1.bn2.running_mean', 'layer1.1.bn2.running_var', 'layer1.1.bn2.num_batches_tracked', 'layer2.0.conv1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn1.weight', 'layer2.0.bn1.bias', 'layer2.0.bn1.running_mean', 'layer2.0.bn1.running_var', 'layer2.0.bn1.num_batches_tracked', 'layer2.0.bn2.weight', 'layer2.0.bn2.bias', 'layer2.0.bn2.running_mean', 'layer2.0.bn2.running_var', 'layer2.0.bn2.num_batches_tracked', 'layer2.0.shortcut.0.weight', 'layer2.0.shortcut.1.weight', 'layer2.0.shortcut.1.bias', 'layer2.0.shortcut.1.running_mean', 'layer2.0.shortcut.1.running_var', 'layer2.0.shortcut.1.num_batches_tracked', 'layer2.1.conv1.weight', 'layer2.1.conv2.weight', 'layer2.1.bn1.weight', 'layer2.1.bn1.bias', 'layer2.1.bn1.running_mean', 'layer2.1.bn1.running_var', 'layer2.1.bn1.num_batches_tracked', 'layer2.1.bn2.weight', 'layer2.1.bn2.bias', 'layer2.1.bn2.running_mean', 'layer2.1.bn2.running_var', 'layer2.1.bn2.num_batches_tracked', 'layer3.0.conv1.weight', 'layer3.0.conv2.weight', 'layer3.0.bn1.weight', 'layer3.0.bn1.bias', 'layer3.0.bn1.running_mean', 'layer3.0.bn1.running_var', 'layer3.0.bn1.num_batches_tracked', 'layer3.0.bn2.weight', 'layer3.0.bn2.bias', 'layer3.0.bn2.running_mean', 'layer3.0.bn2.running_var', 'layer3.0.bn2.num_batches_tracked', 'layer3.0.shortcut.0.weight', 'layer3.0.shortcut.1.weight', 'layer3.0.shortcut.1.bias', 'layer3.0.shortcut.1.running_mean', 'layer3.0.shortcut.1.running_var', 'layer3.0.shortcut.1.num_batches_tracked', 'layer3.1.conv1.weight', 'layer3.1.conv2.weight', 'layer3.1.bn1.weight', 'layer3.1.bn1.bias', 'layer3.1.bn1.running_mean', 'layer3.1.bn1.running_var', 'layer3.1.bn1.num_batches_tracked', 'layer3.1.bn2.weight', 'layer3.1.bn2.bias', 'layer3.1.bn2.running_mean', 'layer3.1.bn2.running_var', 'layer3.1.bn2.num_batches_tracked', 'layer4.0.conv1.weight', 'layer4.0.conv2.weight', 'layer4.0.bn1.weight', 'layer4.0.bn1.bias', 'layer4.0.bn1.running_mean', 'layer4.0.bn1.running_var', 'layer4.0.bn1.num_batches_tracked', 'layer4.0.bn2.weight', 'layer4.0.bn2.bias', 'layer4.0.bn2.running_mean', 'layer4.0.bn2.running_var', 'layer4.0.bn2.num_batches_tracked', 'layer4.0.shortcut.0.weight', 'layer4.0.shortcut.1.weight', 'layer4.0.shortcut.1.bias', 'layer4.0.shortcut.1.running_mean', 'layer4.0.shortcut.1.running_var', 'layer4.0.shortcut.1.num_batches_tracked', 'layer4.1.conv1.weight', 'layer4.1.conv2.weight', 'layer4.1.bn1.weight', 'layer4.1.bn1.bias', 'layer4.1.bn1.running_mean', 'layer4.1.bn1.running_var', 'layer4.1.bn1.num_batches_tracked', 'layer4.1.bn2.weight', 'layer4.1.bn2.bias', 'layer4.1.bn2.running_mean', 'layer4.1.bn2.running_var', 'layer4.1.bn2.num_batches_tracked', 'linear.weight', 'linear.bias'])\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "# look at state dict keys\n",
    "print(state_dict.keys())\n",
    "for i in split_model:\n",
    "    print(len(l.state_dict().keys()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load weights into full model\n",
    "model = io.load_state_dict(model, \n",
    "                    state_dict['model_state_dict'] if 'model_state_dict' in state_dict \n",
    "                    else state_dict['state_dict'] if 'state_dict' in state_dict else state_dict,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches: 1000/1000\n",
      "histogram tensor([  -39.9385,  -904.8687,   326.9596,  -956.4197,   484.1187, -1314.4147,\n",
      "         3051.0825,  -949.1459,  -234.6672, -1036.2506])\n"
     ]
    }
   ],
   "source": [
    "# compare outputs\n",
    "\n",
    "input = torch.rand(1000, 3, 32, 32, device=torch.device(configs['device'])) # 1k images, 3 channels, 32x32 image (cifar100) \n",
    "\n",
    "# put models into eval mode and on device\n",
    "model.eval()\n",
    "model.to(configs['device'])\n",
    "for l in split_model:\n",
    "    l.eval()\n",
    "    l.to(configs['device'])\n",
    "\n",
    "# make inference \n",
    "with torch.no_grad():\n",
    "        output_full = model(input)\n",
    "\n",
    "        output_split = input\n",
    "        for l in split_model:\n",
    "                output_split = l(output_split)\n",
    "\n",
    "\n",
    "match_count = (torch.argmax(output_split, axis=1) == torch.argmax(output_full, axis=1)).sum().item()\n",
    "label_hist = output_full.sum(0)\n",
    "print(f'Matches: {match_count}/{output_full.size(0)}')\n",
    "print(f'histogram {label_hist}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-2.9823e-04, -5.6650e-04, -2.4624e-04,  ..., -5.1510e-04,\n",
       "           -6.3805e-04, -5.2435e-04],\n",
       "          [-4.3773e-04, -7.1539e-04, -2.7594e-04,  ..., -9.2648e-04,\n",
       "           -7.3956e-04, -7.0539e-04],\n",
       "          [-3.7027e-04, -4.2024e-04, -4.8644e-04,  ..., -8.5245e-04,\n",
       "           -8.8775e-04, -6.1091e-04],\n",
       "          ...,\n",
       "          [-6.3645e-04, -7.9164e-04, -5.4833e-04,  ..., -8.4186e-04,\n",
       "           -8.4062e-04, -8.0071e-04],\n",
       "          [-7.3719e-04, -7.7537e-04, -5.9039e-04,  ..., -7.3747e-04,\n",
       "           -1.0451e-03, -8.5831e-04],\n",
       "          [-5.2946e-04, -5.0496e-04, -5.9778e-04,  ..., -8.9288e-04,\n",
       "           -8.1674e-04, -7.0534e-04]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "         [[ 5.9474e-04,  3.9763e-04,  4.9622e-04,  ...,  2.8028e-04,\n",
       "            5.4639e-04,  3.5110e-04],\n",
       "          [ 4.7937e-04,  5.5001e-04,  3.5473e-04,  ...,  3.0471e-04,\n",
       "            2.3662e-04,  7.8213e-05],\n",
       "          [ 1.8307e-04, -1.9454e-05,  3.1363e-05,  ...,  1.5220e-05,\n",
       "           -8.0175e-05,  2.1051e-04],\n",
       "          ...,\n",
       "          [ 4.3794e-04,  5.4386e-04,  2.8930e-04,  ...,  2.9818e-04,\n",
       "            5.3881e-04,  4.0509e-04],\n",
       "          [ 6.3052e-04,  5.6057e-04,  3.7989e-04,  ...,  2.6735e-04,\n",
       "            5.8792e-04,  3.6675e-04],\n",
       "          [-2.4907e-04,  1.8672e-05, -2.3854e-04,  ..., -1.0085e-04,\n",
       "           -1.3095e-04,  4.2730e-05]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[-2.8265e-05, -4.2389e-04, -7.9202e-04,  ..., -5.2510e-04,\n",
       "           -3.3887e-04, -5.4215e-04],\n",
       "          [-3.5404e-04, -9.2698e-04, -8.8722e-04,  ..., -4.5289e-04,\n",
       "           -7.7920e-04, -7.7040e-04],\n",
       "          [-2.2538e-04, -7.2006e-04, -8.5944e-04,  ..., -4.6230e-04,\n",
       "           -6.5721e-04, -9.3898e-04],\n",
       "          ...,\n",
       "          [-4.9620e-04, -7.0785e-04, -8.8578e-04,  ..., -4.9926e-04,\n",
       "           -3.5897e-04, -6.5787e-04],\n",
       "          [-2.6603e-04, -8.2878e-04, -9.5881e-04,  ..., -6.8446e-04,\n",
       "           -4.8708e-04, -9.1354e-04],\n",
       "          [-5.3577e-04, -6.1651e-04, -6.7389e-04,  ..., -5.1100e-04,\n",
       "           -5.9098e-04, -6.3907e-04]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "         [[ 2.6019e-04,  3.2920e-04,  3.8547e-04,  ...,  2.9611e-04,\n",
       "            3.9027e-04,  3.4475e-04],\n",
       "          [ 4.2550e-04, -6.7375e-05,  1.3459e-04,  ...,  9.0958e-05,\n",
       "            2.2531e-04,  4.6479e-04],\n",
       "          [ 2.0139e-05,  3.0947e-04,  2.5412e-04,  ..., -3.6328e-05,\n",
       "            1.5286e-04,  1.6365e-04],\n",
       "          ...,\n",
       "          [ 3.1920e-04,  3.1142e-04,  2.5596e-04,  ...,  1.5027e-04,\n",
       "            2.7588e-04,  4.7208e-04],\n",
       "          [ 1.6927e-04,  1.3494e-04,  4.2766e-05,  ..., -7.0647e-05,\n",
       "            3.5931e-04,  4.9139e-04],\n",
       "          [-1.9740e-04,  8.1730e-05, -2.0494e-04,  ...,  2.7609e-05,\n",
       "           -2.3261e-04,  6.8838e-05]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[-2.2795e-04, -3.9693e-04, -1.9353e-04,  ..., -4.8019e-04,\n",
       "           -7.5748e-04, -5.9622e-04],\n",
       "          [-3.0229e-04, -5.2564e-04, -2.9444e-04,  ..., -9.4751e-04,\n",
       "           -9.8185e-04, -7.0456e-04],\n",
       "          [-3.2210e-04, -2.7953e-04, -5.9654e-04,  ..., -7.6089e-04,\n",
       "           -7.5900e-04, -4.9935e-04],\n",
       "          ...,\n",
       "          [-3.8521e-04, -4.2657e-04, -7.0048e-04,  ..., -5.3914e-04,\n",
       "           -6.5355e-04, -4.6537e-04],\n",
       "          [-1.4329e-04, -7.3993e-04, -6.1853e-04,  ..., -9.4583e-04,\n",
       "           -7.3784e-04, -6.6215e-04],\n",
       "          [-5.2712e-04, -6.8194e-04, -7.3836e-04,  ..., -7.6923e-04,\n",
       "           -6.5093e-04, -4.3941e-04]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "         [[ 2.7742e-04,  2.5739e-04,  1.8194e-04,  ...,  4.6306e-04,\n",
       "           -5.5419e-05,  8.4818e-05],\n",
       "          [ 4.4335e-04,  3.1822e-04,  3.1951e-04,  ..., -3.9656e-05,\n",
       "            8.1453e-04,  4.0828e-04],\n",
       "          [ 1.4297e-04,  3.3788e-04,  2.7549e-04,  ...,  3.9432e-04,\n",
       "            4.7649e-04,  3.0031e-04],\n",
       "          ...,\n",
       "          [ 5.7864e-04,  5.2925e-04,  2.4024e-04,  ..., -7.6561e-05,\n",
       "            6.4250e-04,  3.8789e-04],\n",
       "          [ 1.5632e-04,  5.4232e-05,  1.2464e-04,  ...,  4.5086e-04,\n",
       "            7.6763e-06,  5.1366e-04],\n",
       "          [-2.1896e-04, -2.9654e-05,  1.3303e-05,  ..., -2.9396e-04,\n",
       "           -3.8895e-05,  3.2349e-05]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[-2.3333e-04, -4.0322e-04, -6.3063e-04,  ..., -4.6176e-04,\n",
       "           -5.9835e-04, -6.2032e-04],\n",
       "          [-5.8516e-04, -7.8039e-04, -8.3251e-04,  ..., -8.0718e-04,\n",
       "           -9.8421e-04, -7.4273e-04],\n",
       "          [-5.9241e-04, -7.9858e-04, -6.4709e-04,  ..., -8.6028e-04,\n",
       "           -1.0079e-03, -7.3784e-04],\n",
       "          ...,\n",
       "          [-5.3465e-04, -6.9986e-04, -4.6744e-04,  ..., -6.2127e-04,\n",
       "           -6.5436e-04, -6.9108e-04],\n",
       "          [-5.0895e-04, -4.6917e-04, -7.2931e-04,  ..., -6.3002e-04,\n",
       "           -7.8035e-04, -5.3289e-04],\n",
       "          [-4.0450e-04, -5.6365e-04, -6.6736e-04,  ..., -5.1724e-04,\n",
       "           -7.0216e-04, -2.9338e-04]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "         [[ 3.8287e-04,  5.0289e-04,  1.3476e-04,  ...,  1.8019e-04,\n",
       "            5.5386e-04,  4.3455e-04],\n",
       "          [-3.3554e-05,  4.5780e-05,  3.4397e-04,  ...,  3.6108e-04,\n",
       "            1.2839e-04,  1.6169e-04],\n",
       "          [ 3.1436e-04,  6.1960e-04,  3.6311e-04,  ..., -3.6729e-05,\n",
       "            9.2742e-05,  1.8422e-04],\n",
       "          ...,\n",
       "          [ 1.8918e-06,  2.8260e-04,  2.2697e-04,  ...,  2.5350e-04,\n",
       "            2.0311e-04,  4.6549e-04],\n",
       "          [ 5.4862e-04,  3.2155e-04,  3.3253e-04,  ...,  2.7633e-04,\n",
       "            5.3629e-04,  2.0926e-04],\n",
       "          [-1.9616e-04,  7.9573e-05, -2.7850e-04,  ..., -1.1923e-04,\n",
       "           -2.2448e-04,  1.0935e-04]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[-5.3706e-06, -5.1295e-04, -5.1693e-04,  ..., -3.8231e-04,\n",
       "           -5.2065e-04, -3.5928e-04],\n",
       "          [-4.3224e-04, -7.1564e-04, -6.1467e-04,  ..., -3.5889e-04,\n",
       "           -6.0568e-04, -3.8563e-04],\n",
       "          [-3.5791e-04, -9.1008e-04, -5.7121e-04,  ..., -5.7034e-04,\n",
       "           -3.2516e-04, -6.6276e-04],\n",
       "          ...,\n",
       "          [-3.6920e-04, -5.8127e-04, -5.5825e-04,  ..., -6.2866e-04,\n",
       "           -7.3760e-04, -3.1146e-04],\n",
       "          [-2.8328e-04, -5.4303e-04, -4.6097e-04,  ..., -7.4590e-04,\n",
       "           -4.4485e-04, -5.9361e-04],\n",
       "          [-3.5792e-04, -6.5183e-04, -4.0856e-04,  ..., -7.5535e-04,\n",
       "           -5.3929e-04, -3.5606e-04]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "         [[ 9.9298e-05,  2.3866e-04,  3.0215e-04,  ...,  6.0875e-04,\n",
       "            5.4766e-04,  3.9463e-04],\n",
       "          [ 9.6822e-05,  1.9266e-04,  1.7807e-04,  ...,  5.0829e-04,\n",
       "            6.4866e-04,  4.0789e-04],\n",
       "          [ 4.1610e-04,  1.4925e-04,  4.0713e-04,  ...,  2.9090e-04,\n",
       "            4.5627e-04,  3.9793e-04],\n",
       "          ...,\n",
       "          [-3.3848e-05,  2.6990e-04,  2.6122e-04,  ...,  2.8885e-04,\n",
       "           -1.4999e-07,  3.9794e-04],\n",
       "          [ 1.9405e-04,  2.6250e-04,  2.6988e-04,  ..., -6.6741e-05,\n",
       "            3.0139e-04,  4.2286e-04],\n",
       "          [-1.8263e-04,  9.3052e-05, -3.1346e-04,  ...,  1.1762e-04,\n",
       "           -2.0319e-04,  4.8775e-05]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[-3.2201e-04, -5.5101e-04, -3.7886e-04,  ..., -3.3623e-04,\n",
       "           -2.5336e-04, -4.7901e-04],\n",
       "          [-5.7199e-04, -7.7651e-04, -3.6539e-04,  ..., -7.5967e-04,\n",
       "           -3.4267e-04, -7.9888e-04],\n",
       "          [-4.4162e-04, -5.5255e-04, -7.8136e-04,  ..., -5.3609e-04,\n",
       "           -9.0593e-04, -8.5963e-04],\n",
       "          ...,\n",
       "          [-2.4440e-04, -1.5635e-04, -3.7219e-04,  ..., -6.3984e-04,\n",
       "           -6.6885e-04, -7.2044e-04],\n",
       "          [-2.3133e-04, -3.9861e-04, -6.8556e-04,  ..., -7.1354e-04,\n",
       "           -6.4310e-04, -7.1554e-04],\n",
       "          [-2.4276e-04, -5.3259e-04, -6.5212e-04,  ..., -5.6583e-04,\n",
       "           -4.9873e-04, -6.4036e-04]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "         [[ 4.9255e-04,  4.9555e-04,  4.2586e-04,  ...,  2.3528e-04,\n",
       "            3.9811e-06,  1.0638e-05],\n",
       "          [ 4.7827e-04,  6.2114e-04,  4.5894e-04,  ...,  2.8961e-04,\n",
       "            4.0830e-04,  1.4495e-04],\n",
       "          [ 1.4818e-04,  3.2154e-04,  2.8732e-04,  ...,  1.9411e-04,\n",
       "            1.2131e-04,  2.8898e-04],\n",
       "          ...,\n",
       "          [ 1.7318e-04,  4.3829e-04,  1.2552e-04,  ...,  2.6140e-04,\n",
       "            2.7646e-04,  6.0842e-04],\n",
       "          [ 2.9813e-04, -9.2123e-05,  1.3802e-04,  ...,  1.4120e-04,\n",
       "            3.3500e-04, -4.9756e-05],\n",
       "          [-1.3903e-04, -3.6857e-05,  7.5220e-05,  ..., -6.6418e-05,\n",
       "           -2.0172e-04,  1.5108e-04]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]]]], grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# go layer by layer to identify mismatch\n",
    "split_model[0].conv1(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    Test I/O Logic\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    Test I/O Logic\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE\n",
    "\n",
    "# make dir name \n",
    "time_stamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "if len(configs['load_model']) == 0:\n",
    "    folder_name='{}-{}-{}-np{}-pr{}-lcm{}-{}'.format( \n",
    "                configs['data-code'], \n",
    "                configs['model'], \n",
    "                configs['sparsity-type'], \n",
    "                configs['num_partition'], \n",
    "                configs['prune-ratio'], \n",
    "                configs['lambda-comm'],\n",
    "                time_stamp)\n",
    "else:\n",
    "    folder_name = '{}-{}'.format(configs['load_model'][:-3],time_stamp)\n",
    "\n",
    "# make folder \n",
    "folder_path = os.path.join(os.getcwd(), 'assets', 'models',folder_name)\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "\n",
    "# save weights\n",
    "index = 0\n",
    "for l in split_model:\n",
    "    fpath = os.path.join(os.getcwd(), 'assets', 'models', folder_path, f'layer_model_{index}.pth')\n",
    "    torch.save(l.state_dict(), fpath)\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split models:\n",
      "['cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240615-135016', 'cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240615-141807', 'cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240622-092931', 'cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240622-125523', 'cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240623-182335', 'cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240623-182910', 'cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240624-204135', 'cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240625-073155', 'cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240626-090056', 'cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240626-202301', 'cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240626-213627', 'cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240626-213724', 'cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240626-214353', 'cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240627-105309', 'cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240629-090932', 'cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240629-095759', 'cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240701-181315', 'cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240701-220925', 'cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240702-082928', 'cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240702-083133', 'cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240708-092933', 'cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240708-100959', 'cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240708-101251', 'cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240708-104751', 'cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240708-110342', 'cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240708-111919', 'cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240708-112135', 'cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240713-112843', 'cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240713-122622', 'cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240713-140146', 'cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240713-140307', 'cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240713-150826', 'cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240713-160054', 'cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240713-160303', 'cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240713-161733', 'cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240713-190111', 'cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240714-122801', 'cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240715-215600', 'cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240719-103106', 'cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240723-215600', 'vsplit-cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240708-103841']\n",
      "\n",
      "loading split model cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240615-135016\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 32, 32]           1,728\n",
      "       BatchNorm2d-2           [-1, 64, 32, 32]             128\n",
      "            Conv2d-3           [-1, 64, 32, 32]          36,864\n",
      "       BatchNorm2d-4           [-1, 64, 32, 32]             128\n",
      "            Conv2d-5           [-1, 64, 32, 32]          36,864\n",
      "       BatchNorm2d-6           [-1, 64, 32, 32]             128\n",
      "        BasicBlock-7           [-1, 64, 32, 32]               0\n",
      "            Conv2d-8           [-1, 64, 32, 32]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 32, 32]             128\n",
      "           Conv2d-10           [-1, 64, 32, 32]          36,864\n",
      "      BatchNorm2d-11           [-1, 64, 32, 32]             128\n",
      "       BasicBlock-12           [-1, 64, 32, 32]               0\n",
      "================================================================\n",
      "Total params: 149,824\n",
      "Trainable params: 149,824\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 6.00\n",
      "Params size (MB): 0.57\n",
      "Estimated Total Size (MB): 6.58\n",
      "----------------------------------------------------------------\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 128, 16, 16]          73,728\n",
      "       BatchNorm2d-2          [-1, 128, 16, 16]             256\n",
      "            Conv2d-3          [-1, 128, 16, 16]         147,456\n",
      "       BatchNorm2d-4          [-1, 128, 16, 16]             256\n",
      "            Conv2d-5          [-1, 128, 16, 16]           8,192\n",
      "       BatchNorm2d-6          [-1, 128, 16, 16]             256\n",
      "        BasicBlock-7          [-1, 128, 16, 16]               0\n",
      "            Conv2d-8          [-1, 128, 16, 16]         147,456\n",
      "       BatchNorm2d-9          [-1, 128, 16, 16]             256\n",
      "           Conv2d-10          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-11          [-1, 128, 16, 16]             256\n",
      "       BasicBlock-12          [-1, 128, 16, 16]               0\n",
      "================================================================\n",
      "Total params: 525,568\n",
      "Trainable params: 525,568\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.25\n",
      "Forward/backward pass size (MB): 3.00\n",
      "Params size (MB): 2.00\n",
      "Estimated Total Size (MB): 5.25\n",
      "----------------------------------------------------------------\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 256, 8, 8]         294,912\n",
      "       BatchNorm2d-2            [-1, 256, 8, 8]             512\n",
      "            Conv2d-3            [-1, 256, 8, 8]         589,824\n",
      "       BatchNorm2d-4            [-1, 256, 8, 8]             512\n",
      "            Conv2d-5            [-1, 256, 8, 8]          32,768\n",
      "       BatchNorm2d-6            [-1, 256, 8, 8]             512\n",
      "        BasicBlock-7            [-1, 256, 8, 8]               0\n",
      "            Conv2d-8            [-1, 256, 8, 8]         589,824\n",
      "       BatchNorm2d-9            [-1, 256, 8, 8]             512\n",
      "           Conv2d-10            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-11            [-1, 256, 8, 8]             512\n",
      "       BasicBlock-12            [-1, 256, 8, 8]               0\n",
      "================================================================\n",
      "Total params: 2,099,712\n",
      "Trainable params: 2,099,712\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.12\n",
      "Forward/backward pass size (MB): 1.50\n",
      "Params size (MB): 8.01\n",
      "Estimated Total Size (MB): 9.63\n",
      "----------------------------------------------------------------\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 512, 4, 4]       1,179,648\n",
      "       BatchNorm2d-2            [-1, 512, 4, 4]           1,024\n",
      "            Conv2d-3            [-1, 512, 4, 4]       2,359,296\n",
      "       BatchNorm2d-4            [-1, 512, 4, 4]           1,024\n",
      "            Conv2d-5            [-1, 512, 4, 4]         131,072\n",
      "       BatchNorm2d-6            [-1, 512, 4, 4]           1,024\n",
      "        BasicBlock-7            [-1, 512, 4, 4]               0\n",
      "            Conv2d-8            [-1, 512, 4, 4]       2,359,296\n",
      "       BatchNorm2d-9            [-1, 512, 4, 4]           1,024\n",
      "           Conv2d-10            [-1, 512, 4, 4]       2,359,296\n",
      "      BatchNorm2d-11            [-1, 512, 4, 4]           1,024\n",
      "       BasicBlock-12            [-1, 512, 4, 4]               0\n",
      "           Linear-13                   [-1, 10]           5,130\n",
      "================================================================\n",
      "Total params: 8,398,858\n",
      "Trainable params: 8,398,858\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.06\n",
      "Forward/backward pass size (MB): 0.75\n",
      "Params size (MB): 32.04\n",
      "Estimated Total Size (MB): 32.85\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# LOAD\n",
    "\n",
    "select = 0\n",
    "\n",
    "layer_output_sizes = [(3,32,32), (64,32,32), (128,16,16), (256,8,8)]\n",
    "\n",
    "model_path = os.path.join(os.getcwd(), 'assets', 'models')\n",
    "filenames = os.listdir(model_path)\n",
    "\n",
    "# get dirs\n",
    "split_model_names = []\n",
    "for filename in filenames: # loop through all the files and folders\n",
    "    if os.path.isdir(os.path.join(model_path, filename)): # check whether the current object is a folder or not\n",
    "        split_model_names.append(filename)\n",
    "\n",
    "print('Split models:')\n",
    "print(split_model_names)\n",
    "print()\n",
    "\n",
    "model_name = split_model_names[select] \n",
    "print(f'loading split model {model_name}')\n",
    "\n",
    "index = 0\n",
    "for l in split_model:\n",
    "    layer_state_dict = torch.load(os.path.join(model_path, model_name, f'layer_model_{index}.pth'))\n",
    "    l = io.load_state_dict(l, layer_state_dict)\n",
    "\n",
    "\n",
    "    summary(l, layer_output_sizes[index],device=configs['device'])\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference time per data is 28.290510ms.\n",
      "conv1.weight 1024\n",
      "layer1.0.conv1.weight 1024\n",
      "layer1.0.conv2.weight 1024\n",
      "layer1.1.conv1.weight 1024\n",
      "layer1.1.conv2.weight 1024\n",
      "layer2.0.conv1.weight 256\n",
      "layer2.0.conv2.weight 256\n",
      "layer2.0.shortcut.0.weight 256\n",
      "layer2.1.conv1.weight 256\n",
      "layer2.1.conv2.weight 256\n",
      "layer3.0.conv1.weight 64\n",
      "layer3.0.conv2.weight 64\n",
      "layer3.0.shortcut.0.weight 64\n",
      "layer3.1.conv1.weight 64\n",
      "layer3.1.conv2.weight 64\n",
      "layer4.0.conv1.weight 16\n",
      "layer4.0.conv2.weight 16\n",
      "layer4.0.shortcut.0.weight 16\n",
      "layer4.1.conv1.weight 16\n",
      "layer4.1.conv2.weight 16\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    add partitions and communications to configs\n",
    "'''\n",
    "\n",
    "# gets random test input (with correct size)\n",
    "input_var = engine.get_input_from_code(configs)\n",
    "#print(input_var)\n",
    "\n",
    "# Config partitions and prune_ratio\n",
    "configs['num_partition'] = '4'#'./config/resnet18-v2.yaml'\n",
    "configs = engine.partition_generator(configs, model)\n",
    "            \n",
    "# Compute output size of each layer\n",
    "configs['partition'] = engine.featuremap_summary(model, configs['partition'], input_var)\n",
    "        \n",
    "# Setup communication costs\n",
    "configs['comm_costs'] = engine.set_communication_cost(model, configs['partition'],)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\natet\\Desktop\\graduate school\\thesis\\CaP\\assets\\figs\\cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001\n",
      "name: conv1.weight, img size: (3, 64) weight size: (64, 3)\n",
      "name: layer1.0.conv1.weight, img size: (64, 64) weight size: (64, 64)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    inspect IO per layer \n",
    "'''\n",
    "\n",
    "savepath=io.get_fig_path(\"{}\".format('.'.join(configs[\"load_model\"].split('.')[:-1])))\n",
    "print(savepath)\n",
    "\n",
    "# this function looks for model layers named in \"confgis['partition']\" (other layers are ignored)\n",
    "# -> \"conv\" and shortcut layers are the only ones \"split\"\n",
    "# -> total 20/49 (20/62?) layers are split\n",
    "counter = 0\n",
    "for name, W in model.named_parameters():\n",
    "        if name in configs['partition']:\n",
    "            #print(f'{counter} | {name}')\n",
    "            counter +=1\n",
    "\n",
    "# Plot model\n",
    "layer_id = (0,1,2) # inspect these layers\n",
    "testers.plot_layer(model, configs['partition'], layer_id=layer_id, savepath=savepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "conv1\n",
      "bn1\n",
      "layer1\n",
      "layer1.0\n",
      "layer1.0.conv1\n",
      "layer1.0.conv2\n",
      "layer1.0.bn1\n",
      "layer1.0.bn2\n",
      "layer1.0.shortcut\n",
      "layer1.1\n",
      "layer1.1.conv1\n",
      "layer1.1.conv2\n",
      "layer1.1.bn1\n",
      "layer1.1.bn2\n",
      "layer1.1.shortcut\n",
      "layer2\n",
      "layer2.0\n",
      "layer2.0.conv1\n",
      "layer2.0.conv2\n",
      "layer2.0.bn1\n",
      "layer2.0.bn2\n",
      "layer2.0.shortcut\n",
      "layer2.0.shortcut.0\n",
      "layer2.0.shortcut.1\n",
      "layer2.1\n",
      "layer2.1.conv1\n",
      "layer2.1.conv2\n",
      "layer2.1.bn1\n",
      "layer2.1.bn2\n",
      "layer2.1.shortcut\n",
      "layer3\n",
      "layer3.0\n",
      "layer3.0.conv1\n",
      "layer3.0.conv2\n",
      "layer3.0.bn1\n",
      "layer3.0.bn2\n",
      "layer3.0.shortcut\n",
      "layer3.0.shortcut.0\n",
      "layer3.0.shortcut.1\n",
      "layer3.1\n",
      "layer3.1.conv1\n",
      "layer3.1.conv2\n",
      "layer3.1.bn1\n",
      "layer3.1.bn2\n",
      "layer3.1.shortcut\n",
      "layer4\n",
      "layer4.0\n",
      "layer4.0.conv1\n",
      "layer4.0.conv2\n",
      "layer4.0.bn1\n",
      "layer4.0.bn2\n",
      "layer4.0.shortcut\n",
      "layer4.0.shortcut.0\n",
      "layer4.0.shortcut.1\n",
      "layer4.1\n",
      "layer4.1.conv1\n",
      "layer4.1.conv2\n",
      "layer4.1.bn1\n",
      "layer4.1.bn2\n",
      "layer4.1.shortcut\n",
      "linear\n"
     ]
    }
   ],
   "source": [
    "# split model \n",
    "\n",
    "# make copies of model per machine\n",
    "num_machines = max(configs['partition']['bn_partition']) # TODO: double check this makes sense\n",
    "model_machines = [model]*num_machines\n",
    "\n",
    "module_names =  [module[0] for i, module in enumerate(model.named_modules())]\n",
    "num_total_modules = len(module_names)\n",
    "\n",
    "split_module_names = list(configs['partition'].keys())\n",
    "\n",
    "for aname in module_names:\n",
    "    print(f'{aname}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 5, 6, 11, 12, 18, 19, 26, 27, 33, 34, 41, 42, 48, 49, 56, 57]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Setup datastrcuts to ID layers executed with 'extra' functionality\n",
    "'''\n",
    "\n",
    "# module numbering is based on module.named_parameters()\n",
    "relu_modules = [2, 7,8,13,14,20,2428,29,35,3943,44,50,54,58,59] # execute relu on this  layer \n",
    "split_module_indexes =  [i for i in range(len(module_names)) if 'conv' in module_names[i] ]\n",
    "\n",
    "print(split_module_indexes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nonzero_channels(atensor, dim=1):\n",
    "    return torch.unique(torch.nonzero(atensor, as_tuple=True)[dim]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  compare_tensors(t1, t2, dim=1, rshape=(1,64,-1)):\n",
    "    diff = torch.abs(t1-t2)\n",
    "\n",
    "    max_diff_pin_dim = torch.max(diff.reshape(rshape), dim)\n",
    "    return max_diff_pin_dim[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Prep Inout for Mock Run'''\n",
    "\n",
    "# TODO: reduce size of communicated tensors to only what is necessary \n",
    "# TODO: also check bias for nonzero\n",
    "# TODO: come up with more general scheme to handle residual layers\n",
    "\n",
    "# channel_id == INPUTS\n",
    "# filter_id  == OUTPUTS\n",
    "\n",
    "# setup input \n",
    "N_batch = 1\n",
    "input_tensor = torch.rand(N_batch, 3, 32, 32, device=torch.device(configs['device'])) # 1k images, 3 channels, 32x32 image (cifar100) \n",
    "\n",
    "# broadcast input_tensor to different machines\n",
    "# TODO: find a better datastructure for this\n",
    "#input = np.empty((num_machines, num_machines), dtype=torch.Tensor)\n",
    "input = [None]*num_machines\n",
    "input = [input[:] for i in range(num_machines)]\n",
    "for imach in range(num_machines):\n",
    "    input[imach][0] = input_tensor\n",
    "\n",
    "relu_layers,avg_pool_layers = get_functional_layers(configs['model'])\n",
    "residual_block_start, residual_connection_start, residual_block_end = get_residual_block_indexes(model)\n",
    "\n",
    "# put models into eval mode and on device\n",
    "model.eval()\n",
    "model.to(configs['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing module 1: conv1\n",
      "\tExecuting on machine 0\n",
      "\t\t received input channels tensor([0, 1, 2])\n",
      "\t\t-No input assigned to this machine. Skipping...\n",
      "\tExecuting on machine 1\n",
      "\t\t received input channels tensor([0, 1, 2])\n",
      "\t\t Output tensor shape : torch.Size([1, 64, 32, 32])\n",
      "\t\t sending C_out tensor([16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 31]) to machine 1\n",
      "\t\t sending C_out tensor([56, 62]) to machine 3\n",
      "\tExecuting on machine 2\n",
      "\t\t received input channels tensor([0, 1, 2])\n",
      "\t\t Output tensor shape : torch.Size([1, 64, 32, 32])\n",
      "\t\t sending C_out tensor([ 0, 12]) to machine 0\n",
      "\t\t sending C_out tensor([38, 42]) to machine 2\n",
      "\t\t sending C_out tensor([54, 57]) to machine 3\n",
      "\tExecuting on machine 3\n",
      "\t\t received input channels tensor([0, 1, 2])\n",
      "\t\t Output tensor shape : torch.Size([1, 64, 32, 32])\n",
      "\t\t sending C_out tensor([ 4,  8, 14]) to machine 0\n",
      "\t\t sending C_out tensor([16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 31]) to machine 1\n",
      "\t\t sending C_out tensor([36, 42, 45, 46, 47]) to machine 2\n",
      "\t\t sending C_out tensor([54, 57]) to machine 3\n",
      "Finished execution of layer 1\n",
      "\n",
      "Executing module 2: bn1\n",
      "\tExecuting on machine 0\n",
      "\t\t received input channels tensor([ 0,  4,  8, 12, 14])\n",
      "\t\t Output tensor shape : torch.Size([1, 64, 32, 32])\n",
      "\t\t-Applying ReLU\n",
      "\t\t sending C_out tensor([4]) to machine 0\n",
      "\tExecuting on machine 1\n",
      "\t\t received input channels tensor([16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 31])\n",
      "\t\t Output tensor shape : torch.Size([1, 64, 32, 32])\n",
      "\t\t-Applying ReLU\n",
      "\t\t sending C_out tensor([16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 29, 31]) to machine 1\n",
      "\tExecuting on machine 2\n",
      "\t\t received input channels tensor([36, 38, 42, 45, 46, 47])\n",
      "\t\t Output tensor shape : torch.Size([1, 64, 32, 32])\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 3\n",
      "\t\t received input channels tensor([54, 56, 57, 62])\n",
      "\t\t Output tensor shape : torch.Size([1, 64, 32, 32])\n",
      "\t\t-Applying ReLU\n",
      "\t\t sending C_out tensor([54, 57]) to machine 3\n",
      "Finished execution of layer 2\n",
      "\n",
      "Executing module 3: layer1\n",
      "\tExecuting on machine 0\n",
      "\t\t received input channels tensor([4])\n",
      "\t\t-Skipping module Sequential\n",
      "\tExecuting on machine 1\n",
      "\t\t received input channels tensor([16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 29, 31])\n",
      "\t\t-Skipping module Sequential\n",
      "\tExecuting on machine 2\n",
      "\t\t-No input sent to this machine. Skipping module\n",
      "\tExecuting on machine 3\n",
      "\t\t received input channels tensor([54, 57])\n",
      "\t\t-Skipping module Sequential\n",
      "Finished execution of layer 3\n",
      "\n",
      "Executing module 4: layer1.0\n",
      "\tExecuting on machine 0\n",
      "\t\t received input channels tensor([4])\n",
      "\t\t-Skipping module BasicBlock\n",
      "\tExecuting on machine 1\n",
      "\t\t received input channels tensor([16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 29, 31])\n",
      "\t\t-Skipping module BasicBlock\n",
      "\tExecuting on machine 2\n",
      "\t\t-No input sent to this machine. Skipping module\n",
      "\tExecuting on machine 3\n",
      "\t\t received input channels tensor([54, 57])\n",
      "\t\t-Skipping module BasicBlock\n",
      "Finished execution of layer 4\n",
      "\n",
      "Executing module 5: layer1.0.conv1\n",
      "\tExecuting on machine 0\n",
      "\t\t received input channels tensor([4])\n",
      "\t\t Output tensor shape : torch.Size([1, 64, 32, 32])\n",
      "\t\t sending C_out tensor([ 1,  3,  5, 12, 14]) to machine 0\n",
      "\t\t sending C_out tensor([18, 20, 24, 27, 28]) to machine 1\n",
      "\t\t sending C_out tensor([33, 35, 41, 44]) to machine 2\n",
      "\t\t sending C_out tensor([48, 57, 58, 59, 60, 63]) to machine 3\n",
      "\tExecuting on machine 1\n",
      "\t\t received input channels tensor([16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 29, 31])\n",
      "\t\t Output tensor shape : torch.Size([1, 64, 32, 32])\n",
      "\t\t sending C_out tensor([ 0,  2,  3,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15]) to machine 0\n",
      "\t\t sending C_out tensor([16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]) to machine 1\n",
      "\t\t sending C_out tensor([32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]) to machine 2\n",
      "\t\t sending C_out tensor([48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]) to machine 3\n",
      "\tExecuting on machine 2\n",
      "\t\t-No input sent to this machine. Skipping module\n",
      "\tExecuting on machine 3\n",
      "\t\t received input channels tensor([54, 57])\n",
      "\t\t Output tensor shape : torch.Size([1, 64, 32, 32])\n",
      "\t\t sending C_out tensor([ 0,  1,  4,  5, 10, 11, 13, 14]) to machine 0\n",
      "\t\t sending C_out tensor([16, 19, 21, 25, 30]) to machine 1\n",
      "\t\t sending C_out tensor([32, 33, 36, 38, 39, 41, 42, 44, 45, 46, 47]) to machine 2\n",
      "\t\t sending C_out tensor([51]) to machine 3\n",
      "Finished execution of layer 5\n",
      "\n",
      "Executing module 6: layer1.0.conv2\n",
      "\tExecuting on machine 0\n",
      "\t\t received input channels tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15])\n",
      "\t\t Output tensor shape : torch.Size([1, 64, 32, 32])\n",
      "\t\t sending C_out tensor([ 4,  9, 13, 14, 15]) to machine 0\n",
      "\t\t sending C_out tensor([16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]) to machine 1\n",
      "\t\t sending C_out tensor([32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]) to machine 2\n",
      "\t\t sending C_out tensor([48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]) to machine 3\n",
      "\tExecuting on machine 1\n",
      "\t\t received input channels tensor([16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31])\n",
      "\t\t Output tensor shape : torch.Size([1, 64, 32, 32])\n",
      "\t\t sending C_out tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15]) to machine 0\n",
      "\t\t sending C_out tensor([16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 31]) to machine 1\n",
      "\t\t sending C_out tensor([32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]) to machine 2\n",
      "\t\t sending C_out tensor([48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]) to machine 3\n",
      "\tExecuting on machine 2\n",
      "\t\t received input channels tensor([32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47])\n",
      "\t\t Output tensor shape : torch.Size([1, 64, 32, 32])\n",
      "\t\t sending C_out tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15]) to machine 0\n",
      "\t\t sending C_out tensor([16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]) to machine 1\n",
      "\t\t sending C_out tensor([34, 36, 40, 42, 43, 45, 46, 47]) to machine 2\n",
      "\t\t sending C_out tensor([48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]) to machine 3\n",
      "\tExecuting on machine 3\n",
      "\t\t received input channels tensor([48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63])\n",
      "\t\t Output tensor shape : torch.Size([1, 64, 32, 32])\n",
      "\t\t sending C_out tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15]) to machine 0\n",
      "\t\t sending C_out tensor([16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]) to machine 1\n",
      "\t\t sending C_out tensor([32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 45, 46, 47]) to machine 2\n",
      "\t\t sending C_out tensor([48, 49, 54, 57, 59, 61]) to machine 3\n",
      "Finished execution of layer 6\n",
      "\n",
      "Executing module 7: layer1.0.bn1\n",
      "\tExecuting on machine 0\n",
      "\t\t received input channels tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15])\n",
      "\t\t Output tensor shape : torch.Size([1, 64, 32, 32])\n",
      "\t\t-Applying ReLU\n",
      "\t\t sending C_out tensor([ 4, 10, 14]) to machine 0\n",
      "\tExecuting on machine 1\n",
      "\t\t received input channels tensor([16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31])\n",
      "\t\t Output tensor shape : torch.Size([1, 64, 32, 32])\n",
      "\t\t-Applying ReLU\n",
      "\t\t sending C_out tensor([16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 31]) to machine 1\n",
      "\tExecuting on machine 2\n",
      "\t\t received input channels tensor([32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47])\n",
      "\t\t Output tensor shape : torch.Size([1, 64, 32, 32])\n",
      "\t\t-Applying ReLU\n",
      "\t\t sending C_out tensor([36, 42, 43]) to machine 2\n",
      "\tExecuting on machine 3\n",
      "\t\t received input channels tensor([48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63])\n",
      "\t\t Output tensor shape : torch.Size([1, 64, 32, 32])\n",
      "\t\t-Applying ReLU\n",
      "\t\t sending C_out tensor([52, 54, 57]) to machine 3\n",
      "Finished execution of layer 7\n",
      "\n",
      "Executing module 8: layer1.0.bn2\n",
      "\tExecuting on machine 0\n",
      "\t\t received input channels tensor([ 4, 10, 14])\n",
      "\t\t Output tensor shape : torch.Size([1, 64, 32, 32])\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 1\n",
      "\t\t received input channels tensor([16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 31])\n",
      "\t\t Output tensor shape : torch.Size([1, 64, 32, 32])\n",
      "\t\t-Applying ReLU\n",
      "\t\t sending C_out tensor([16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 31]) to machine 1\n",
      "\tExecuting on machine 2\n",
      "\t\t received input channels tensor([36, 42, 43])\n",
      "\t\t Output tensor shape : torch.Size([1, 64, 32, 32])\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 3\n",
      "\t\t received input channels tensor([52, 54, 57])\n",
      "\t\t Output tensor shape : torch.Size([1, 64, 32, 32])\n",
      "\t\t-Applying ReLU\n",
      "\t\t sending C_out tensor([54]) to machine 3\n",
      "Finished execution of layer 8\n",
      "\n",
      "Executing module 9: layer1.0.shortcut\n",
      "\tExecuting on machine 0\n",
      "\t\t-No input sent to this machine. Skipping module\n",
      "\tExecuting on machine 1\n",
      "\t\t received input channels tensor([16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 31])\n",
      "\t\t-Skipping module Sequential\n",
      "\tExecuting on machine 2\n",
      "\t\t-No input sent to this machine. Skipping module\n",
      "\tExecuting on machine 3\n",
      "\t\t received input channels tensor([54])\n",
      "\t\t-Skipping module Sequential\n",
      "Finished execution of layer 9\n",
      "\n",
      "Executing module 10: layer1.1\n",
      "\tExecuting on machine 0\n",
      "\t\t-No input sent to this machine. Skipping module\n",
      "\tExecuting on machine 1\n",
      "\t\t received input channels tensor([16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 31])\n",
      "\t\t-Skipping module BasicBlock\n",
      "\tExecuting on machine 2\n",
      "\t\t-No input sent to this machine. Skipping module\n",
      "\tExecuting on machine 3\n",
      "\t\t received input channels tensor([54])\n",
      "\t\t-Skipping module BasicBlock\n",
      "Finished execution of layer 10\n",
      "\n",
      "Executing module 11: layer1.1.conv1\n",
      "\tExecuting on machine 0\n",
      "\t\t-No input sent to this machine. Skipping module\n",
      "\tExecuting on machine 1\n",
      "\t\t received input channels tensor([16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 31])\n",
      "\t\t Output tensor shape : torch.Size([1, 64, 32, 32])\n",
      "\t\t sending C_out tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15]) to machine 0\n",
      "\t\t sending C_out tensor([16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]) to machine 1\n",
      "\t\t sending C_out tensor([32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]) to machine 2\n",
      "\t\t sending C_out tensor([48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]) to machine 3\n",
      "\tExecuting on machine 2\n",
      "\t\t-No input sent to this machine. Skipping module\n",
      "\tExecuting on machine 3\n",
      "\t\t received input channels tensor([54])\n",
      "\t\t Output tensor shape : torch.Size([1, 64, 32, 32])\n",
      "\t\t sending C_out tensor([ 1,  2,  3,  6, 10, 11]) to machine 0\n",
      "\t\t sending C_out tensor([23, 30]) to machine 1\n",
      "\t\t sending C_out tensor([41]) to machine 2\n",
      "\t\t sending C_out tensor([61]) to machine 3\n",
      "Finished execution of layer 11\n",
      "\n",
      "Executing module 12: layer1.1.conv2\n",
      "\tExecuting on machine 0\n",
      "\t\t received input channels tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15])\n",
      "\t\t Output tensor shape : torch.Size([1, 64, 32, 32])\n",
      "\t\t sending C_out tensor([ 0,  5,  9, 11, 13]) to machine 0\n",
      "\t\t sending C_out tensor([16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]) to machine 1\n",
      "\t\t sending C_out tensor([32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]) to machine 2\n",
      "\t\t sending C_out tensor([48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]) to machine 3\n",
      "\tExecuting on machine 1\n",
      "\t\t received input channels tensor([16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31])\n",
      "\t\t Output tensor shape : torch.Size([1, 64, 32, 32])\n",
      "\t\t sending C_out tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15]) to machine 0\n",
      "\t\t sending C_out tensor([16, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 31]) to machine 1\n",
      "\t\t sending C_out tensor([32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]) to machine 2\n",
      "\t\t sending C_out tensor([48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]) to machine 3\n",
      "\tExecuting on machine 2\n",
      "\t\t received input channels tensor([32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47])\n",
      "\t\t Output tensor shape : torch.Size([1, 64, 32, 32])\n",
      "\t\t sending C_out tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15]) to machine 0\n",
      "\t\t sending C_out tensor([16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]) to machine 1\n",
      "\t\t sending C_out tensor([34, 35, 36, 46]) to machine 2\n",
      "\t\t sending C_out tensor([48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]) to machine 3\n",
      "\tExecuting on machine 3\n",
      "\t\t received input channels tensor([48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63])\n",
      "\t\t Output tensor shape : torch.Size([1, 64, 32, 32])\n",
      "\t\t sending C_out tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15]) to machine 0\n",
      "\t\t sending C_out tensor([16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]) to machine 1\n",
      "\t\t sending C_out tensor([32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]) to machine 2\n",
      "\t\t sending C_out tensor([48, 52, 56, 57, 62]) to machine 3\n",
      "Finished execution of layer 12\n",
      "\n",
      "Executing module 13: layer1.1.bn1\n",
      "\tExecuting on machine 0\n",
      "\t\t received input channels tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15])\n",
      "\t\t Output tensor shape : torch.Size([1, 64, 32, 32])\n",
      "\t\t-Applying ReLU\n",
      "\t\t sending C_out tensor([ 3,  6,  9, 10, 11, 13, 14]) to machine 0\n",
      "\tExecuting on machine 1\n",
      "\t\t received input channels tensor([16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31])\n",
      "\t\t Output tensor shape : torch.Size([1, 64, 32, 32])\n",
      "\t\t-Applying ReLU\n",
      "\t\t sending C_out tensor([16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]) to machine 1\n",
      "\tExecuting on machine 2\n",
      "\t\t received input channels tensor([32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47])\n",
      "\t\t Output tensor shape : torch.Size([1, 64, 32, 32])\n",
      "\t\t-Applying ReLU\n",
      "\t\t sending C_out tensor([32, 35, 36, 37, 38, 40, 41, 42, 43, 45]) to machine 2\n",
      "\tExecuting on machine 3\n",
      "\t\t received input channels tensor([48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63])\n",
      "\t\t Output tensor shape : torch.Size([1, 64, 32, 32])\n",
      "\t\t-Applying ReLU\n",
      "\t\t sending C_out tensor([50, 52, 54, 55, 56, 57, 58, 63]) to machine 3\n",
      "Finished execution of layer 13\n",
      "\n",
      "Executing module 14: layer1.1.bn2\n",
      "\tExecuting on machine 0\n",
      "\t\t received input channels tensor([ 3,  6,  9, 10, 11, 13, 14])\n",
      "\t\t Output tensor shape : torch.Size([1, 64, 32, 32])\n",
      "\t\t-Applying ReLU\n",
      "\t\t sending C_out tensor([ 3,  4,  6,  7,  9, 10, 11]) to machine 0\n",
      "\tExecuting on machine 1\n",
      "\t\t received input channels tensor([16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31])\n",
      "\t\t Output tensor shape : torch.Size([1, 64, 32, 32])\n",
      "\t\t-Applying ReLU\n",
      "\t\t sending C_out tensor([16, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]) to machine 1\n",
      "\tExecuting on machine 2\n",
      "\t\t received input channels tensor([32, 35, 36, 37, 38, 40, 41, 42, 43, 45])\n",
      "\t\t Output tensor shape : torch.Size([1, 64, 32, 32])\n",
      "\t\t-Applying ReLU\n",
      "\t\t sending C_out tensor([32, 35, 36, 37, 38, 40, 41, 42, 43]) to machine 2\n",
      "\tExecuting on machine 3\n",
      "\t\t received input channels tensor([50, 52, 54, 55, 56, 57, 58, 63])\n",
      "\t\t Output tensor shape : torch.Size([1, 64, 32, 32])\n",
      "\t\t-Applying ReLU\n",
      "\t\t sending C_out tensor([50, 52, 54, 55, 56, 57, 63]) to machine 3\n",
      "Finished execution of layer 14\n",
      "\n",
      "Executing module 15: layer1.1.shortcut\n",
      "\tExecuting on machine 0\n",
      "\t\t received input channels tensor([ 3,  4,  6,  7,  9, 10, 11])\n",
      "\t\t-Skipping module Sequential\n",
      "\tExecuting on machine 1\n",
      "\t\t received input channels tensor([16, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31])\n",
      "\t\t-Skipping module Sequential\n",
      "\tExecuting on machine 2\n",
      "\t\t received input channels tensor([32, 35, 36, 37, 38, 40, 41, 42, 43])\n",
      "\t\t-Skipping module Sequential\n",
      "\tExecuting on machine 3\n",
      "\t\t received input channels tensor([50, 52, 54, 55, 56, 57, 63])\n",
      "\t\t-Skipping module Sequential\n",
      "Finished execution of layer 15\n",
      "\n",
      "Executing module 16: layer2\n",
      "\tExecuting on machine 0\n",
      "\t\t received input channels tensor([ 3,  4,  6,  7,  9, 10, 11])\n",
      "\t\t-Skipping module Sequential\n",
      "\tExecuting on machine 1\n",
      "\t\t received input channels tensor([16, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31])\n",
      "\t\t-Skipping module Sequential\n",
      "\tExecuting on machine 2\n",
      "\t\t received input channels tensor([32, 35, 36, 37, 38, 40, 41, 42, 43])\n",
      "\t\t-Skipping module Sequential\n",
      "\tExecuting on machine 3\n",
      "\t\t received input channels tensor([50, 52, 54, 55, 56, 57, 63])\n",
      "\t\t-Skipping module Sequential\n",
      "Finished execution of layer 16\n",
      "\n",
      "Executing module 17: layer2.0\n",
      "\tExecuting on machine 0\n",
      "\t\t received input channels tensor([ 3,  4,  6,  7,  9, 10, 11])\n",
      "\t\t-Skipping module BasicBlock\n",
      "\tExecuting on machine 1\n",
      "\t\t received input channels tensor([16, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31])\n",
      "\t\t-Skipping module BasicBlock\n",
      "\tExecuting on machine 2\n",
      "\t\t received input channels tensor([32, 35, 36, 37, 38, 40, 41, 42, 43])\n",
      "\t\t-Skipping module BasicBlock\n",
      "\tExecuting on machine 3\n",
      "\t\t received input channels tensor([50, 52, 54, 55, 56, 57, 63])\n",
      "\t\t-Skipping module BasicBlock\n",
      "Finished execution of layer 17\n",
      "\n",
      "Executing module 18: layer2.0.conv1\n",
      "\tExecuting on machine 0\n",
      "\t\t received input channels tensor([ 3,  4,  6,  7,  9, 10, 11])\n",
      "\t\t-Saving input for later...\n",
      "\t\t Output tensor shape : torch.Size([1, 128, 16, 16])\n",
      "\t\t sending C_out tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 12, 14, 16, 17, 18, 19, 20, 21,\n",
      "        25, 27, 28, 29, 30, 31]) to machine 0\n",
      "\t\t sending C_out tensor([33, 34, 35, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 49, 50, 51, 52, 53,\n",
      "        54, 57, 58, 59, 60, 61, 62, 63]) to machine 1\n",
      "\t\t sending C_out tensor([65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 77, 79, 80, 81, 82, 85, 86,\n",
      "        87, 89, 90, 91, 92, 93, 95]) to machine 2\n",
      "\t\t sending C_out tensor([ 97,  99, 100, 101, 102, 103, 105, 106, 107, 109, 110, 111, 113, 115,\n",
      "        116, 118, 119, 120, 121, 122, 123, 125, 127]) to machine 3\n",
      "\tExecuting on machine 1\n",
      "\t\t received input channels tensor([16, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31])\n",
      "\t\t-Saving input for later...\n",
      "\t\t Output tensor shape : torch.Size([1, 128, 16, 16])\n",
      "\t\t sending C_out tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 26, 27, 28, 30, 31]) to machine 0\n",
      "\t\t sending C_out tensor([32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 47, 48, 49, 50,\n",
      "        51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 62, 63]) to machine 1\n",
      "\t\t sending C_out tensor([64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81,\n",
      "        82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95]) to machine 2\n",
      "\t\t sending C_out tensor([ 96,  97,  98,  99, 100, 101, 102, 104, 105, 106, 107, 108, 109, 110,\n",
      "        111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124,\n",
      "        125, 126, 127]) to machine 3\n",
      "\tExecuting on machine 2\n",
      "\t\t received input channels tensor([32, 35, 36, 37, 38, 40, 41, 42, 43])\n",
      "\t\t-Saving input for later...\n",
      "\t\t Output tensor shape : torch.Size([1, 128, 16, 16])\n",
      "\t\t sending C_out tensor([ 0,  1,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 19,\n",
      "        20, 21, 23, 24, 25, 26, 27, 28, 30, 31]) to machine 0\n",
      "\t\t sending C_out tensor([32, 34, 36, 37, 39, 40, 42, 43, 44, 45, 46, 47, 48, 50, 51, 52, 53, 54,\n",
      "        55, 56, 57, 58, 59, 60, 61, 62, 63]) to machine 1\n",
      "\t\t sending C_out tensor([64, 65, 66, 67, 68, 69, 70, 71, 72, 74, 77, 79, 82, 83, 86, 87, 88, 89,\n",
      "        90, 92, 93, 94, 95]) to machine 2\n",
      "\t\t sending C_out tensor([ 96,  97,  98,  99, 100, 101, 102, 103, 105, 106, 107, 108, 109, 110,\n",
      "        111, 112, 113, 114, 115, 116, 117, 118, 120, 121, 122, 124, 125, 126,\n",
      "        127]) to machine 3\n",
      "\tExecuting on machine 3\n",
      "\t\t received input channels tensor([50, 52, 54, 55, 56, 57, 63])\n",
      "\t\t-Saving input for later...\n",
      "\t\t Output tensor shape : torch.Size([1, 128, 16, 16])\n",
      "\t\t sending C_out tensor([ 0,  1,  2,  4,  6,  8,  9, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 25,\n",
      "        26, 28, 29, 31]) to machine 0\n",
      "\t\t sending C_out tensor([32, 33, 34, 35, 37, 38, 40, 41, 43, 44, 46, 47, 49, 50, 51, 53, 54, 56,\n",
      "        57, 58, 59, 61, 62, 63]) to machine 1\n",
      "\t\t sending C_out tensor([64, 65, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 79, 81, 84, 85, 89,\n",
      "        91, 92, 94, 95]) to machine 2\n",
      "\t\t sending C_out tensor([ 96,  97,  98,  99, 100, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 126, 127]) to machine 3\n",
      "Finished execution of layer 18\n",
      "\n",
      "Executing module 19: layer2.0.conv2\n",
      "\tExecuting on machine 0\n",
      "\t\t received input channels tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31])\n",
      "\t\t Output tensor shape : torch.Size([1, 128, 16, 16])\n",
      "\t\t sending C_out tensor([ 0,  1,  2,  4,  5,  7,  8,  9, 10, 11, 12, 13, 15, 16, 17, 18, 19, 20,\n",
      "        21, 23, 24, 25, 26, 27, 28, 29, 30, 31]) to machine 0\n",
      "\t\t sending C_out tensor([32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49,\n",
      "        50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]) to machine 1\n",
      "\t\t sending C_out tensor([64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81,\n",
      "        82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95]) to machine 2\n",
      "\t\t sending C_out tensor([ 96,  97,  98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109,\n",
      "        110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123,\n",
      "        124, 125, 126, 127]) to machine 3\n",
      "\tExecuting on machine 1\n",
      "\t\t received input channels tensor([32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49,\n",
      "        50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63])\n",
      "\t\t Output tensor shape : torch.Size([1, 128, 16, 16])\n",
      "\t\t sending C_out tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]) to machine 0\n",
      "\t\t sending C_out tensor([32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 46, 47, 49, 52, 53,\n",
      "        54, 55, 56, 58, 59, 60, 61, 62, 63]) to machine 1\n",
      "\t\t sending C_out tensor([64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81,\n",
      "        82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95]) to machine 2\n",
      "\t\t sending C_out tensor([ 96,  97,  98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109,\n",
      "        110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123,\n",
      "        124, 125, 126, 127]) to machine 3\n",
      "\tExecuting on machine 2\n",
      "\t\t received input channels tensor([64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81,\n",
      "        82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95])\n",
      "\t\t Output tensor shape : torch.Size([1, 128, 16, 16])\n",
      "\t\t sending C_out tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]) to machine 0\n",
      "\t\t sending C_out tensor([32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49,\n",
      "        50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]) to machine 1\n",
      "\t\t sending C_out tensor([65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 80, 81, 83, 84, 85,\n",
      "        86, 88, 89, 90, 91, 92, 94, 95]) to machine 2\n",
      "\t\t sending C_out tensor([ 96,  97,  98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109,\n",
      "        110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123,\n",
      "        124, 125, 126, 127]) to machine 3\n",
      "\tExecuting on machine 3\n",
      "\t\t received input channels tensor([ 96,  97,  98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109,\n",
      "        110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123,\n",
      "        124, 125, 126, 127])\n",
      "\t\t Output tensor shape : torch.Size([1, 128, 16, 16])\n",
      "\t\t sending C_out tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]) to machine 0\n",
      "\t\t sending C_out tensor([32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49,\n",
      "        50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]) to machine 1\n",
      "\t\t sending C_out tensor([64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81,\n",
      "        82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95]) to machine 2\n",
      "\t\t sending C_out tensor([ 96,  97,  98,  99, 100, 101, 103, 104, 108, 109, 110, 112, 113, 114,\n",
      "        115, 116, 117, 118, 119, 120, 121, 123, 124, 126, 127]) to machine 3\n",
      "Finished execution of layer 19\n",
      "\n",
      "Executing module 20: layer2.0.bn1\n",
      "\tExecuting on machine 0\n",
      "\t\t received input channels tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31])\n",
      "\t\t Output tensor shape : torch.Size([1, 128, 16, 16])\n",
      "\t\t-Applying ReLU\n",
      "\t\t sending C_out tensor([13]) to machine 0\n",
      "\tExecuting on machine 1\n",
      "\t\t received input channels tensor([32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49,\n",
      "        50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63])\n",
      "\t\t Output tensor shape : torch.Size([1, 128, 16, 16])\n",
      "\t\t-Applying ReLU\n",
      "\t\t sending C_out tensor([33, 34, 42, 44, 46, 47, 53, 56, 59, 60, 61]) to machine 1\n",
      "\tExecuting on machine 2\n",
      "\t\t received input channels tensor([64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81,\n",
      "        82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95])\n",
      "\t\t Output tensor shape : torch.Size([1, 128, 16, 16])\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 3\n",
      "\t\t received input channels tensor([ 96,  97,  98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109,\n",
      "        110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123,\n",
      "        124, 125, 126, 127])\n",
      "\t\t Output tensor shape : torch.Size([1, 128, 16, 16])\n",
      "\t\t-Applying ReLU\n",
      "\t\t sending C_out tensor([101]) to machine 3\n",
      "Finished execution of layer 20\n",
      "\n",
      "Executing module 21: layer2.0.bn2\n",
      "\tExecuting on machine 0\n",
      "\t\t received input channels tensor([13])\n",
      "\t\t Output tensor shape : torch.Size([1, 128, 16, 16])\n",
      "\t\t sending C_out tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]) to machine 0\n",
      "\tExecuting on machine 1\n",
      "\t\t received input channels tensor([33, 34, 42, 44, 46, 47, 53, 56, 59, 60, 61])\n",
      "\t\t Output tensor shape : torch.Size([1, 128, 16, 16])\n",
      "\t\t sending C_out tensor([32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49,\n",
      "        50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]) to machine 1\n",
      "\tExecuting on machine 2\n",
      "\t\t-No input sent to this machine. Skipping module\n",
      "\tExecuting on machine 3\n",
      "\t\t received input channels tensor([101])\n",
      "\t\t Output tensor shape : torch.Size([1, 128, 16, 16])\n",
      "\t\t sending C_out tensor([ 96,  97,  98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109,\n",
      "        110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123,\n",
      "        124, 125, 126, 127]) to machine 3\n",
      "Finished execution of layer 21\n",
      "\n",
      "Executing module 22: layer2.0.shortcut\n",
      "\tExecuting on machine 0\n",
      "\t\t received input channels tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31])\n",
      "\t\t-Skipping module Sequential\n",
      "\tExecuting on machine 1\n",
      "\t\t received input channels tensor([32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49,\n",
      "        50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63])\n",
      "\t\t-Skipping module Sequential\n",
      "\tExecuting on machine 2\n",
      "\t\t-No input sent to this machine. Skipping module\n",
      "\tExecuting on machine 3\n",
      "\t\t received input channels tensor([ 96,  97,  98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109,\n",
      "        110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123,\n",
      "        124, 125, 126, 127])\n",
      "\t\t-Skipping module Sequential\n",
      "Finished execution of layer 22\n",
      "\n",
      "Executing module 23: layer2.0.shortcut.0\n",
      "\tExecuting on machine 0\n",
      "\t\t received input channels tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31])\n",
      "\t\t-Saving current input. Swapping for input saved from start of block\n",
      "\t\t Output tensor shape : torch.Size([1, 128, 16, 16])\n",
      "\t\t sending C_out tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]) to machine 0\n",
      "\t\t sending C_out tensor([32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 45, 46, 47, 50, 51, 52,\n",
      "        53, 54, 57, 58, 59, 61, 62, 63]) to machine 1\n",
      "\t\t sending C_out tensor([64, 65, 66, 67, 69, 71, 72, 74, 75, 77, 78, 79, 81, 82, 83, 85, 88, 91,\n",
      "        93, 94, 95]) to machine 2\n",
      "\t\t sending C_out tensor([ 97,  98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 113, 114, 115,\n",
      "        117, 118, 119, 120, 122, 123, 125, 126, 127]) to machine 3\n",
      "\tExecuting on machine 1\n",
      "\t\t received input channels tensor([32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49,\n",
      "        50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63])\n",
      "\t\t-Saving current input. Swapping for input saved from start of block\n",
      "\t\t Output tensor shape : torch.Size([1, 128, 16, 16])\n",
      "\t\t sending C_out tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]) to machine 0\n",
      "\t\t sending C_out tensor([32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49,\n",
      "        50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]) to machine 1\n",
      "\t\t sending C_out tensor([65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82,\n",
      "        83, 84, 85, 86, 87, 88, 89, 90, 92, 93, 94]) to machine 2\n",
      "\t\t sending C_out tensor([ 96,  97,  98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109,\n",
      "        110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123,\n",
      "        124, 125, 126, 127]) to machine 3\n",
      "\tExecuting on machine 2\n",
      "\t\t-No input sent to this machine. Skipping module\n",
      "\tExecuting on machine 3\n",
      "\t\t received input channels tensor([ 96,  97,  98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109,\n",
      "        110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123,\n",
      "        124, 125, 126, 127])\n",
      "\t\t-Saving current input. Swapping for input saved from start of block\n",
      "\t\t Output tensor shape : torch.Size([1, 128, 16, 16])\n",
      "\t\t sending C_out tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 11, 12, 13, 14, 15, 17, 18, 19,\n",
      "        20, 22, 23, 25, 27, 28, 29, 30, 31]) to machine 0\n",
      "\t\t sending C_out tensor([33, 34, 35, 36, 37, 39, 41, 43, 46, 47, 48, 49, 52, 53, 54, 55, 56, 58,\n",
      "        59, 61, 62, 63]) to machine 1\n",
      "\t\t sending C_out tensor([64, 65, 66, 67, 68, 69, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82,\n",
      "        85, 86, 87, 90, 91, 92, 93, 94, 95]) to machine 2\n",
      "\t\t sending C_out tensor([ 96,  97,  98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109,\n",
      "        110, 111, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124,\n",
      "        125, 126, 127]) to machine 3\n",
      "Finished execution of layer 23\n",
      "\n",
      "Executing module 24: layer2.0.shortcut.1\n",
      "\tExecuting on machine 0\n",
      "\t\t received input channels tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31])\n",
      "\t\t Output tensor shape : torch.Size([1, 128, 16, 16])\n",
      "\t\t-adding residual\n",
      "\t\t sending C_out tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]) to machine 0\n",
      "\tExecuting on machine 1\n",
      "\t\t received input channels tensor([32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49,\n",
      "        50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63])\n",
      "\t\t Output tensor shape : torch.Size([1, 128, 16, 16])\n",
      "\t\t-adding residual\n",
      "\t\t sending C_out tensor([32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49,\n",
      "        50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]) to machine 1\n",
      "\tExecuting on machine 2\n",
      "\t\t received input channels tensor([64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81,\n",
      "        82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95])\n",
      "\t\t Output tensor shape : torch.Size([1, 128, 16, 16])\n",
      "\t\t sending C_out tensor([64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81,\n",
      "        82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95]) to machine 2\n",
      "\tExecuting on machine 3\n",
      "\t\t received input channels tensor([ 96,  97,  98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109,\n",
      "        110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123,\n",
      "        124, 125, 126, 127])\n",
      "\t\t Output tensor shape : torch.Size([1, 128, 16, 16])\n",
      "\t\t-adding residual\n",
      "\t\t sending C_out tensor([ 96,  97,  98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109,\n",
      "        110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123,\n",
      "        124, 125, 126, 127]) to machine 3\n",
      "Finished execution of layer 24\n",
      "\n",
      "Executing module 25: layer2.1\n",
      "\tExecuting on machine 0\n",
      "\t\t received input channels tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31])\n",
      "\t\t-Skipping module BasicBlock\n",
      "\tExecuting on machine 1\n",
      "\t\t received input channels tensor([32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49,\n",
      "        50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63])\n",
      "\t\t-Skipping module BasicBlock\n",
      "\tExecuting on machine 2\n",
      "\t\t received input channels tensor([64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81,\n",
      "        82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95])\n",
      "\t\t-Skipping module BasicBlock\n",
      "\tExecuting on machine 3\n",
      "\t\t received input channels tensor([ 96,  97,  98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109,\n",
      "        110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123,\n",
      "        124, 125, 126, 127])\n",
      "\t\t-Skipping module BasicBlock\n",
      "Finished execution of layer 25\n",
      "\n",
      "Executing module 26: layer2.1.conv1\n",
      "\tExecuting on machine 0\n",
      "\t\t received input channels tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31])\n",
      "\t\t Output tensor shape : torch.Size([1, 128, 16, 16])\n",
      "\t\t sending C_out tensor([ 0,  1,  2,  3,  5,  6, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22,\n",
      "        23, 24, 26, 28, 29, 30, 31]) to machine 0\n",
      "\t\t sending C_out tensor([32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49,\n",
      "        50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]) to machine 1\n",
      "\t\t sending C_out tensor([64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81,\n",
      "        82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95]) to machine 2\n",
      "\t\t sending C_out tensor([ 96,  97,  98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109,\n",
      "        110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123,\n",
      "        124, 125, 126, 127]) to machine 3\n",
      "\tExecuting on machine 1\n",
      "\t\t received input channels tensor([32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49,\n",
      "        50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63])\n",
      "\t\t Output tensor shape : torch.Size([1, 128, 16, 16])\n",
      "\t\t sending C_out tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]) to machine 0\n",
      "\t\t sending C_out tensor([32, 33, 34, 35, 36, 37, 38, 40, 41, 42, 43, 44, 45, 47, 48, 49, 50, 51,\n",
      "        53, 55, 56, 59, 60, 61, 62, 63]) to machine 1\n",
      "\t\t sending C_out tensor([64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81,\n",
      "        82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95]) to machine 2\n",
      "\t\t sending C_out tensor([ 96,  97,  98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109,\n",
      "        110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123,\n",
      "        124, 125, 126, 127]) to machine 3\n",
      "\tExecuting on machine 2\n",
      "\t\t received input channels tensor([64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81,\n",
      "        82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95])\n",
      "\t\t Output tensor shape : torch.Size([1, 128, 16, 16])\n",
      "\t\t sending C_out tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]) to machine 0\n",
      "\t\t sending C_out tensor([32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49,\n",
      "        50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]) to machine 1\n",
      "\t\t sending C_out tensor([64, 66, 67, 69, 71, 73, 74, 75, 76, 78, 81, 82, 83, 86, 89, 90, 94, 95]) to machine 2\n",
      "\t\t sending C_out tensor([ 96,  97,  98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109,\n",
      "        110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123,\n",
      "        124, 125, 126, 127]) to machine 3\n",
      "\tExecuting on machine 3\n",
      "\t\t received input channels tensor([ 96,  97,  98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109,\n",
      "        110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123,\n",
      "        124, 125, 126, 127])\n",
      "\t\t Output tensor shape : torch.Size([1, 128, 16, 16])\n",
      "\t\t sending C_out tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]) to machine 0\n",
      "\t\t sending C_out tensor([32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49,\n",
      "        50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]) to machine 1\n",
      "\t\t sending C_out tensor([64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81,\n",
      "        82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95]) to machine 2\n",
      "\t\t sending C_out tensor([ 98,  99, 102, 105, 108, 109, 110, 111, 112, 113, 116, 117, 119, 120,\n",
      "        121, 122, 124, 125, 126, 127]) to machine 3\n",
      "Finished execution of layer 26\n",
      "\n",
      "Executing module 27: layer2.1.conv2\n",
      "\tExecuting on machine 0\n",
      "\t\t received input channels tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31])\n",
      "\t\t Output tensor shape : torch.Size([1, 128, 16, 16])\n",
      "\t\t sending C_out tensor([ 0,  5,  7,  8,  9, 10, 11, 12, 15, 18, 19, 21, 23, 27, 28, 29, 30, 31]) to machine 0\n",
      "\t\t sending C_out tensor([32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49,\n",
      "        50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]) to machine 1\n",
      "\t\t sending C_out tensor([64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81,\n",
      "        82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95]) to machine 2\n",
      "\t\t sending C_out tensor([ 96,  97,  98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109,\n",
      "        110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123,\n",
      "        124, 125, 126, 127]) to machine 3\n",
      "\tExecuting on machine 1\n",
      "\t\t received input channels tensor([32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49,\n",
      "        50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63])\n",
      "\t\t Output tensor shape : torch.Size([1, 128, 16, 16])\n",
      "\t\t sending C_out tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]) to machine 0\n",
      "\t\t sending C_out tensor([32, 33, 34, 35, 36, 37, 38, 42, 46, 47, 50, 51, 52, 53, 54, 57, 58, 59,\n",
      "        60, 61]) to machine 1\n",
      "\t\t sending C_out tensor([64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81,\n",
      "        82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95]) to machine 2\n",
      "\t\t sending C_out tensor([ 96,  97,  98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109,\n",
      "        110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123,\n",
      "        124, 125, 126, 127]) to machine 3\n",
      "\tExecuting on machine 2\n",
      "\t\t received input channels tensor([64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81,\n",
      "        82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95])\n",
      "\t\t Output tensor shape : torch.Size([1, 128, 16, 16])\n",
      "\t\t sending C_out tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]) to machine 0\n",
      "\t\t sending C_out tensor([32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49,\n",
      "        50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]) to machine 1\n",
      "\t\t sending C_out tensor([69, 70, 71, 74, 75, 76, 77, 80, 83, 88, 89, 91, 92, 95]) to machine 2\n",
      "\t\t sending C_out tensor([ 96,  97,  98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109,\n",
      "        110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123,\n",
      "        124, 125, 126, 127]) to machine 3\n",
      "\tExecuting on machine 3\n",
      "\t\t received input channels tensor([ 96,  97,  98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109,\n",
      "        110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123,\n",
      "        124, 125, 126, 127])\n",
      "\t\t Output tensor shape : torch.Size([1, 128, 16, 16])\n",
      "\t\t sending C_out tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]) to machine 0\n",
      "\t\t sending C_out tensor([32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49,\n",
      "        50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]) to machine 1\n",
      "\t\t sending C_out tensor([64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81,\n",
      "        82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95]) to machine 2\n",
      "\t\t sending C_out tensor([ 96,  97,  98,  99, 100, 101, 103, 104, 107, 108, 109, 110, 112, 113,\n",
      "        114, 115, 117, 118, 119, 123, 124, 126]) to machine 3\n",
      "Finished execution of layer 27\n",
      "\n",
      "Executing module 28: layer2.1.bn1\n",
      "\tExecuting on machine 0\n",
      "\t\t received input channels tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31])\n",
      "\t\t Output tensor shape : torch.Size([1, 128, 16, 16])\n",
      "\t\t sending C_out tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]) to machine 0\n",
      "\tExecuting on machine 1\n",
      "\t\t received input channels tensor([32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49,\n",
      "        50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63])\n",
      "\t\t Output tensor shape : torch.Size([1, 128, 16, 16])\n",
      "\t\t sending C_out tensor([32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49,\n",
      "        50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]) to machine 1\n",
      "\tExecuting on machine 2\n",
      "\t\t received input channels tensor([64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81,\n",
      "        82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95])\n",
      "\t\t Output tensor shape : torch.Size([1, 128, 16, 16])\n",
      "\t\t sending C_out tensor([64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81,\n",
      "        82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95]) to machine 2\n",
      "\tExecuting on machine 3\n",
      "\t\t received input channels tensor([ 96,  97,  98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109,\n",
      "        110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123,\n",
      "        124, 125, 126, 127])\n",
      "\t\t Output tensor shape : torch.Size([1, 128, 16, 16])\n",
      "\t\t sending C_out tensor([ 96,  97,  98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109,\n",
      "        110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123,\n",
      "        124, 125, 126, 127]) to machine 3\n",
      "Finished execution of layer 28\n",
      "\n",
      "Executing module 29: layer2.1.bn2\n",
      "\tExecuting on machine 0\n",
      "\t\t received input channels tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31])\n",
      "\t\t Output tensor shape : torch.Size([1, 128, 16, 16])\n",
      "\t\t-Applying ReLU\n",
      "\t\t sending C_out tensor([10, 20, 23, 24, 26]) to machine 0\n",
      "\tExecuting on machine 1\n",
      "\t\t received input channels tensor([32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49,\n",
      "        50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63])\n",
      "\t\t Output tensor shape : torch.Size([1, 128, 16, 16])\n",
      "\t\t-Applying ReLU\n",
      "\t\t sending C_out tensor([47, 51, 59, 60, 63]) to machine 1\n",
      "\tExecuting on machine 2\n",
      "\t\t received input channels tensor([64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81,\n",
      "        82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95])\n",
      "\t\t Output tensor shape : torch.Size([1, 128, 16, 16])\n",
      "\t\t-Applying ReLU\n",
      "\t\t sending C_out tensor([69, 76, 95]) to machine 2\n",
      "\tExecuting on machine 3\n",
      "\t\t received input channels tensor([ 96,  97,  98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109,\n",
      "        110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123,\n",
      "        124, 125, 126, 127])\n",
      "\t\t Output tensor shape : torch.Size([1, 128, 16, 16])\n",
      "\t\t-Applying ReLU\n",
      "\t\t sending C_out tensor([ 96, 127]) to machine 3\n",
      "Finished execution of layer 29\n",
      "\n",
      "Executing module 30: layer2.1.shortcut\n",
      "\tExecuting on machine 0\n",
      "\t\t received input channels tensor([10, 20, 23, 24, 26])\n",
      "\t\t-Skipping module Sequential\n",
      "\tExecuting on machine 1\n",
      "\t\t received input channels tensor([47, 51, 59, 60, 63])\n",
      "\t\t-Skipping module Sequential\n",
      "\tExecuting on machine 2\n",
      "\t\t received input channels tensor([69, 76, 95])\n",
      "\t\t-Skipping module Sequential\n",
      "\tExecuting on machine 3\n",
      "\t\t received input channels tensor([ 96, 127])\n",
      "\t\t-Skipping module Sequential\n",
      "Finished execution of layer 30\n",
      "\n",
      "Executing module 31: layer3\n",
      "\tExecuting on machine 0\n",
      "\t\t received input channels tensor([10, 20, 23, 24, 26])\n",
      "\t\t-Skipping module Sequential\n",
      "\tExecuting on machine 1\n",
      "\t\t received input channels tensor([47, 51, 59, 60, 63])\n",
      "\t\t-Skipping module Sequential\n",
      "\tExecuting on machine 2\n",
      "\t\t received input channels tensor([69, 76, 95])\n",
      "\t\t-Skipping module Sequential\n",
      "\tExecuting on machine 3\n",
      "\t\t received input channels tensor([ 96, 127])\n",
      "\t\t-Skipping module Sequential\n",
      "Finished execution of layer 31\n",
      "\n",
      "Executing module 32: layer3.0\n",
      "\tExecuting on machine 0\n",
      "\t\t received input channels tensor([10, 20, 23, 24, 26])\n",
      "\t\t-Skipping module BasicBlock\n",
      "\tExecuting on machine 1\n",
      "\t\t received input channels tensor([47, 51, 59, 60, 63])\n",
      "\t\t-Skipping module BasicBlock\n",
      "\tExecuting on machine 2\n",
      "\t\t received input channels tensor([69, 76, 95])\n",
      "\t\t-Skipping module BasicBlock\n",
      "\tExecuting on machine 3\n",
      "\t\t received input channels tensor([ 96, 127])\n",
      "\t\t-Skipping module BasicBlock\n",
      "Finished execution of layer 32\n",
      "\n",
      "Executing module 33: layer3.0.conv1\n",
      "\tExecuting on machine 0\n",
      "\t\t received input channels tensor([10, 20, 23, 24, 26])\n",
      "\t\t-Saving input for later...\n",
      "\t\t Output tensor shape : torch.Size([1, 256, 8, 8])\n",
      "\t\t sending C_out tensor([ 0,  1,  2,  3,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
      "        19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36,\n",
      "        37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54,\n",
      "        55, 56, 57, 58, 59, 60, 61, 62, 63]) to machine 0\n",
      "\t\t sending C_out tensor([ 64,  65,  67,  69,  72,  73,  74,  79,  80,  81,  82,  83,  84,  85,\n",
      "         86,  89,  94,  95,  96,  98,  99, 100, 101, 106, 109, 110, 112, 115,\n",
      "        116, 119, 120, 126, 127]) to machine 1\n",
      "\t\t sending C_out tensor([130, 134, 135, 136, 137, 140, 141, 142, 144, 145, 148, 151, 152, 154,\n",
      "        155, 156, 157, 164, 166, 168, 173, 176, 177, 178, 179, 182, 183, 185,\n",
      "        187, 188, 190]) to machine 2\n",
      "\t\t sending C_out tensor([193, 195, 196, 198, 200, 201, 203, 206, 207, 209, 210, 214, 215, 216,\n",
      "        217, 218, 219, 224, 225, 227, 228, 231, 234, 235, 238, 239, 240, 241,\n",
      "        248, 250, 251, 255]) to machine 3\n",
      "\tExecuting on machine 1\n",
      "\t\t received input channels tensor([47, 51, 59, 60, 63])\n",
      "\t\t-Saving input for later...\n",
      "\t\t Output tensor shape : torch.Size([1, 256, 8, 8])\n",
      "\t\t sending C_out tensor([ 1,  2,  8, 11, 12, 16, 17, 21, 23, 24, 25, 27, 28, 29, 30, 31, 33, 34,\n",
      "        39, 40, 42, 44, 46, 49, 61]) to machine 0\n",
      "\t\t sending C_out tensor([ 64,  65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
      "         78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  92,\n",
      "         94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104, 105, 106, 107,\n",
      "        108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121,\n",
      "        122, 123, 125, 126, 127]) to machine 1\n",
      "\t\t sending C_out tensor([128, 130, 133, 135, 139, 140, 141, 142, 144, 147, 148, 149, 150, 151,\n",
      "        155, 156, 159, 165, 167, 172, 174, 175, 181, 182, 187]) to machine 2\n",
      "\t\t sending C_out tensor([193, 195, 197, 198, 199, 204, 208, 209, 210, 211, 212, 213, 214, 216,\n",
      "        218, 220, 221, 225, 228, 229, 235, 237, 239, 242, 243, 246, 250, 252]) to machine 3\n",
      "\tExecuting on machine 2\n",
      "\t\t received input channels tensor([69, 76, 95])\n",
      "\t\t-Saving input for later...\n",
      "\t\t Output tensor shape : torch.Size([1, 256, 8, 8])\n",
      "\t\t sending C_out tensor([ 1,  3,  7, 14, 16, 18, 20, 24, 25, 26, 35, 46, 47, 52, 55, 56, 58]) to machine 0\n",
      "\t\t sending C_out tensor([ 72,  74,  76,  81,  83,  86,  88,  94,  95,  98, 101, 102, 103, 107,\n",
      "        115, 116]) to machine 1\n",
      "\t\t sending C_out tensor([128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141,\n",
      "        142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
      "        156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 168, 169, 171,\n",
      "        172, 173, 174, 175, 176, 177, 178, 179, 181, 182, 183, 184, 185, 186,\n",
      "        187, 188, 189, 190, 191]) to machine 2\n",
      "\t\t sending C_out tensor([194, 199, 201, 203, 212, 217, 223, 224, 225, 226, 227, 228, 230, 231,\n",
      "        236, 237, 244, 245, 248, 251, 253, 254]) to machine 3\n",
      "\tExecuting on machine 3\n",
      "\t\t received input channels tensor([ 96, 127])\n",
      "\t\t-Saving input for later...\n",
      "\t\t Output tensor shape : torch.Size([1, 256, 8, 8])\n",
      "\t\t sending C_out tensor([21, 24, 27, 32, 47, 53, 55, 56, 61]) to machine 0\n",
      "\t\t sending C_out tensor([ 66,  68,  74,  75,  81,  84,  88,  91,  97, 101, 102, 105, 108, 114,\n",
      "        118, 124]) to machine 1\n",
      "\t\t sending C_out tensor([128, 142, 145, 159, 162, 165, 166, 167, 172, 188, 191]) to machine 2\n",
      "\t\t sending C_out tensor([192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205,\n",
      "        206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219,\n",
      "        220, 221, 222, 224, 225, 226, 227, 229, 230, 231, 232, 234, 235, 236,\n",
      "        238, 239, 240, 241, 245, 247, 248, 249, 250, 251, 252, 253, 254, 255]) to machine 3\n",
      "Finished execution of layer 33\n",
      "\n",
      "Executing module 34: layer3.0.conv2\n",
      "\tExecuting on machine 0\n",
      "\t\t received input channels tensor([ 0,  1,  2,  3,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
      "        19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36,\n",
      "        37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54,\n",
      "        55, 56, 57, 58, 59, 60, 61, 62, 63])\n",
      "\t\t Output tensor shape : torch.Size([1, 256, 8, 8])\n",
      "\t\t sending C_out tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
      "        54, 55, 56, 57, 58, 59, 60, 61, 62, 63]) to machine 0\n",
      "\t\t sending C_out tensor([ 64,  65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
      "         78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,\n",
      "         92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104, 105,\n",
      "        106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119,\n",
      "        120, 121, 122, 123, 124, 125, 126, 127]) to machine 1\n",
      "\t\t sending C_out tensor([128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141,\n",
      "        142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
      "        156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169,\n",
      "        170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183,\n",
      "        184, 185, 186, 187, 188, 189, 190, 191]) to machine 2\n",
      "\t\t sending C_out tensor([192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205,\n",
      "        206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219,\n",
      "        220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
      "        234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247,\n",
      "        248, 249, 250, 251, 252, 253, 254, 255]) to machine 3\n",
      "\tExecuting on machine 1\n",
      "\t\t received input channels tensor([ 64,  65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
      "         78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,\n",
      "         92,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104, 105, 106,\n",
      "        107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120,\n",
      "        121, 122, 123, 124, 125, 126, 127])\n",
      "\t\t Output tensor shape : torch.Size([1, 256, 8, 8])\n",
      "\t\t sending C_out tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
      "        54, 55, 56, 57, 58, 59, 60, 61, 62, 63]) to machine 0\n",
      "\t\t sending C_out tensor([ 64,  65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
      "         78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,\n",
      "         92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104, 105,\n",
      "        106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119,\n",
      "        120, 121, 122, 123, 124, 125, 126, 127]) to machine 1\n",
      "\t\t sending C_out tensor([128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141,\n",
      "        142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
      "        156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169,\n",
      "        170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183,\n",
      "        184, 185, 186, 187, 188, 189, 190, 191]) to machine 2\n",
      "\t\t sending C_out tensor([192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205,\n",
      "        206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219,\n",
      "        220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
      "        234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247,\n",
      "        248, 249, 250, 251, 252, 253, 254, 255]) to machine 3\n",
      "\tExecuting on machine 2\n",
      "\t\t received input channels tensor([128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141,\n",
      "        142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
      "        156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169,\n",
      "        171, 172, 173, 174, 175, 176, 177, 178, 179, 181, 182, 183, 184, 185,\n",
      "        186, 187, 188, 189, 190, 191])\n",
      "\t\t Output tensor shape : torch.Size([1, 256, 8, 8])\n",
      "\t\t sending C_out tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
      "        54, 55, 56, 57, 58, 59, 60, 61, 62, 63]) to machine 0\n",
      "\t\t sending C_out tensor([ 64,  65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
      "         78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,\n",
      "         92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104, 105,\n",
      "        106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119,\n",
      "        120, 121, 122, 123, 124, 125, 126, 127]) to machine 1\n",
      "\t\t sending C_out tensor([128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141,\n",
      "        142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
      "        156, 157, 158, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170,\n",
      "        171, 172, 173, 174, 175, 176, 177, 178, 180, 181, 182, 183, 184, 185,\n",
      "        187, 188, 189, 190, 191]) to machine 2\n",
      "\t\t sending C_out tensor([192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205,\n",
      "        206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219,\n",
      "        220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
      "        234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247,\n",
      "        248, 249, 250, 251, 252, 253, 254, 255]) to machine 3\n",
      "\tExecuting on machine 3\n",
      "\t\t received input channels tensor([192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205,\n",
      "        206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219,\n",
      "        220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 234,\n",
      "        235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248,\n",
      "        249, 250, 251, 252, 253, 254, 255])\n",
      "\t\t Output tensor shape : torch.Size([1, 256, 8, 8])\n",
      "\t\t sending C_out tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
      "        54, 55, 56, 57, 58, 59, 60, 61, 62, 63]) to machine 0\n",
      "\t\t sending C_out tensor([ 64,  65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
      "         78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,\n",
      "         92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104, 105,\n",
      "        106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119,\n",
      "        120, 121, 122, 123, 124, 125, 126, 127]) to machine 1\n",
      "\t\t sending C_out tensor([128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141,\n",
      "        142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
      "        156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169,\n",
      "        170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183,\n",
      "        184, 185, 186, 187, 188, 189, 190, 191]) to machine 2\n",
      "\t\t sending C_out tensor([193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206,\n",
      "        207, 208, 209, 210, 211, 212, 213, 215, 216, 217, 218, 219, 220, 221,\n",
      "        222, 223, 224, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236,\n",
      "        237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250,\n",
      "        252, 253, 254, 255]) to machine 3\n",
      "Finished execution of layer 34\n",
      "\n",
      "Executing module 35: layer3.0.bn1\n",
      "\tExecuting on machine 0\n",
      "\t\t received input channels tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
      "        54, 55, 56, 57, 58, 59, 60, 61, 62, 63])\n",
      "\t\t Output tensor shape : torch.Size([1, 256, 8, 8])\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 1\n",
      "\t\t received input channels tensor([ 64,  65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
      "         78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,\n",
      "         92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104, 105,\n",
      "        106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119,\n",
      "        120, 121, 122, 123, 124, 125, 126, 127])\n",
      "\t\t Output tensor shape : torch.Size([1, 256, 8, 8])\n",
      "\t\t-Applying ReLU\n",
      "\t\t sending C_out tensor([ 64,  68,  74,  75,  81,  84,  85,  87,  90,  91,  92,  93,  94,  95,\n",
      "         96,  99, 101, 103, 106, 108, 111, 112, 114, 115, 116, 117, 119, 122,\n",
      "        124, 127]) to machine 1\n",
      "\tExecuting on machine 2\n",
      "\t\t received input channels tensor([128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141,\n",
      "        142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
      "        156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169,\n",
      "        170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183,\n",
      "        184, 185, 186, 187, 188, 189, 190, 191])\n",
      "\t\t Output tensor shape : torch.Size([1, 256, 8, 8])\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 3\n",
      "\t\t received input channels tensor([192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205,\n",
      "        206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219,\n",
      "        220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
      "        234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247,\n",
      "        248, 249, 250, 251, 252, 253, 254, 255])\n",
      "\t\t Output tensor shape : torch.Size([1, 256, 8, 8])\n",
      "\t\t-Applying ReLU\n",
      "Finished execution of layer 35\n",
      "\n",
      "Executing module 36: layer3.0.bn2\n",
      "\tExecuting on machine 0\n",
      "\t\t-No input sent to this machine. Skipping module\n",
      "\tExecuting on machine 1\n",
      "\t\t received input channels tensor([ 64,  68,  74,  75,  81,  84,  85,  87,  90,  91,  92,  93,  94,  95,\n",
      "         96,  99, 101, 103, 106, 108, 111, 112, 114, 115, 116, 117, 119, 122,\n",
      "        124, 127])\n",
      "\t\t Output tensor shape : torch.Size([1, 256, 8, 8])\n",
      "\t\t sending C_out tensor([ 64,  65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
      "         78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,\n",
      "         92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104, 105,\n",
      "        106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119,\n",
      "        120, 121, 122, 123, 124, 125, 126, 127]) to machine 1\n",
      "\tExecuting on machine 2\n",
      "\t\t-No input sent to this machine. Skipping module\n",
      "\tExecuting on machine 3\n",
      "\t\t-No input sent to this machine. Skipping module\n",
      "Finished execution of layer 36\n",
      "\n",
      "Executing module 37: layer3.0.shortcut\n",
      "\tExecuting on machine 0\n",
      "\t\t-No input sent to this machine. Skipping module\n",
      "\tExecuting on machine 1\n",
      "\t\t received input channels tensor([ 64,  65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
      "         78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,\n",
      "         92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104, 105,\n",
      "        106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119,\n",
      "        120, 121, 122, 123, 124, 125, 126, 127])\n",
      "\t\t-Skipping module Sequential\n",
      "\tExecuting on machine 2\n",
      "\t\t-No input sent to this machine. Skipping module\n",
      "\tExecuting on machine 3\n",
      "\t\t-No input sent to this machine. Skipping module\n",
      "Finished execution of layer 37\n",
      "\n",
      "Executing module 38: layer3.0.shortcut.0\n",
      "\tExecuting on machine 0\n",
      "\t\t-No input sent to this machine. Skipping module\n",
      "\tExecuting on machine 1\n",
      "\t\t received input channels tensor([ 64,  65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
      "         78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,\n",
      "         92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104, 105,\n",
      "        106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119,\n",
      "        120, 121, 122, 123, 124, 125, 126, 127])\n",
      "\t\t-Saving current input. Swapping for input saved from start of block\n",
      "\t\t Output tensor shape : torch.Size([1, 256, 8, 8])\n",
      "\t\t sending C_out tensor([ 1,  7,  8, 11, 14, 15, 17, 18, 19, 21, 23, 24, 27, 28, 31, 34, 37, 43,\n",
      "        44, 45, 47, 54, 58, 63]) to machine 0\n",
      "\t\t sending C_out tensor([ 64,  65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
      "         78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,\n",
      "         92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104, 105,\n",
      "        106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119,\n",
      "        120, 121, 122, 123, 124, 125, 126, 127]) to machine 1\n",
      "\t\t sending C_out tensor([132, 133, 134, 137, 138, 139, 140, 141, 143, 145, 147, 150, 153, 154,\n",
      "        157, 158, 159, 160, 165, 172, 174, 176, 178, 180, 181, 187]) to machine 2\n",
      "\t\t sending C_out tensor([192, 194, 196, 201, 203, 204, 205, 206, 211, 215, 217, 221, 222, 225,\n",
      "        226, 228, 231, 236, 238, 239, 247, 248]) to machine 3\n",
      "\tExecuting on machine 2\n",
      "\t\t-No input sent to this machine. Skipping module\n",
      "\tExecuting on machine 3\n",
      "\t\t-No input sent to this machine. Skipping module\n",
      "Finished execution of layer 38\n",
      "\n",
      "Executing module 39: layer3.0.shortcut.1\n",
      "\tExecuting on machine 0\n",
      "\t\t received input channels tensor([ 1,  7,  8, 11, 14, 15, 17, 18, 19, 21, 23, 24, 27, 28, 31, 34, 37, 43,\n",
      "        44, 45, 47, 54, 58, 63])\n",
      "\t\t Output tensor shape : torch.Size([1, 256, 8, 8])\n",
      "\t\t sending C_out tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
      "        54, 55, 56, 57, 58, 59, 60, 61, 62, 63]) to machine 0\n",
      "\tExecuting on machine 1\n",
      "\t\t received input channels tensor([ 64,  65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
      "         78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,\n",
      "         92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104, 105,\n",
      "        106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119,\n",
      "        120, 121, 122, 123, 124, 125, 126, 127])\n",
      "\t\t Output tensor shape : torch.Size([1, 256, 8, 8])\n",
      "\t\t-adding residual\n",
      "\t\t sending C_out tensor([ 64,  65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
      "         78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,\n",
      "         92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104, 105,\n",
      "        106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119,\n",
      "        120, 121, 122, 123, 124, 125, 126, 127]) to machine 1\n",
      "\tExecuting on machine 2\n",
      "\t\t received input channels tensor([132, 133, 134, 137, 138, 139, 140, 141, 143, 145, 147, 150, 153, 154,\n",
      "        157, 158, 159, 160, 165, 172, 174, 176, 178, 180, 181, 187])\n",
      "\t\t Output tensor shape : torch.Size([1, 256, 8, 8])\n",
      "\t\t sending C_out tensor([128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141,\n",
      "        142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
      "        156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169,\n",
      "        170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183,\n",
      "        184, 185, 186, 187, 188, 189, 190, 191]) to machine 2\n",
      "\tExecuting on machine 3\n",
      "\t\t received input channels tensor([192, 194, 196, 201, 203, 204, 205, 206, 211, 215, 217, 221, 222, 225,\n",
      "        226, 228, 231, 236, 238, 239, 247, 248])\n",
      "\t\t Output tensor shape : torch.Size([1, 256, 8, 8])\n",
      "\t\t sending C_out tensor([192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205,\n",
      "        206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219,\n",
      "        220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
      "        234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247,\n",
      "        248, 249, 250, 251, 252, 253, 254, 255]) to machine 3\n",
      "Finished execution of layer 39\n",
      "\n",
      "Executing module 40: layer3.1\n",
      "\tExecuting on machine 0\n",
      "\t\t received input channels tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
      "        54, 55, 56, 57, 58, 59, 60, 61, 62, 63])\n",
      "\t\t-Skipping module BasicBlock\n",
      "\tExecuting on machine 1\n",
      "\t\t received input channels tensor([ 64,  65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
      "         78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,\n",
      "         92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104, 105,\n",
      "        106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119,\n",
      "        120, 121, 122, 123, 124, 125, 126, 127])\n",
      "\t\t-Skipping module BasicBlock\n",
      "\tExecuting on machine 2\n",
      "\t\t received input channels tensor([128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141,\n",
      "        142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
      "        156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169,\n",
      "        170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183,\n",
      "        184, 185, 186, 187, 188, 189, 190, 191])\n",
      "\t\t-Skipping module BasicBlock\n",
      "\tExecuting on machine 3\n",
      "\t\t received input channels tensor([192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205,\n",
      "        206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219,\n",
      "        220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
      "        234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247,\n",
      "        248, 249, 250, 251, 252, 253, 254, 255])\n",
      "\t\t-Skipping module BasicBlock\n",
      "Finished execution of layer 40\n",
      "\n",
      "Executing module 41: layer3.1.conv1\n",
      "\tExecuting on machine 0\n",
      "\t\t received input channels tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
      "        54, 55, 56, 57, 58, 59, 60, 61, 62, 63])\n",
      "\t\t Output tensor shape : torch.Size([1, 256, 8, 8])\n",
      "\t\t sending C_out tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 16, 17, 18,\n",
      "        19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37,\n",
      "        38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55,\n",
      "        56, 57, 58, 59, 60, 61, 62, 63]) to machine 0\n",
      "\t\t sending C_out tensor([ 64,  65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
      "         78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,\n",
      "         92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104, 105,\n",
      "        106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119,\n",
      "        120, 121, 122, 123, 124, 125, 126, 127]) to machine 1\n",
      "\t\t sending C_out tensor([128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141,\n",
      "        142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
      "        156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169,\n",
      "        170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183,\n",
      "        184, 185, 186, 187, 188, 189, 190, 191]) to machine 2\n",
      "\t\t sending C_out tensor([192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205,\n",
      "        206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219,\n",
      "        220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
      "        234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247,\n",
      "        248, 249, 250, 251, 252, 253, 254, 255]) to machine 3\n",
      "\tExecuting on machine 1\n",
      "\t\t received input channels tensor([ 64,  65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
      "         78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,\n",
      "         92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104, 105,\n",
      "        106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119,\n",
      "        120, 121, 122, 123, 124, 125, 126, 127])\n",
      "\t\t Output tensor shape : torch.Size([1, 256, 8, 8])\n",
      "\t\t sending C_out tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
      "        54, 55, 56, 57, 58, 59, 60, 61, 62, 63]) to machine 0\n",
      "\t\t sending C_out tensor([ 64,  65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
      "         78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,\n",
      "         92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104, 105,\n",
      "        106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119,\n",
      "        120, 121, 122, 123, 124, 125, 127]) to machine 1\n",
      "\t\t sending C_out tensor([128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141,\n",
      "        142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
      "        156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169,\n",
      "        170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183,\n",
      "        184, 185, 186, 187, 188, 189, 190, 191]) to machine 2\n",
      "\t\t sending C_out tensor([192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205,\n",
      "        206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219,\n",
      "        220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
      "        234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247,\n",
      "        248, 249, 250, 251, 252, 253, 254, 255]) to machine 3\n",
      "\tExecuting on machine 2\n",
      "\t\t received input channels tensor([128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141,\n",
      "        142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
      "        156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169,\n",
      "        170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183,\n",
      "        184, 185, 186, 187, 188, 189, 190, 191])\n",
      "\t\t Output tensor shape : torch.Size([1, 256, 8, 8])\n",
      "\t\t sending C_out tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
      "        54, 55, 56, 57, 58, 59, 60, 61, 62, 63]) to machine 0\n",
      "\t\t sending C_out tensor([ 64,  65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
      "         78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,\n",
      "         92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104, 105,\n",
      "        106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119,\n",
      "        120, 121, 122, 123, 124, 125, 126, 127]) to machine 1\n",
      "\t\t sending C_out tensor([128, 129, 130, 131, 132, 133, 134, 135, 136, 138, 139, 140, 141, 142,\n",
      "        143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 154, 155, 157, 158,\n",
      "        159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172,\n",
      "        173, 174, 175, 177, 178, 180, 181, 182, 183, 184, 185, 186, 187, 188,\n",
      "        189, 190]) to machine 2\n",
      "\t\t sending C_out tensor([192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205,\n",
      "        206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219,\n",
      "        220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
      "        234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247,\n",
      "        248, 249, 250, 251, 252, 253, 254, 255]) to machine 3\n",
      "\tExecuting on machine 3\n",
      "\t\t received input channels tensor([192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205,\n",
      "        206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219,\n",
      "        220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
      "        234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247,\n",
      "        248, 249, 250, 251, 252, 253, 254, 255])\n",
      "\t\t Output tensor shape : torch.Size([1, 256, 8, 8])\n",
      "\t\t sending C_out tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
      "        54, 55, 56, 57, 58, 59, 60, 61, 62, 63]) to machine 0\n",
      "\t\t sending C_out tensor([ 64,  65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
      "         78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,\n",
      "         92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104, 105,\n",
      "        106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119,\n",
      "        120, 121, 122, 123, 124, 125, 126, 127]) to machine 1\n",
      "\t\t sending C_out tensor([128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141,\n",
      "        142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
      "        156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169,\n",
      "        170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183,\n",
      "        184, 185, 186, 187, 188, 189, 190, 191]) to machine 2\n",
      "\t\t sending C_out tensor([192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205,\n",
      "        206, 208, 209, 211, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222,\n",
      "        223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236,\n",
      "        237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250,\n",
      "        251, 252, 253, 254, 255]) to machine 3\n",
      "Finished execution of layer 41\n",
      "\n",
      "Executing module 42: layer3.1.conv2\n",
      "\tExecuting on machine 0\n",
      "\t\t received input channels tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
      "        54, 55, 56, 57, 58, 59, 60, 61, 62, 63])\n",
      "\t\t Output tensor shape : torch.Size([1, 256, 8, 8])\n",
      "\t\t sending C_out tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "        36, 37, 38, 39, 40, 41, 42, 43, 45, 46, 47, 49, 50, 51, 52, 53, 54, 55,\n",
      "        56, 57, 58, 59, 60, 61, 62, 63]) to machine 0\n",
      "\t\t sending C_out tensor([ 64,  65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
      "         78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,\n",
      "         92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104, 105,\n",
      "        106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119,\n",
      "        120, 121, 122, 123, 124, 125, 126, 127]) to machine 1\n",
      "\t\t sending C_out tensor([128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141,\n",
      "        142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
      "        156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169,\n",
      "        170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183,\n",
      "        184, 185, 186, 187, 188, 189, 190, 191]) to machine 2\n",
      "\t\t sending C_out tensor([192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205,\n",
      "        206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219,\n",
      "        220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
      "        234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247,\n",
      "        248, 249, 250, 251, 252, 253, 254, 255]) to machine 3\n",
      "\tExecuting on machine 1\n",
      "\t\t received input channels tensor([ 64,  65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
      "         78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,\n",
      "         92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104, 105,\n",
      "        106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119,\n",
      "        120, 121, 122, 123, 124, 125, 126, 127])\n",
      "\t\t Output tensor shape : torch.Size([1, 256, 8, 8])\n",
      "\t\t sending C_out tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
      "        54, 55, 56, 57, 58, 59, 60, 61, 62, 63]) to machine 0\n",
      "\t\t sending C_out tensor([ 64,  65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
      "         78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,\n",
      "         92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104, 105,\n",
      "        106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119,\n",
      "        120, 121, 122, 123, 124, 125, 126, 127]) to machine 1\n",
      "\t\t sending C_out tensor([128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141,\n",
      "        142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
      "        156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169,\n",
      "        170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183,\n",
      "        184, 185, 186, 187, 188, 189, 190, 191]) to machine 2\n",
      "\t\t sending C_out tensor([192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205,\n",
      "        206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219,\n",
      "        220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
      "        234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247,\n",
      "        248, 249, 250, 251, 252, 253, 254, 255]) to machine 3\n",
      "\tExecuting on machine 2\n",
      "\t\t received input channels tensor([128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141,\n",
      "        142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
      "        156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169,\n",
      "        170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183,\n",
      "        184, 185, 186, 187, 188, 189, 190, 191])\n",
      "\t\t Output tensor shape : torch.Size([1, 256, 8, 8])\n",
      "\t\t sending C_out tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
      "        54, 55, 56, 57, 58, 59, 60, 61, 62, 63]) to machine 0\n",
      "\t\t sending C_out tensor([ 64,  65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
      "         78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,\n",
      "         92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104, 105,\n",
      "        106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119,\n",
      "        120, 121, 122, 123, 124, 125, 126, 127]) to machine 1\n",
      "\t\t sending C_out tensor([128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141,\n",
      "        142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
      "        156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169,\n",
      "        170, 171, 172, 173, 174, 176, 177, 178, 179, 180, 181, 182, 183, 184,\n",
      "        185, 186, 187, 188, 189, 190, 191]) to machine 2\n",
      "\t\t sending C_out tensor([192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205,\n",
      "        206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219,\n",
      "        220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
      "        234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247,\n",
      "        248, 249, 250, 251, 252, 253, 254, 255]) to machine 3\n",
      "\tExecuting on machine 3\n",
      "\t\t received input channels tensor([192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205,\n",
      "        206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219,\n",
      "        220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
      "        234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247,\n",
      "        248, 249, 250, 251, 252, 253, 254, 255])\n",
      "\t\t Output tensor shape : torch.Size([1, 256, 8, 8])\n",
      "\t\t sending C_out tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
      "        54, 55, 56, 57, 58, 59, 60, 61, 62, 63]) to machine 0\n",
      "\t\t sending C_out tensor([ 64,  65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
      "         78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,\n",
      "         92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104, 105,\n",
      "        106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119,\n",
      "        120, 121, 122, 123, 124, 125, 126, 127]) to machine 1\n",
      "\t\t sending C_out tensor([128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141,\n",
      "        142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
      "        156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169,\n",
      "        170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183,\n",
      "        184, 185, 186, 187, 188, 189, 190, 191]) to machine 2\n",
      "\t\t sending C_out tensor([193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206,\n",
      "        207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220,\n",
      "        221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234,\n",
      "        235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 246, 247, 248, 249,\n",
      "        250, 251, 252, 253, 254, 255]) to machine 3\n",
      "Finished execution of layer 42\n",
      "\n",
      "Executing module 43: layer3.1.bn1\n",
      "\tExecuting on machine 0\n",
      "\t\t received input channels tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
      "        54, 55, 56, 57, 58, 59, 60, 61, 62, 63])\n",
      "\t\t Output tensor shape : torch.Size([1, 256, 8, 8])\n",
      "\t\t sending C_out tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
      "        54, 55, 56, 57, 58, 59, 60, 61, 62, 63]) to machine 0\n",
      "\tExecuting on machine 1\n",
      "\t\t received input channels tensor([ 64,  65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
      "         78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,\n",
      "         92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104, 105,\n",
      "        106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119,\n",
      "        120, 121, 122, 123, 124, 125, 126, 127])\n",
      "\t\t Output tensor shape : torch.Size([1, 256, 8, 8])\n",
      "\t\t sending C_out tensor([ 64,  65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
      "         78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,\n",
      "         92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104, 105,\n",
      "        106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119,\n",
      "        120, 121, 122, 123, 124, 125, 126, 127]) to machine 1\n",
      "\tExecuting on machine 2\n",
      "\t\t received input channels tensor([128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141,\n",
      "        142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
      "        156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169,\n",
      "        170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183,\n",
      "        184, 185, 186, 187, 188, 189, 190, 191])\n",
      "\t\t Output tensor shape : torch.Size([1, 256, 8, 8])\n",
      "\t\t sending C_out tensor([128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141,\n",
      "        142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
      "        156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169,\n",
      "        170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183,\n",
      "        184, 185, 186, 187, 188, 189, 190, 191]) to machine 2\n",
      "\tExecuting on machine 3\n",
      "\t\t received input channels tensor([192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205,\n",
      "        206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219,\n",
      "        220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
      "        234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247,\n",
      "        248, 249, 250, 251, 252, 253, 254, 255])\n",
      "\t\t Output tensor shape : torch.Size([1, 256, 8, 8])\n",
      "\t\t sending C_out tensor([192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205,\n",
      "        206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219,\n",
      "        220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
      "        234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247,\n",
      "        248, 249, 250, 251, 252, 253, 254, 255]) to machine 3\n",
      "Finished execution of layer 43\n",
      "\n",
      "Executing module 44: layer3.1.bn2\n",
      "\tExecuting on machine 0\n",
      "\t\t received input channels tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
      "        54, 55, 56, 57, 58, 59, 60, 61, 62, 63])\n",
      "\t\t Output tensor shape : torch.Size([1, 256, 8, 8])\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 1\n",
      "\t\t received input channels tensor([ 64,  65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
      "         78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,\n",
      "         92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104, 105,\n",
      "        106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119,\n",
      "        120, 121, 122, 123, 124, 125, 126, 127])\n",
      "\t\t Output tensor shape : torch.Size([1, 256, 8, 8])\n",
      "\t\t-Applying ReLU\n",
      "\t\t sending C_out tensor([106]) to machine 1\n",
      "\tExecuting on machine 2\n",
      "\t\t received input channels tensor([128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141,\n",
      "        142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
      "        156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169,\n",
      "        170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183,\n",
      "        184, 185, 186, 187, 188, 189, 190, 191])\n",
      "\t\t Output tensor shape : torch.Size([1, 256, 8, 8])\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 3\n",
      "\t\t received input channels tensor([192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205,\n",
      "        206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219,\n",
      "        220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
      "        234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247,\n",
      "        248, 249, 250, 251, 252, 253, 254, 255])\n",
      "\t\t Output tensor shape : torch.Size([1, 256, 8, 8])\n",
      "\t\t-Applying ReLU\n",
      "Finished execution of layer 44\n",
      "\n",
      "Executing module 45: layer3.1.shortcut\n",
      "\tExecuting on machine 0\n",
      "\t\t-No input sent to this machine. Skipping module\n",
      "\tExecuting on machine 1\n",
      "\t\t received input channels tensor([106])\n",
      "\t\t-Skipping module Sequential\n",
      "\tExecuting on machine 2\n",
      "\t\t-No input sent to this machine. Skipping module\n",
      "\tExecuting on machine 3\n",
      "\t\t-No input sent to this machine. Skipping module\n",
      "Finished execution of layer 45\n",
      "\n",
      "Executing module 46: layer4\n",
      "\tExecuting on machine 0\n",
      "\t\t-No input sent to this machine. Skipping module\n",
      "\tExecuting on machine 1\n",
      "\t\t received input channels tensor([106])\n",
      "\t\t-Skipping module Sequential\n",
      "\tExecuting on machine 2\n",
      "\t\t-No input sent to this machine. Skipping module\n",
      "\tExecuting on machine 3\n",
      "\t\t-No input sent to this machine. Skipping module\n",
      "Finished execution of layer 46\n",
      "\n",
      "Executing module 47: layer4.0\n",
      "\tExecuting on machine 0\n",
      "\t\t-No input sent to this machine. Skipping module\n",
      "\tExecuting on machine 1\n",
      "\t\t received input channels tensor([106])\n",
      "\t\t-Skipping module BasicBlock\n",
      "\tExecuting on machine 2\n",
      "\t\t-No input sent to this machine. Skipping module\n",
      "\tExecuting on machine 3\n",
      "\t\t-No input sent to this machine. Skipping module\n",
      "Finished execution of layer 47\n",
      "\n",
      "Executing module 48: layer4.0.conv1\n",
      "\tExecuting on machine 0\n",
      "\t\t-No input sent to this machine. Skipping module\n",
      "\tExecuting on machine 1\n",
      "\t\t received input channels tensor([106])\n",
      "\t\t-Saving input for later...\n",
      "\t\t Output tensor shape : torch.Size([1, 512, 4, 4])\n",
      "\t\t sending C_out tensor([128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141,\n",
      "        142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
      "        156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169,\n",
      "        170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183,\n",
      "        184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197,\n",
      "        198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211,\n",
      "        212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225,\n",
      "        226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239,\n",
      "        240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253,\n",
      "        254, 255]) to machine 1\n",
      "\tExecuting on machine 2\n",
      "\t\t-No input sent to this machine. Skipping module\n",
      "\tExecuting on machine 3\n",
      "\t\t-No input sent to this machine. Skipping module\n",
      "Finished execution of layer 48\n",
      "\n",
      "Executing module 49: layer4.0.conv2\n",
      "\tExecuting on machine 0\n",
      "\t\t-No input sent to this machine. Skipping module\n",
      "\tExecuting on machine 1\n",
      "\t\t received input channels tensor([128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141,\n",
      "        142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
      "        156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169,\n",
      "        170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183,\n",
      "        184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197,\n",
      "        198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211,\n",
      "        212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225,\n",
      "        226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239,\n",
      "        240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253,\n",
      "        254, 255])\n",
      "\t\t Output tensor shape : torch.Size([1, 512, 4, 4])\n",
      "\t\t sending C_out tensor([  0,   2,   7,  10,  12,  16,  19,  21,  23,  28,  30,  31,  32,  34,\n",
      "         37,  48,  49,  51,  52,  55,  58,  59,  62,  63,  68,  71,  75,  77,\n",
      "         79,  80,  81,  87,  88,  90,  97, 101, 104, 106, 108, 111, 115, 117,\n",
      "        125, 126, 127]) to machine 0\n",
      "\t\t sending C_out tensor([128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141,\n",
      "        142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
      "        156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169,\n",
      "        170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183,\n",
      "        184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197,\n",
      "        198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211,\n",
      "        212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225,\n",
      "        226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239,\n",
      "        240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253,\n",
      "        254, 255]) to machine 1\n",
      "\t\t sending C_out tensor([256, 257, 259, 265, 267, 270, 271, 272, 277, 281, 285, 286, 290, 291,\n",
      "        293, 295, 296, 299, 300, 305, 306, 308, 310, 311, 312, 313, 318, 321,\n",
      "        323, 325, 326, 332, 337, 342, 346, 349, 350, 356, 357, 359, 361, 363,\n",
      "        364, 367, 368, 369, 370, 374, 375, 376, 383]) to machine 2\n",
      "\t\t sending C_out tensor([387, 389, 403, 408, 419, 420, 422, 424, 429, 431, 432, 433, 435, 437,\n",
      "        438, 439, 440, 442, 444, 445, 446, 447, 448, 449, 450, 454, 456, 457,\n",
      "        462, 463, 464, 466, 468, 470, 471, 477, 478, 486, 490, 492, 493, 495,\n",
      "        499, 500, 501, 502, 506, 507, 510]) to machine 3\n",
      "\tExecuting on machine 2\n",
      "\t\t-No input sent to this machine. Skipping module\n",
      "\tExecuting on machine 3\n",
      "\t\t-No input sent to this machine. Skipping module\n",
      "Finished execution of layer 49\n",
      "\n",
      "Executing module 50: layer4.0.bn1\n",
      "\tExecuting on machine 0\n",
      "\t\t received input channels tensor([  0,   2,   7,  10,  12,  16,  19,  21,  23,  28,  30,  31,  32,  34,\n",
      "         37,  48,  49,  51,  52,  55,  58,  59,  62,  63,  68,  71,  75,  77,\n",
      "         79,  80,  81,  87,  88,  90,  97, 101, 104, 106, 108, 111, 115, 117,\n",
      "        125, 126, 127])\n",
      "\t\t Output tensor shape : torch.Size([1, 512, 4, 4])\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 1\n",
      "\t\t received input channels tensor([128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141,\n",
      "        142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
      "        156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169,\n",
      "        170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183,\n",
      "        184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197,\n",
      "        198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211,\n",
      "        212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225,\n",
      "        226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239,\n",
      "        240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253,\n",
      "        254, 255])\n",
      "\t\t Output tensor shape : torch.Size([1, 512, 4, 4])\n",
      "\t\t-Applying ReLU\n",
      "\t\t sending C_out tensor([128, 129, 134, 136, 143, 150, 152, 158, 163, 166, 168, 170, 171, 172,\n",
      "        173, 175, 178, 181, 184, 187, 191, 193, 194, 200, 201, 203, 204, 205,\n",
      "        209, 213, 214, 216, 222, 227, 230, 233, 235, 237, 242, 245, 251, 255]) to machine 1\n",
      "\tExecuting on machine 2\n",
      "\t\t received input channels tensor([256, 257, 259, 265, 267, 270, 271, 272, 277, 281, 285, 286, 290, 291,\n",
      "        293, 295, 296, 299, 300, 305, 306, 308, 310, 311, 312, 313, 318, 321,\n",
      "        323, 325, 326, 332, 337, 342, 346, 349, 350, 356, 357, 359, 361, 363,\n",
      "        364, 367, 368, 369, 370, 374, 375, 376, 383])\n",
      "\t\t Output tensor shape : torch.Size([1, 512, 4, 4])\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 3\n",
      "\t\t received input channels tensor([387, 389, 403, 408, 419, 420, 422, 424, 429, 431, 432, 433, 435, 437,\n",
      "        438, 439, 440, 442, 444, 445, 446, 447, 448, 449, 450, 454, 456, 457,\n",
      "        462, 463, 464, 466, 468, 470, 471, 477, 478, 486, 490, 492, 493, 495,\n",
      "        499, 500, 501, 502, 506, 507, 510])\n",
      "\t\t Output tensor shape : torch.Size([1, 512, 4, 4])\n",
      "\t\t-Applying ReLU\n",
      "Finished execution of layer 50\n",
      "\n",
      "Executing module 51: layer4.0.bn2\n",
      "\tExecuting on machine 0\n",
      "\t\t-No input sent to this machine. Skipping module\n",
      "\tExecuting on machine 1\n",
      "\t\t received input channels tensor([128, 129, 134, 136, 143, 150, 152, 158, 163, 166, 168, 170, 171, 172,\n",
      "        173, 175, 178, 181, 184, 187, 191, 193, 194, 200, 201, 203, 204, 205,\n",
      "        209, 213, 214, 216, 222, 227, 230, 233, 235, 237, 242, 245, 251, 255])\n",
      "\t\t Output tensor shape : torch.Size([1, 512, 4, 4])\n",
      "\t\t sending C_out tensor([128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141,\n",
      "        142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
      "        156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169,\n",
      "        170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183,\n",
      "        184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197,\n",
      "        198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211,\n",
      "        212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225,\n",
      "        226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239,\n",
      "        240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253,\n",
      "        254, 255]) to machine 1\n",
      "\tExecuting on machine 2\n",
      "\t\t-No input sent to this machine. Skipping module\n",
      "\tExecuting on machine 3\n",
      "\t\t-No input sent to this machine. Skipping module\n",
      "Finished execution of layer 51\n",
      "\n",
      "Executing module 52: layer4.0.shortcut\n",
      "\tExecuting on machine 0\n",
      "\t\t-No input sent to this machine. Skipping module\n",
      "\tExecuting on machine 1\n",
      "\t\t received input channels tensor([128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141,\n",
      "        142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
      "        156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169,\n",
      "        170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183,\n",
      "        184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197,\n",
      "        198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211,\n",
      "        212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225,\n",
      "        226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239,\n",
      "        240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253,\n",
      "        254, 255])\n",
      "\t\t-Skipping module Sequential\n",
      "\tExecuting on machine 2\n",
      "\t\t-No input sent to this machine. Skipping module\n",
      "\tExecuting on machine 3\n",
      "\t\t-No input sent to this machine. Skipping module\n",
      "Finished execution of layer 52\n",
      "\n",
      "Executing module 53: layer4.0.shortcut.0\n",
      "\tExecuting on machine 0\n",
      "\t\t-No input sent to this machine. Skipping module\n",
      "\tExecuting on machine 1\n",
      "\t\t received input channels tensor([128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141,\n",
      "        142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
      "        156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169,\n",
      "        170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183,\n",
      "        184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197,\n",
      "        198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211,\n",
      "        212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225,\n",
      "        226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239,\n",
      "        240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253,\n",
      "        254, 255])\n",
      "\t\t-Saving current input. Swapping for input saved from start of block\n",
      "\t\t Output tensor shape : torch.Size([1, 512, 4, 4])\n",
      "\t\t sending C_out tensor([120]) to machine 0\n",
      "\t\t sending C_out tensor([128, 129, 130, 131, 132, 133, 135, 136, 137, 138, 139, 140, 141, 142,\n",
      "        143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156,\n",
      "        157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170,\n",
      "        171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184,\n",
      "        185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198,\n",
      "        199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212,\n",
      "        213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226,\n",
      "        227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240,\n",
      "        241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254,\n",
      "        255]) to machine 1\n",
      "\t\t sending C_out tensor([300, 313, 351, 374]) to machine 2\n",
      "\t\t sending C_out tensor([386, 397, 461, 481]) to machine 3\n",
      "\tExecuting on machine 2\n",
      "\t\t-No input sent to this machine. Skipping module\n",
      "\tExecuting on machine 3\n",
      "\t\t-No input sent to this machine. Skipping module\n",
      "Finished execution of layer 53\n",
      "\n",
      "Executing module 54: layer4.0.shortcut.1\n",
      "\tExecuting on machine 0\n",
      "\t\t received input channels tensor([120])\n",
      "\t\t Output tensor shape : torch.Size([1, 512, 4, 4])\n",
      "\t\t-Applying ReLU\n",
      "\t\t sending C_out tensor([  3,   5,  49, 106, 118]) to machine 0\n",
      "\tExecuting on machine 1\n",
      "\t\t received input channels tensor([128, 129, 130, 131, 132, 133, 135, 136, 137, 138, 139, 140, 141, 142,\n",
      "        143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156,\n",
      "        157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170,\n",
      "        171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184,\n",
      "        185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198,\n",
      "        199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212,\n",
      "        213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226,\n",
      "        227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240,\n",
      "        241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254,\n",
      "        255])\n",
      "\t\t Output tensor shape : torch.Size([1, 512, 4, 4])\n",
      "\t\t-adding residual\n",
      "\t\t-Applying ReLU\n",
      "\t\t sending C_out tensor([128, 129, 131, 132, 136, 137, 141, 143, 144, 145, 149, 153, 154, 157,\n",
      "        159, 161, 162, 164, 166, 169, 172, 177, 180, 181, 182, 185, 187, 188,\n",
      "        190, 192, 193, 196, 199, 200, 201, 210, 212, 214, 215, 216, 219, 220,\n",
      "        221, 222, 223, 224, 225, 227, 229, 230, 233, 235, 236, 237, 238, 242,\n",
      "        243, 246, 251, 255]) to machine 1\n",
      "\tExecuting on machine 2\n",
      "\t\t received input channels tensor([300, 313, 351, 374])\n",
      "\t\t Output tensor shape : torch.Size([1, 512, 4, 4])\n",
      "\t\t-Applying ReLU\n",
      "\t\t sending C_out tensor([259, 296, 315, 325, 337, 339]) to machine 2\n",
      "\tExecuting on machine 3\n",
      "\t\t received input channels tensor([386, 397, 461, 481])\n",
      "\t\t Output tensor shape : torch.Size([1, 512, 4, 4])\n",
      "\t\t-Applying ReLU\n",
      "\t\t sending C_out tensor([416]) to machine 3\n",
      "Finished execution of layer 54\n",
      "\n",
      "Executing module 55: layer4.1\n",
      "\tExecuting on machine 0\n",
      "\t\t received input channels tensor([  3,   5,  49, 106, 118])\n",
      "\t\t-Skipping module BasicBlock\n",
      "\tExecuting on machine 1\n",
      "\t\t received input channels tensor([128, 129, 131, 132, 136, 137, 141, 143, 144, 145, 149, 153, 154, 157,\n",
      "        159, 161, 162, 164, 166, 169, 172, 177, 180, 181, 182, 185, 187, 188,\n",
      "        190, 192, 193, 196, 199, 200, 201, 210, 212, 214, 215, 216, 219, 220,\n",
      "        221, 222, 223, 224, 225, 227, 229, 230, 233, 235, 236, 237, 238, 242,\n",
      "        243, 246, 251, 255])\n",
      "\t\t-Skipping module BasicBlock\n",
      "\tExecuting on machine 2\n",
      "\t\t received input channels tensor([259, 296, 315, 325, 337, 339])\n",
      "\t\t-Skipping module BasicBlock\n",
      "\tExecuting on machine 3\n",
      "\t\t received input channels tensor([416])\n",
      "\t\t-Skipping module BasicBlock\n",
      "Finished execution of layer 55\n",
      "\n",
      "Executing module 56: layer4.1.conv1\n",
      "\tExecuting on machine 0\n",
      "\t\t received input channels tensor([  3,   5,  49, 106, 118])\n",
      "\t\t Output tensor shape : torch.Size([1, 512, 4, 4])\n",
      "\t\t sending C_out tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127]) to machine 0\n",
      "\t\t sending C_out tensor([141, 216, 218, 231, 234, 236]) to machine 1\n",
      "\t\t sending C_out tensor([269, 294, 316, 325, 341, 342, 351, 362, 366]) to machine 2\n",
      "\t\t sending C_out tensor([392, 415, 428, 430, 432, 456, 464, 511]) to machine 3\n",
      "\tExecuting on machine 1\n",
      "\t\t received input channels tensor([128, 129, 131, 132, 136, 137, 141, 143, 144, 145, 149, 153, 154, 157,\n",
      "        159, 161, 162, 164, 166, 169, 172, 177, 180, 181, 182, 185, 187, 188,\n",
      "        190, 192, 193, 196, 199, 200, 201, 210, 212, 214, 215, 216, 219, 220,\n",
      "        221, 222, 223, 224, 225, 227, 229, 230, 233, 235, 236, 237, 238, 242,\n",
      "        243, 246, 251, 255])\n",
      "\t\t Output tensor shape : torch.Size([1, 512, 4, 4])\n",
      "\t\t sending C_out tensor([  0,   3,   4,   5,   6,   8,  10,  11,  16,  17,  19,  22,  25,  27,\n",
      "         33,  35,  36,  37,  39,  46,  47,  48,  50,  53,  55,  57,  60,  62,\n",
      "         67,  68,  69,  70,  71,  74,  77,  78,  84,  86,  87,  90,  91,  97,\n",
      "         98,  99, 100, 103, 105, 107, 108, 109, 112, 116, 118, 119, 121]) to machine 0\n",
      "\t\t sending C_out tensor([128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141,\n",
      "        142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
      "        156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169,\n",
      "        170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183,\n",
      "        184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197,\n",
      "        198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211,\n",
      "        212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225,\n",
      "        226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239,\n",
      "        240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253,\n",
      "        254, 255]) to machine 1\n",
      "\t\t sending C_out tensor([259, 260, 261, 263, 266, 271, 280, 283, 284, 287, 294, 295, 298, 299,\n",
      "        301, 307, 310, 315, 323, 324, 327, 331, 332, 333, 335, 339, 343, 344,\n",
      "        345, 349, 350, 352, 354, 357, 360, 367, 368, 369, 370, 371, 372, 373,\n",
      "        374, 376, 382, 383]) to machine 2\n",
      "\t\t sending C_out tensor([384, 388, 389, 394, 396, 399, 402, 403, 406, 407, 416, 417, 423, 425,\n",
      "        429, 430, 431, 432, 433, 439, 442, 443, 447, 449, 452, 453, 454, 456,\n",
      "        458, 459, 464, 469, 472, 473, 474, 475, 476, 482, 483, 484, 486, 492,\n",
      "        497, 505, 506, 507, 508]) to machine 3\n",
      "\tExecuting on machine 2\n",
      "\t\t received input channels tensor([259, 296, 315, 325, 337, 339])\n",
      "\t\t Output tensor shape : torch.Size([1, 512, 4, 4])\n",
      "\t\t sending C_out tensor([  6,  16,  35,  38,  50,  69,  79,  81, 124]) to machine 0\n",
      "\t\t sending C_out tensor([167, 201]) to machine 1\n",
      "\t\t sending C_out tensor([256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269,\n",
      "        270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283,\n",
      "        284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297,\n",
      "        298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311,\n",
      "        312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325,\n",
      "        326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339,\n",
      "        340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353,\n",
      "        354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367,\n",
      "        368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381,\n",
      "        382, 383]) to machine 2\n",
      "\t\t sending C_out tensor([393, 394, 402, 412, 417, 418, 439, 440, 442, 461, 468, 470, 480, 483,\n",
      "        493, 500]) to machine 3\n",
      "\tExecuting on machine 3\n",
      "\t\t received input channels tensor([416])\n",
      "\t\t Output tensor shape : torch.Size([1, 512, 4, 4])\n",
      "\t\t sending C_out tensor([56, 92, 96]) to machine 0\n",
      "\t\t sending C_out tensor([327, 352]) to machine 2\n",
      "\t\t sending C_out tensor([384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397,\n",
      "        398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411,\n",
      "        412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425,\n",
      "        426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439,\n",
      "        440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453,\n",
      "        454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467,\n",
      "        468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481,\n",
      "        482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495,\n",
      "        496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509,\n",
      "        510, 511]) to machine 3\n",
      "Finished execution of layer 56\n",
      "\n",
      "Executing module 57: layer4.1.conv2\n",
      "\tExecuting on machine 0\n",
      "\t\t received input channels tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127])\n",
      "\t\t Output tensor shape : torch.Size([1, 512, 4, 4])\n",
      "\t\t sending C_out tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127]) to machine 0\n",
      "\t\t sending C_out tensor([128, 129, 131, 132, 133, 134, 136, 137, 138, 140, 142, 143, 144, 146,\n",
      "        147, 149, 150, 151, 152, 154, 156, 157, 158, 161, 162, 164, 165, 170,\n",
      "        171, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187,\n",
      "        190, 191, 192, 193, 194, 197, 198, 199, 200, 201, 203, 204, 206, 208,\n",
      "        209, 210, 211, 212, 213, 214, 216, 217, 218, 219, 220, 222, 223, 224,\n",
      "        225, 226, 228, 230, 231, 232, 233, 234, 237, 238, 239, 242, 243, 244,\n",
      "        245, 247, 248, 249, 251, 253, 254, 255]) to machine 1\n",
      "\t\t sending C_out tensor([256, 258, 259, 260, 261, 262, 263, 265, 266, 267, 268, 269, 270, 271,\n",
      "        272, 273, 274, 276, 278, 279, 281, 284, 288, 289, 290, 291, 292, 293,\n",
      "        294, 295, 296, 297, 298, 300, 301, 302, 303, 304, 305, 307, 308, 309,\n",
      "        310, 312, 313, 315, 316, 318, 319, 320, 321, 323, 324, 325, 326, 327,\n",
      "        330, 331, 332, 333, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344,\n",
      "        345, 346, 347, 348, 350, 352, 354, 355, 356, 357, 358, 359, 360, 362,\n",
      "        365, 366, 368, 369, 372, 373, 376, 377, 378, 379, 380, 381, 383]) to machine 2\n",
      "\t\t sending C_out tensor([384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 396, 398, 399,\n",
      "        400, 401, 402, 404, 405, 406, 408, 409, 411, 412, 413, 414, 415, 416,\n",
      "        417, 418, 419, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431,\n",
      "        432, 433, 434, 436, 437, 438, 439, 442, 445, 447, 448, 449, 450, 452,\n",
      "        453, 454, 455, 456, 457, 458, 459, 461, 462, 466, 467, 468, 469, 470,\n",
      "        471, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 485, 486, 487,\n",
      "        488, 489, 490, 491, 492, 494, 497, 498, 500, 502, 503, 505, 506, 507,\n",
      "        508, 510, 511]) to machine 3\n",
      "\tExecuting on machine 1\n",
      "\t\t received input channels tensor([128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141,\n",
      "        142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
      "        156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169,\n",
      "        170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183,\n",
      "        184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197,\n",
      "        198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211,\n",
      "        212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225,\n",
      "        226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239,\n",
      "        240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253,\n",
      "        254, 255])\n",
      "\t\t Output tensor shape : torch.Size([1, 512, 4, 4])\n",
      "\t\t sending C_out tensor([  0,   3,   4,   6,   7,   8,  10,  11,  12,  14,  15,  16,  18,  19,\n",
      "         20,  21,  22,  24,  25,  29,  31,  34,  35,  36,  39,  40,  41,  42,\n",
      "         44,  45,  46,  47,  48,  49,  50,  51,  53,  54,  55,  56,  57,  58,\n",
      "         59,  60,  61,  63,  64,  65,  66,  68,  70,  73,  75,  76,  77,  78,\n",
      "         79,  80,  82,  83,  84,  85,  86,  87,  88,  90,  91,  92,  93,  95,\n",
      "         96,  97,  98,  99, 100, 101, 103, 105, 106, 107, 108, 109, 110, 112,\n",
      "        114, 115, 117, 118, 119, 120, 121, 123, 124, 126, 127]) to machine 0\n",
      "\t\t sending C_out tensor([128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141,\n",
      "        142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
      "        156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169,\n",
      "        170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183,\n",
      "        184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197,\n",
      "        198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211,\n",
      "        212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225,\n",
      "        226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239,\n",
      "        240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253,\n",
      "        254, 255]) to machine 1\n",
      "\t\t sending C_out tensor([256, 259, 260, 261, 263, 264, 265, 267, 268, 269, 270, 271, 274, 275,\n",
      "        277, 279, 281, 282, 285, 286, 287, 288, 289, 290, 291, 293, 294, 295,\n",
      "        296, 297, 298, 300, 301, 303, 304, 305, 307, 308, 309, 311, 312, 313,\n",
      "        315, 316, 318, 319, 320, 321, 322, 323, 324, 325, 327, 328, 329, 330,\n",
      "        332, 333, 334, 335, 336, 339, 340, 341, 342, 344, 348, 349, 350, 351,\n",
      "        352, 353, 354, 355, 356, 358, 361, 362, 363, 365, 367, 369, 370, 373,\n",
      "        376, 378, 379, 380, 381, 382]) to machine 2\n",
      "\t\t sending C_out tensor([384, 385, 387, 389, 390, 392, 393, 395, 396, 398, 399, 401, 402, 403,\n",
      "        406, 407, 408, 409, 410, 414, 415, 417, 418, 419, 421, 422, 423, 424,\n",
      "        425, 426, 428, 429, 430, 431, 432, 434, 435, 436, 437, 438, 440, 441,\n",
      "        442, 444, 445, 446, 447, 448, 449, 450, 451, 452, 454, 455, 459, 460,\n",
      "        461, 462, 464, 465, 466, 467, 468, 470, 471, 472, 473, 474, 475, 476,\n",
      "        479, 480, 481, 484, 485, 486, 487, 489, 491, 492, 493, 494, 496, 497,\n",
      "        498, 500, 501, 502, 504, 505, 506, 508, 510, 511]) to machine 3\n",
      "\tExecuting on machine 2\n",
      "\t\t received input channels tensor([256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269,\n",
      "        270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283,\n",
      "        284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297,\n",
      "        298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311,\n",
      "        312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325,\n",
      "        326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339,\n",
      "        340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353,\n",
      "        354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367,\n",
      "        368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381,\n",
      "        382, 383])\n",
      "\t\t Output tensor shape : torch.Size([1, 512, 4, 4])\n",
      "\t\t sending C_out tensor([  0,   2,   3,   5,   6,   7,   8,   9,  10,  11,  13,  15,  16,  17,\n",
      "         18,  19,  21,  22,  23,  24,  25,  26,  27,  28,  29,  30,  31,  32,\n",
      "         33,  34,  35,  37,  38,  39,  40,  42,  43,  44,  45,  46,  47,  48,\n",
      "         49,  50,  51,  52,  53,  56,  57,  59,  60,  61,  63,  64,  65,  66,\n",
      "         67,  68,  69,  70,  72,  73,  74,  75,  77,  78,  79,  80,  81,  82,\n",
      "         83,  84,  85,  86,  87,  88,  89,  90,  92,  93,  95,  96,  97,  98,\n",
      "        100, 102, 104, 106, 107, 108, 109, 110, 112, 114, 115, 116, 118, 119,\n",
      "        120, 121, 122, 124, 125, 126, 127]) to machine 0\n",
      "\t\t sending C_out tensor([128, 130, 131, 132, 133, 134, 136, 137, 138, 139, 140, 141, 142, 143,\n",
      "        144, 146, 147, 148, 149, 150, 152, 153, 155, 157, 158, 159, 160, 162,\n",
      "        164, 165, 166, 167, 170, 171, 172, 173, 174, 176, 177, 179, 180, 182,\n",
      "        184, 185, 188, 190, 193, 194, 196, 197, 198, 199, 200, 202, 203, 204,\n",
      "        205, 206, 207, 208, 209, 210, 212, 213, 215, 216, 217, 219, 220, 221,\n",
      "        222, 224, 225, 226, 227, 228, 231, 232, 234, 235, 240, 241, 242, 243,\n",
      "        244, 246, 247, 248, 249, 250, 251, 252, 253, 254]) to machine 1\n",
      "\t\t sending C_out tensor([256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269,\n",
      "        270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283,\n",
      "        284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297,\n",
      "        298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311,\n",
      "        312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325,\n",
      "        326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339,\n",
      "        340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353,\n",
      "        354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367,\n",
      "        368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381,\n",
      "        382, 383]) to machine 2\n",
      "\t\t sending C_out tensor([384, 385, 386, 387, 389, 390, 391, 392, 393, 396, 397, 399, 403, 406,\n",
      "        407, 408, 409, 410, 412, 414, 415, 416, 417, 419, 420, 422, 423, 424,\n",
      "        425, 426, 427, 428, 429, 430, 431, 433, 434, 435, 436, 437, 438, 441,\n",
      "        442, 444, 445, 446, 448, 449, 450, 451, 452, 453, 454, 455, 456, 458,\n",
      "        459, 461, 462, 463, 464, 465, 466, 468, 470, 471, 472, 473, 474, 475,\n",
      "        476, 477, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490,\n",
      "        491, 492, 493, 494, 496, 497, 498, 501, 502, 503, 505, 507, 508, 509,\n",
      "        511]) to machine 3\n",
      "\tExecuting on machine 3\n",
      "\t\t received input channels tensor([384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397,\n",
      "        398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411,\n",
      "        412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425,\n",
      "        426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439,\n",
      "        440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453,\n",
      "        454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467,\n",
      "        468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481,\n",
      "        482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495,\n",
      "        496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509,\n",
      "        510, 511])\n",
      "\t\t Output tensor shape : torch.Size([1, 512, 4, 4])\n",
      "\t\t sending C_out tensor([  3,   5,   6,   7,   9,  11,  12,  13,  14,  15,  16,  17,  18,  21,\n",
      "         22,  23,  24,  25,  26,  28,  29,  30,  31,  32,  33,  34,  35,  37,\n",
      "         38,  39,  41,  42,  43,  44,  45,  48,  49,  50,  53,  55,  56,  57,\n",
      "         58,  59,  60,  61,  62,  63,  64,  65,  67,  68,  69,  70,  71,  72,\n",
      "         73,  74,  75,  76,  77,  78,  80,  81,  82,  83,  84,  87,  88,  90,\n",
      "         91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 104, 105,\n",
      "        106, 107, 108, 109, 110, 111, 112, 113, 114, 116, 118, 119, 121, 122,\n",
      "        124, 125, 127]) to machine 0\n",
      "\t\t sending C_out tensor([128, 129, 130, 134, 136, 137, 138, 139, 140, 141, 143, 144, 146, 148,\n",
      "        150, 151, 152, 154, 155, 156, 157, 160, 161, 162, 164, 168, 169, 170,\n",
      "        172, 173, 174, 175, 176, 177, 178, 180, 182, 184, 185, 187, 188, 189,\n",
      "        190, 191, 192, 193, 195, 196, 197, 198, 200, 201, 202, 203, 204, 205,\n",
      "        206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219,\n",
      "        220, 224, 225, 226, 227, 228, 229, 230, 231, 233, 234, 235, 236, 238,\n",
      "        239, 241, 243, 244, 245, 246, 247, 248, 250, 251, 252, 253, 254, 255]) to machine 1\n",
      "\t\t sending C_out tensor([256, 257, 258, 259, 260, 261, 262, 263, 264, 266, 267, 269, 270, 271,\n",
      "        274, 275, 276, 277, 278, 279, 280, 281, 282, 284, 285, 286, 287, 288,\n",
      "        289, 290, 291, 292, 293, 294, 296, 297, 298, 300, 301, 304, 305, 306,\n",
      "        307, 310, 311, 312, 313, 316, 317, 318, 319, 320, 321, 322, 323, 324,\n",
      "        327, 328, 329, 330, 331, 332, 335, 336, 337, 338, 339, 340, 341, 343,\n",
      "        344, 345, 347, 348, 349, 351, 353, 354, 356, 357, 358, 359, 362, 363,\n",
      "        364, 365, 367, 368, 369, 370, 371, 372, 374, 375, 376, 377, 380, 381,\n",
      "        382, 383]) to machine 2\n",
      "\t\t sending C_out tensor([384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397,\n",
      "        398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411,\n",
      "        412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425,\n",
      "        426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439,\n",
      "        440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453,\n",
      "        454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467,\n",
      "        468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481,\n",
      "        482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495,\n",
      "        496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509,\n",
      "        510, 511]) to machine 3\n",
      "Finished execution of layer 57\n",
      "\n",
      "Executing module 58: layer4.1.bn1\n",
      "\tExecuting on machine 0\n",
      "\t\t received input channels tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127])\n",
      "\t\t Output tensor shape : torch.Size([1, 512, 4, 4])\n",
      "\t\t-Applying ReLU\n",
      "\t\t sending C_out tensor([  3,   4,  12,  20,  25,  26,  34,  35,  38,  40,  49,  57,  60,  66,\n",
      "         70,  73,  82,  87,  88,  90,  91,  98, 106, 108, 112, 114, 121, 126]) to machine 0\n",
      "\tExecuting on machine 1\n",
      "\t\t received input channels tensor([128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141,\n",
      "        142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
      "        156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169,\n",
      "        170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183,\n",
      "        184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197,\n",
      "        198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211,\n",
      "        212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225,\n",
      "        226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239,\n",
      "        240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253,\n",
      "        254, 255])\n",
      "\t\t Output tensor shape : torch.Size([1, 512, 4, 4])\n",
      "\t\t-Applying ReLU\n",
      "\t\t sending C_out tensor([128, 129, 130, 131, 132, 133, 134, 135, 136, 138, 139, 140, 141, 142,\n",
      "        143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156,\n",
      "        157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170,\n",
      "        171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184,\n",
      "        185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198,\n",
      "        199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 213,\n",
      "        214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227,\n",
      "        228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241,\n",
      "        242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]) to machine 1\n",
      "\tExecuting on machine 2\n",
      "\t\t received input channels tensor([256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269,\n",
      "        270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283,\n",
      "        284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297,\n",
      "        298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311,\n",
      "        312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325,\n",
      "        326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339,\n",
      "        340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353,\n",
      "        354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367,\n",
      "        368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381,\n",
      "        382, 383])\n",
      "\t\t Output tensor shape : torch.Size([1, 512, 4, 4])\n",
      "\t\t-Applying ReLU\n",
      "\t\t sending C_out tensor([264, 271, 275, 285, 286, 288, 291, 293, 301, 305, 306, 307, 311, 313,\n",
      "        318, 319, 322, 324, 329, 340, 348, 349, 350, 352, 358, 362, 363, 371]) to machine 2\n",
      "\tExecuting on machine 3\n",
      "\t\t received input channels tensor([384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397,\n",
      "        398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411,\n",
      "        412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425,\n",
      "        426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439,\n",
      "        440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453,\n",
      "        454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467,\n",
      "        468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481,\n",
      "        482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495,\n",
      "        496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509,\n",
      "        510, 511])\n",
      "\t\t Output tensor shape : torch.Size([1, 512, 4, 4])\n",
      "\t\t-Applying ReLU\n",
      "\t\t sending C_out tensor([385, 391, 402, 403, 408, 410, 417, 419, 427, 430, 431, 432, 434, 445,\n",
      "        446, 447, 461, 462, 465, 474, 475, 478, 487, 493, 496, 498, 500, 501,\n",
      "        504, 506, 510]) to machine 3\n",
      "Finished execution of layer 58\n",
      "\n",
      "Executing module 59: layer4.1.bn2\n",
      "\tExecuting on machine 0\n",
      "\t\t received input channels tensor([  3,   4,  12,  20,  25,  26,  34,  35,  38,  40,  49,  57,  60,  66,\n",
      "         70,  73,  82,  87,  88,  90,  91,  98, 106, 108, 112, 114, 121, 126])\n",
      "\t\t Output tensor shape : torch.Size([1, 512, 4, 4])\n",
      "\t\t-Applying ReLU\n",
      "\t\t sending C_out tensor([ 4, 88]) to machine 0\n",
      "\tExecuting on machine 1\n",
      "\t\t received input channels tensor([128, 129, 130, 131, 132, 133, 134, 135, 136, 138, 139, 140, 141, 142,\n",
      "        143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156,\n",
      "        157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170,\n",
      "        171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184,\n",
      "        185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198,\n",
      "        199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 213,\n",
      "        214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227,\n",
      "        228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241,\n",
      "        242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255])\n",
      "\t\t Output tensor shape : torch.Size([1, 512, 4, 4])\n",
      "\t\t-Applying ReLU\n",
      "\t\t sending C_out tensor([128, 129, 130, 131, 132, 133, 134, 135, 136, 139, 140, 141, 142, 143,\n",
      "        144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 156, 157, 158,\n",
      "        159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172,\n",
      "        173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186,\n",
      "        187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200,\n",
      "        201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 213, 214, 215, 216,\n",
      "        217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230,\n",
      "        231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244,\n",
      "        245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]) to machine 1\n",
      "\tExecuting on machine 2\n",
      "\t\t received input channels tensor([264, 271, 275, 285, 286, 288, 291, 293, 301, 305, 306, 307, 311, 313,\n",
      "        318, 319, 322, 324, 329, 340, 348, 349, 350, 352, 358, 362, 363, 371])\n",
      "\t\t Output tensor shape : torch.Size([1, 512, 4, 4])\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 3\n",
      "\t\t received input channels tensor([385, 391, 402, 403, 408, 410, 417, 419, 427, 430, 431, 432, 434, 445,\n",
      "        446, 447, 461, 462, 465, 474, 475, 478, 487, 493, 496, 498, 500, 501,\n",
      "        504, 506, 510])\n",
      "\t\t Output tensor shape : torch.Size([1, 512, 4, 4])\n",
      "\t\t-Applying ReLU\n",
      "\t\t sending C_out tensor([419, 431, 465]) to machine 3\n",
      "Finished execution of layer 59\n",
      "\n",
      "Executing module 60: layer4.1.shortcut\n",
      "\tExecuting on machine 0\n",
      "\t\t received input channels tensor([ 4, 88])\n",
      "\t\t-Skipping module Sequential\n",
      "\tExecuting on machine 1\n",
      "\t\t received input channels tensor([128, 129, 130, 131, 132, 133, 134, 135, 136, 139, 140, 141, 142, 143,\n",
      "        144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 156, 157, 158,\n",
      "        159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172,\n",
      "        173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186,\n",
      "        187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200,\n",
      "        201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 213, 214, 215, 216,\n",
      "        217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230,\n",
      "        231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244,\n",
      "        245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255])\n",
      "\t\t-Skipping module Sequential\n",
      "\tExecuting on machine 2\n",
      "\t\t-No input sent to this machine. Skipping module\n",
      "\tExecuting on machine 3\n",
      "\t\t received input channels tensor([419, 431, 465])\n",
      "\t\t-Skipping module Sequential\n",
      "Finished execution of layer 60\n",
      "\n",
      "Executing module 61: linear\n",
      "\tExecuting on machine 0\n",
      "\t\t received input channels tensor([ 4, 88])\n",
      "\t\t Output tensor shape : torch.Size([1, 10])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[67], line 233\u001b[0m\n\u001b[0;32m    231\u001b[0m \u001b[38;5;66;03m# TODO: this is inefficient, redo. Probbably need to send a tensor and some info what output channels are being sent\u001b[39;00m\n\u001b[0;32m    232\u001b[0m tmp_out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(out_tensor\u001b[38;5;241m.\u001b[39mshape) \n\u001b[1;32m--> 233\u001b[0m tmp_out[:,communication_mask,:,:] \u001b[38;5;241m=\u001b[39m \u001b[43mout_tensor\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcommunication_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    234\u001b[0m output[rx_mach][imach] \u001b[38;5;241m=\u001b[39m tmp_out\n\u001b[0;32m    236\u001b[0m \u001b[38;5;66;03m# debug\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    mock run through inference using split models \n",
    "'''\n",
    "\n",
    "residual_input = {} # use this to keep track of inputs stored in machine memory for residule layers\n",
    "add_bias = False # add bias for previous conv layer \n",
    "\n",
    "# make inference \n",
    "with torch.no_grad():\n",
    "        # iterate through layers 1 module at a time \n",
    "        for imodule in range(num_total_modules): # 16 <=> layer_1 block \n",
    "\n",
    "                if imodule in [0]:\n",
    "                        continue\n",
    "\n",
    "                # initialize output for ilayer\n",
    "                #output = np.empty((num_machines, num_machines), dtype=torch.Tensor) # square list indexed as: output[destination/RX machine][origin/TX machine]\n",
    "                # TODO: find a better datastructure for this \n",
    "                output = [None]*num_machines\n",
    "                output = [output[:] for i in range(num_machines)]\n",
    "                \n",
    "                send_module_outputs = True\n",
    "\n",
    "                print(f'Executing module {imodule}: {module_names[imodule]}')\n",
    "\n",
    "                # iterate through each machine (done in parallel later)\n",
    "                for imach in range(num_machines):\n",
    "                        print(f'\\tExecuting on machine {imach}')\n",
    "                        \n",
    "                        add_residual = False\n",
    "\n",
    "                        # combine inputs from machines\n",
    "                        curr_input = False \n",
    "                        rx_count = 0\n",
    "                        for i in range(num_machines):\n",
    "                                if not input[imach][i] == None:\n",
    "                                        if not torch.is_tensor(curr_input):\n",
    "                                                curr_input = input[imach][i] # initialize curr_input with first input tensor \n",
    "                                        else:\n",
    "                                                curr_input += input[imach][i]\n",
    "                                        rx_count += 1\n",
    "                        if add_bias:\n",
    "                                # TODO: check if this works (this is not required for resnet18 because no bias on conv layers)\n",
    "                                dummy, prev_module = next((x for i,x in enumerate(model.named_modules()) if i==imodule-1))\n",
    "                                bias = prev_module.bias \n",
    "                                curr_input += bias/rx_count\n",
    "\n",
    "                        # skip this machine+module if there is no input to compute \n",
    "                        if not torch.is_tensor(curr_input):\n",
    "                                print('\\t\\t-No input sent to this machine. Skipping module')\n",
    "                                continue\n",
    "\n",
    "                        # debug\n",
    "                        print(f'\\t\\t received input channels {get_nonzero_channels(curr_input)}')\n",
    "\n",
    "                        if imodule in residual_block_start:\n",
    "                                # save input for later \n",
    "                                residual_input[str(imach)] = {}\n",
    "                                residual_input[str(imach)]['block_in'] = curr_input\n",
    "                                print('\\t\\t-Saving input for later...')\n",
    "                        elif imodule in residual_connection_start:\n",
    "                                # swap tensors\n",
    "                                residual_input[str(imach)]['block_out'] = curr_input\n",
    "                                curr_input = residual_input[str(imach)]['block_in'] \n",
    "                                print('\\t\\t-Saving current input. Swapping for input saved from start of block')\n",
    "\n",
    "                        # get the current module\n",
    "                        # TODO: is this very bad for latency? Only load module if you have to \n",
    "                        curr_name, curr_module = next((x for i,x in enumerate(model.named_modules()) if i==imodule)) \n",
    "\n",
    "                        # update communication I/O for this layer  \n",
    "                        # TODO: revist this implementation\n",
    "                        split_param_name = curr_name + '.weight'\n",
    "                        if split_param_name in split_module_names:\n",
    "\n",
    "                                # skip if machine doesnt expect input\n",
    "                                if len(configs['partition'][split_param_name]['channel_id'][imach]) == 0:\n",
    "                                        print(f'\\t\\t-No input assigned to this machine. Skipping...')\n",
    "                                        continue\n",
    "                                \n",
    "                                # TODO: reconsider implementation \n",
    "                                # What input channels does this machine compute?\n",
    "                                input_channels = torch.tensor(configs['partition'][split_param_name]['channel_id'][imach],\n",
    "                                        device=torch.device(configs['device']))\n",
    "                                N_in = len(input_channels) # TODO: is this used?\n",
    "\n",
    "                                # Where to send output (map of output channels to different machines)\n",
    "                                output_channel_map = configs['partition'][split_param_name]['filter_id']\n",
    "                        elif type(curr_module) == nn.BatchNorm2d:\n",
    "                                # TODO: address the following assumptions:\n",
    "                                #       - assume all BN layers have C_in divisable by num_machines\n",
    "                                #       - assume C_in are evenly split in sequential order WARNING THIS WILL BREAK WHEN WE START TO DO ASSIGN WEIGHTS TO DIFF MACHINES\n",
    "                                N_Cin = curr_module.num_features\n",
    "                                Cin_per_machine = N_Cin/num_machines\n",
    "                                if Cin_per_machine % 1 > 0:\n",
    "                                        print('ERROR: UNEXPECTED NUMBER OF I/O FOR BATCH NORMAL MODULE {imodule}')\n",
    "                                Cin_per_machine = int(Cin_per_machine)\n",
    "                                input_channels = np.arange(Cin_per_machine) + imach*Cin_per_machine\n",
    "                                output_channel_map = [None]*num_machines\n",
    "                                for i in range(num_machines):\n",
    "                                        if i == imach:\n",
    "                                                output_channel_map[i] = input_channels\n",
    "                                        else:\n",
    "                                                output_channel_map[i] = np.array([])\n",
    "                                input_channels = torch.tensor(input_channels, device=torch.device(configs['device']))\n",
    "                        elif type(curr_module) == nn.Linear and imodule == num_total_modules-1:\n",
    "                                # if final layer output all goes to machine 0 \n",
    "                                # TODO: find better way to handle this. Also will we encounter Linear layers not at the end of the model\n",
    "                                N_Cin = curr_module.in_features\n",
    "                                Cin_per_machine = N_Cin/num_machines\n",
    "                                if Cin_per_machine % 1 > 0:\n",
    "                                        print('ERROR: UNEXPECTED NUMBER OF I/O FOR LINEAR MODULE {imodule}')\n",
    "                                Cin_per_machine = int(Cin_per_machine)\n",
    "                                input_channels = np.arange(Cin_per_machine) + imach*Cin_per_machine\n",
    "                                N_Cout = curr_module.out_features \n",
    "                                output_channel_map = [None]*num_machines\n",
    "                                for i in range(num_machines):\n",
    "                                        if i == 0:\n",
    "                                                output_channel_map[i] = np.arange(N_Cout) \n",
    "                                        else:\n",
    "                                                output_channel_map[i] = np.array([])\n",
    "                                input_channels = torch.tensor(input_channels, device=torch.device(configs['device']))\n",
    "\n",
    "\n",
    "                        # reduce computation-- make vertically split layer \n",
    "                        # TODO: generalize this to more than conv layers \n",
    "                        if type(curr_module) == nn.Conv2d:\n",
    "                                split_layer = nn.Conv2d(N_in,\n",
    "                                                curr_module.weight.shape[0], # TODO does this need to be an int? (currently tensor)\n",
    "                                                kernel_size= curr_module.kernel_size,\n",
    "                                                stride=curr_module.stride,\n",
    "                                                padding=curr_module.padding, \n",
    "                                                bias=False) # TODO: add bias during input collecting step on next layer \n",
    "\n",
    "                                # write parameters to split layer \n",
    "                                split_layer.weight = torch.nn.Parameter(curr_module.weight.index_select(1, input_channels))\n",
    "\n",
    "                                # TODO: add support for splitting bias\n",
    "\n",
    "                        elif type(curr_module) == nn.BatchNorm2d:\n",
    "                                split_layer = nn.BatchNorm2d(N_in, \n",
    "                                                curr_module.eps,\n",
    "                                                momentum=curr_module.momentum, \n",
    "                                                affine=curr_module.affine, \n",
    "                                                track_running_stats=curr_module.track_running_stats)\n",
    "\n",
    "                                # write parameters to split layer \n",
    "                                split_layer.weight = torch.nn.Parameter(curr_module.weight.index_select(0, input_channels))\n",
    "                                split_layer.running_mean = torch.nn.Parameter(curr_module.running_mean.index_select(0, input_channels))\n",
    "                                split_layer.running_var = torch.nn.Parameter(curr_module.running_var.index_select(0, input_channels))\n",
    "\n",
    "                                if not curr_module.bias == None:\n",
    "                                        split_layer.bias = torch.nn.Parameter(curr_module.bias.index_select(0, input_channels))\n",
    "\n",
    "                                # TODO: revise implementation to only compute necessary C_in to C_out \n",
    "                                # assume mach-Cout map from previous conv layer can be used as inputs for this bn layer\n",
    "                                #input_channels = output_channel_map[imach]\n",
    "\n",
    "                        elif type(curr_module) == nn.Linear:\n",
    "                                # TODO: assumes there is a bias \n",
    "                                split_layer = nn.Linear(N_in, \n",
    "                                                curr_module.weight.shape[0])\n",
    "\n",
    "                                # write parameters to split layer \n",
    "                                split_layer.weight = torch.nn.Parameter(curr_module.weight.index_select(1, input_channels))\n",
    "\n",
    "                                # TODO: double check bias is applied correctly\n",
    "                                if not curr_module.bias == None:\n",
    "                                        split_layer.bias = curr_module.bias\n",
    "\n",
    "                                # prep for linear layer\n",
    "                                # TODO: assumes this always happens before linear layer \n",
    "                                # bn takes one in channel C_in_i and produces one out channel C_out_j. No communication is needed. \n",
    "                                curr_input = F.avg_pool2d(curr_input, 4)\n",
    "                                curr_input = curr_input.view(curr_input.size(0), -1)\n",
    "\n",
    "                        else:\n",
    "                                print(f'\\t\\t-Skipping module {type(curr_module).__name__}')\n",
    "                                send_module_outputs = False\n",
    "                                continue\n",
    "                        \n",
    "                        # make sure layer is in eval mode\n",
    "                        # TODO: if you set model.eval() can we skip this, also only required for bn layers? Maybe \n",
    "                        split_layer.eval()\n",
    "\n",
    "                        # eval split\n",
    "                        out_tensor = split_layer(curr_input.index_select(1, input_channels))\n",
    "                        if type(curr_module) == nn.BatchNorm2d:\n",
    "                                tmp_out_tensor = torch.zeros(curr_input.shape)\n",
    "                                tmp_out_tensor[:,input_channels.numpy(),:,:] = out_tensor\n",
    "                                out_tensor = tmp_out_tensor\n",
    "\n",
    "\n",
    "                        print(f'\\t\\t Output tensor shape : {out_tensor.shape}')\n",
    "\n",
    "                        # debug\n",
    "                        nonzero_out_tensor = torch.unique(torch.nonzero(out_tensor, as_tuple=True)[1])\n",
    "\n",
    "\n",
    "                        # check if this is residual layer\n",
    "                        if imodule in residual_block_end and 'block_out' in residual_input[str(imach)]: # TODO: does this conditional make sense?\n",
    "                                print('\\t\\t-adding residual')\n",
    "                                out_tensor += residual_input[str(imach)]['block_out']\n",
    "\n",
    "                                # erase stored \n",
    "                                residual_input[str(imach)] = {}\n",
    "\n",
    "                        # apply ReLU after batch layers\n",
    "                        if imodule in relu_modules:\n",
    "                                print('\\t\\t-Applying ReLU')\n",
    "                                out_tensor = F.relu(out_tensor)\n",
    "\n",
    "                        # look at which C_out need to be computed and sent\n",
    "                        #nonzero_Cout = torch.unique(torch.nonzero(split_layer.weight, as_tuple=True)[0]) # find nonzero dimensions in output channels\n",
    "                        nonzero_Cout = get_nonzero_channels(out_tensor)\n",
    "\n",
    "                        # communicate\n",
    "                        out_channel_array = torch.arange(out_tensor.shape[1])\n",
    "                        for rx_mach in range(num_machines):\n",
    "                                # only add to output if communication is necessary \n",
    "\n",
    "                                # Get output channels for current rx machine? TODO: consider removing, this just maps C_out's to machine\n",
    "                                output_channels = torch.tensor(output_channel_map[rx_mach],\n",
    "                                        device=torch.device(configs['device']))\n",
    "\n",
    "                                # TODO: is there a faster way to do this? Consider putting larger array 1st... just not sure which one that'd be\n",
    "                                nonzero_out_channels = nonzero_Cout[torch.isin(nonzero_Cout, output_channels)]\n",
    "                                if nonzero_out_channels.nelement() > 0:\n",
    "                                        communication_mask = torch.isin(out_channel_array, nonzero_out_channels)\n",
    "\n",
    "                                        # TODO: this is inefficient, redo. Probbably need to send a tensor and some info what output channels are being sent\n",
    "                                        tmp_out = torch.zeros(out_tensor.shape) \n",
    "                                        tmp_out[:,communication_mask,:,:] = out_tensor[:,communication_mask,:,:]\n",
    "                                        output[rx_mach][imach] = tmp_out\n",
    "\n",
    "                                        # debug\n",
    "                                        print(f'\\t\\t sending C_out {nonzero_out_channels} to machine {rx_mach}')\n",
    "\n",
    "                # send to next layer  \n",
    "                if send_module_outputs:      \n",
    "                        input = output\n",
    "                print(f'Finished execution of layer {imodule}')\n",
    "                print()\n",
    "\n",
    "# collect outputs -- assumes ends with Linear layer. Not sure how generalizable this is\n",
    "# if loop stops on module that doesnt calculate anything use input struct \n",
    "if send_module_outputs:\n",
    "        tmp_output = output\n",
    "else:\n",
    "        tmp_output = input \n",
    "need_to_init  = True\n",
    "for rx_mach in range(num_machines):\n",
    "        for tx_mach in range(num_machines):\n",
    "                if not tmp_output[rx_mach][tx_mach] == None:\n",
    "                        if need_to_init:\n",
    "                                final_output = tmp_output[rx_mach][tx_mach]\n",
    "                                need_to_init = False\n",
    "                        else:\n",
    "                                # TODO: += causes assignment issues, switched to x = x+y which might be more more inefficent memory wise ... \n",
    "                                final_output = final_output + tmp_output[rx_mach][tx_mach] \n",
    "                                nz_channels = get_nonzero_channels(final_output)\n",
    "                                #print(f'({rx_mach},{tx_mach}) {nz_channels}')\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine 0\n",
      "[]\n",
      "0\n",
      "Machine 1\n",
      "[]\n",
      "0\n",
      "Machine 2\n",
      "[]\n",
      "0\n",
      "Machine 3\n",
      "[]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "'''Sanity Check C_out'''\n",
    "\n",
    "# emulate the rx machine receiving its inputs and summing them \n",
    "# does the nonzero C_in channels make sense?\n",
    "\n",
    "for rx_mach in range(num_machines):\n",
    "    \n",
    "    need_to_init = True\n",
    "    for tx_mach in range(num_machines):\n",
    "\n",
    "        if not output[rx_mach][tx_mach] == None:\n",
    "            if need_to_init:\n",
    "                    tmp_output = output[rx_mach][tx_mach]\n",
    "                    need_to_init = False\n",
    "            else:\n",
    "                    tmp_output = tmp_output+ output[rx_mach][tx_mach]\n",
    "    \n",
    "    print(f'Machine {rx_mach}')\n",
    "    if need_to_init:\n",
    "        nz_channels = []      \n",
    "    else:\n",
    "        nz_channels = get_nonzero_channels(tmp_output)\n",
    "    print(nz_channels)     \n",
    "    print(len(nz_channels))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Compare I/O for one layer \n",
    "'''\n",
    "\n",
    "def compare_IO(model, module_list, input, split_output):\n",
    "    ''' This function is used for debugging. It computes \n",
    "    the output for a particular layer in the model using the input argument \n",
    "    and compares it to the split output'''\n",
    "\n",
    "\n",
    "    for imodule in module_list:\n",
    "        curr_name, curr_module = next((x for i,x in enumerate(model.named_modules()) if i==imodule)) \n",
    "        print(curr_name)\n",
    "\n",
    "        curr_module.eval()\n",
    "\n",
    "        input = curr_module(input)\n",
    "        if imodule in relu_modules:\n",
    "            print('Apllying ReLU')\n",
    "            input = F.relu(input)\n",
    "    full_output = input \n",
    "\n",
    "    io_match = torch.all(torch.eq(split_output, full_output))\n",
    "\n",
    "    return (io_match, full_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1\n",
      "bn1\n",
      "Apllying ReLU\n",
      "layer1.0.conv1\n",
      "layer1.0.conv2\n",
      "layer1.0.bn1\n",
      "Apllying ReLU\n",
      "layer1.0.bn2\n",
      "Apllying ReLU\n",
      "layer1.0.shortcut\n",
      "layer1.1.conv1\n",
      "layer1.1.conv2\n",
      "layer1.1.bn1\n",
      "Apllying ReLU\n",
      "layer1.1.bn2\n",
      "Apllying ReLU\n",
      "tensor([9.5367e-07], grad_fn=<MaxBackward0>)\n",
      "\n",
      "tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.7462e-10, 4.1473e-10,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 1.4901e-07, 2.3842e-07, 4.1723e-07, 5.9605e-08, 2.3842e-07,\n",
      "         1.1921e-07, 0.0000e+00, 9.5367e-07, 1.1921e-07, 0.0000e+00, 1.1921e-07,\n",
      "         1.8626e-09, 2.9802e-08, 2.3283e-10, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 9.3132e-10,\n",
      "         2.3283e-10, 1.8626e-09, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.8626e-09, 0.0000e+00,\n",
      "         1.3039e-08, 0.0000e+00, 6.9849e-10, 1.7881e-07, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]],\n",
      "       grad_fn=<MaxBackward0>)\n",
      "tensor([10, 11, 19, 20, 21, 22, 23, 24, 26, 27, 29, 30, 31, 32, 41, 42, 43, 52,\n",
      "        54, 56, 57])\n",
      "\n",
      "failing Cout = tensor([10, 11, 19, 20, 21, 22, 23, 24, 26, 27, 29, 30, 31, 32, 41, 42, 43, 52,\n",
      "        54, 56, 57])  (len = 21)\n",
      "passing Cout = tensor([ 4,  7, 18, 28])  (len = 4)\n"
     ]
    }
   ],
   "source": [
    "''' Single Layer Test'''\n",
    "io_match, full_output= compare_IO(model, [1,2,5,6,7,8, 9,11,12, 13,14], input_tensor, final_output) #\n",
    "\n",
    "# module 9 sequential seems to be messing stuff up \n",
    "\n",
    "\n",
    "#print(io_match)\n",
    "diff_output = torch.abs(full_output - final_output)\n",
    "\n",
    "print(torch.max(torch.reshape(diff_output, (N_batch, -1)), dim=1)[0])\n",
    "#plt.hist(diff_output.reshape((-1,)))\n",
    "#plt.show()\n",
    "\n",
    "max_by_Cout = torch.max(torch.abs(diff_output.reshape((1,64,-1))), dim=2)\n",
    "\n",
    "print()\n",
    "print(max_by_Cout[0])\n",
    "print(get_nonzero_channels(max_by_Cout[0]))\n",
    "\n",
    "\n",
    "# get C_out with zero and non-zero diff\n",
    "nonzero_Cout = get_nonzero_channels(full_output)\n",
    "failing_Cout = nonzero_Cout[torch.isin(nonzero_Cout, get_nonzero_channels(max_by_Cout[0]))]\n",
    "passing_Cout = nonzero_Cout[torch.isin(nonzero_Cout, get_nonzero_channels(max_by_Cout[0])) == False]\n",
    "print() \n",
    "print(f'failing Cout = {failing_Cout}  (len = {len(failing_Cout)})')\n",
    "print(f'passing Cout = {passing_Cout}  (len = {len(passing_Cout)})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 9.2539e-01, 3.2518e-01,\n",
      "        3.7491e-01, 5.9586e-01, 6.1565e-01, 6.4117e-01, 4.1604e-01, 3.3312e-01,\n",
      "        3.6875e-01, 8.9991e-01, 8.7877e-01, 2.9385e-01, 3.3101e-01, 9.6239e-01,\n",
      "        0.0000e+00, 2.8456e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 4.8229e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 4.9386e-04, 0.0000e+00],\n",
      "       grad_fn=<ReshapeAliasBackward0>)\n"
     ]
    }
   ],
   "source": [
    "''' \n",
    "Look at weights in conv1\n",
    "'''\n",
    "\n",
    "w = model.conv1.weight\n",
    "\n",
    "max_weight_by_Cout = torch.max(torch.abs(w[:,0,:].reshape((64,1,-1))), dim=2)[0].reshape((-1))\n",
    "print(max_weight_by_Cout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResnetBlockOne(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "torch.return_types.max(\n",
      "values=tensor([3.7672], grad_fn=<MaxBackward0>),\n",
      "indices=tensor([241]))\n"
     ]
    }
   ],
   "source": [
    "''' \n",
    "    Check Model output 1 horizontal block at a time \n",
    "'''\n",
    "\n",
    "# imodule = 16\n",
    "\n",
    "\n",
    "print(layer_1)\n",
    "layer_1_output = layer_1(input_tensor)\n",
    "\n",
    "diff_output = torch.abs(layer_1_output - final_output)\n",
    "\n",
    "print(torch.max(torch.reshape(diff_output, (N_batch, -1)), dim=1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'layer1.1.shortcut.weight'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m split_output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(full_output\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m imachine \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_machines):\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# get input channels\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m     input_channels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[43mconfigs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpartition\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43msplit_param_name\u001b[49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchannel_id\u001b[39m\u001b[38;5;124m'\u001b[39m][imach],\n\u001b[0;32m     12\u001b[0m                                         device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(configs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[0;32m     13\u001b[0m     N_in \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(input_channels) \u001b[38;5;66;03m# TODO: is this used?\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;66;03m# make split layer \u001b[39;00m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'layer1.1.shortcut.weight'"
     ]
    }
   ],
   "source": [
    "''' Outside framework test'''\n",
    "\n",
    "imodule = 1\n",
    "curr_name, curr_module = next((x for i,x in enumerate(model.named_modules()) if i==imodule)) \n",
    "\n",
    "full_output = curr_module(input_tensor)\n",
    "\n",
    "split_output = torch.zeros(full_output.shape)\n",
    "for imachine in range(num_machines):\n",
    "    # get input channels\n",
    "    input_channels = torch.tensor(configs['partition'][split_param_name]['channel_id'][imach],\n",
    "                                        device=torch.device(configs['device']))\n",
    "    N_in = len(input_channels) # TODO: is this used?\n",
    "\n",
    "    # make split layer \n",
    "    split_layer = nn.Conv2d(N_in,\n",
    "                    curr_module.weight.shape[0], # TODO does this need to be an int? (currently tensor)\n",
    "                    kernel_size= curr_module.kernel_size,\n",
    "                    stride=curr_module.stride,\n",
    "                    padding=curr_module.padding, \n",
    "                    bias=False) # TODO: add bias during input collecting step on next layer \n",
    "    split_layer.weight = torch.nn.Parameter(curr_module.weight.index_select(1, input_channels))\n",
    "\n",
    "    # eval and add\n",
    "    split_output += split_layer(input_tensor.index_select(1, input_channels))\n",
    "\n",
    "# compare \n",
    "io_match = torch.all(torch.eq(split_output, full_output))\n",
    "diff_output = torch.abs(full_output - final_output)\n",
    "print(io_match)\n",
    "print(torch.max(diff_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in_tensor\n",
      "tensor([[[[ 0.,  1.,  2.],\n",
      "          [ 3.,  4.,  5.],\n",
      "          [ 6.,  7.,  8.]],\n",
      "\n",
      "         [[ 9., 10., 11.],\n",
      "          [12., 13., 14.],\n",
      "          [15., 16., 17.]]]], dtype=torch.float64)\n",
      "\n",
      "kernals\n",
      "Parameter containing:\n",
      "tensor([[[[2., 1.],\n",
      "          [1., 0.]],\n",
      "\n",
      "         [[2., 1.],\n",
      "          [1., 2.]]]], dtype=torch.float64, requires_grad=True)\n",
      "\n",
      "out tensor\n",
      "tensor([[[[ 70.,  80.],\n",
      "          [100., 110.]]]], dtype=torch.float64, grad_fn=<ConvolutionBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''2D conv test'''\n",
    "\n",
    "in_tensor = torch.arange(18, dtype=float).reshape([1,2,3,3])\n",
    "print('in_tensor')\n",
    "print(in_tensor)\n",
    "print()\n",
    "\n",
    "kernals = torch.randint(0,3,(1,2,2,2),  dtype=float)\n",
    "kernals = torch.nn.Parameter(kernals)\n",
    "print('kernals')\n",
    "print(kernals)\n",
    "print()\n",
    "\n",
    "conv1 = nn.Conv2d(2,1, (2,2), padding=0, bias=False)\n",
    "conv1.weight = kernals\n",
    "\n",
    "out_tensor = conv1(in_tensor)\n",
    "print('out tensor')\n",
    "print(out_tensor)\n",
    "print()\n",
    "\n",
    "\n",
    "# verified this is the same as documentation \n",
    "# error computing 1st conv layer is probably from missing a computation?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'final_output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m         output_full \u001b[38;5;241m=\u001b[39m model(input_tensor)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# TODO: finish\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m match_count \u001b[38;5;241m=\u001b[39m (torch\u001b[38;5;241m.\u001b[39margmax(\u001b[43mfinal_output\u001b[49m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(output_full, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     10\u001b[0m label_hist \u001b[38;5;241m=\u001b[39m output_full\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMatches: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmatch_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_full\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'final_output' is not defined"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Test entire model : vertically split vs full\n",
    "'''\n",
    "\n",
    "with torch.no_grad():\n",
    "        output_full = model(input_tensor)\n",
    "\n",
    "# TODO: finish\n",
    "match_count = (torch.argmax(final_output, axis=1) == torch.argmax(output_full, axis=1)).sum().item()\n",
    "label_hist = output_full.sum(0)\n",
    "print(f'Matches: {match_count}/{output_full.size(0)}')\n",
    "print(f'histogram {label_hist}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'layer_names' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m imach \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m      6\u001b[0m ilayer\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m----> 7\u001b[0m input_channels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(configs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpartition\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[43mlayer_names\u001b[49m[ilayer]][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchannel_id\u001b[39m\u001b[38;5;124m'\u001b[39m][imach],\n\u001b[0;32m      8\u001b[0m                               device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(configs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[0;32m      9\u001b[0m output_channels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(configs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpartition\u001b[39m\u001b[38;5;124m'\u001b[39m][layer_names[ilayer]][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilter_id\u001b[39m\u001b[38;5;124m'\u001b[39m][imach],\n\u001b[0;32m     10\u001b[0m                                device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(configs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput channels \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_channels\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'layer_names' is not defined"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Test partial execution of a single layer\n",
    "'''\n",
    "\n",
    "imach = 1\n",
    "ilayer= 0\n",
    "input_channels = torch.tensor(configs['partition'][layer_names[ilayer]]['channel_id'][imach],\n",
    "                              device=torch.device(configs['device']))\n",
    "output_channels = torch.tensor(configs['partition'][layer_names[ilayer]]['filter_id'][imach],\n",
    "                               device=torch.device(configs['device']))\n",
    "print(f'Input channels {input_channels}')\n",
    "print(f'Output channels {output_channels}')\n",
    "\n",
    "# TODO: generalize this to more than conv layers \n",
    "a_layer = model.conv1\n",
    "split_layer = nn.Conv2d(len(input_channels),\n",
    "                    len(output_channels),\n",
    "                    kernel_size= a_layer.kernel_size,\n",
    "                    stride=a_layer.stride,\n",
    "                    padding=a_layer.padding, \n",
    "                    bias=a_layer.bias)\n",
    "\n",
    "split_layer.parameters = a_layer.weight.index_select(0, output_channels).index_select(1, input_channels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - 0\n",
      "1 - 1\n",
      "2 - 2\n",
      "3 - 3\n",
      "4 - 4\n",
      "5 - 5\n",
      "6 - 6\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    iterate using children method?\n",
    "'''\n",
    "\n",
    "# creates nested structure for each layer/block \n",
    "i = 0\n",
    "for name,module in enumerate( model_machines[imach].children()):\n",
    "    print(f'{i} - {name}')\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - name: ; type: ResNet\n",
      "1 - name: conv1; type: Conv2d\n",
      "2 - name: bn1; type: BatchNorm1d\n",
      "3 - name: layer1; type: Sequential\n",
      "4 - name: layer1.0; type: BasicBlock\n",
      "5 - name: layer1.0.conv1; type: Conv2d\n",
      "6 - name: layer1.0.conv2; type: Conv2d\n",
      "7 - name: layer1.0.bn1; type: BatchNorm1d\n",
      "8 - name: layer1.0.bn2; type: BatchNorm1d\n",
      "9 - name: layer1.0.shortcut; type: Sequential\n",
      "10 - name: layer1.1; type: BasicBlock\n",
      "11 - name: layer1.1.conv1; type: Conv2d\n",
      "12 - name: layer1.1.conv2; type: Conv2d\n",
      "13 - name: layer1.1.bn1; type: BatchNorm1d\n",
      "14 - name: layer1.1.bn2; type: BatchNorm1d\n",
      "15 - name: layer1.1.shortcut; type: Sequential\n",
      "16 - name: layer2; type: Sequential\n",
      "17 - name: layer2.0; type: BasicBlock\n",
      "18 - name: layer2.0.conv1; type: Conv2d\n",
      "19 - name: layer2.0.conv2; type: Conv2d\n",
      "20 - name: layer2.0.bn1; type: BatchNorm1d\n",
      "21 - name: layer2.0.bn2; type: BatchNorm1d\n",
      "22 - name: layer2.0.shortcut; type: Sequential\n",
      "23 - name: layer2.0.shortcut.0; type: Conv2d\n",
      "24 - name: layer2.0.shortcut.1; type: BatchNorm1d\n",
      "25 - name: layer2.1; type: BasicBlock\n",
      "26 - name: layer2.1.conv1; type: Conv2d\n",
      "27 - name: layer2.1.conv2; type: Conv2d\n",
      "28 - name: layer2.1.bn1; type: BatchNorm1d\n",
      "29 - name: layer2.1.bn2; type: BatchNorm1d\n",
      "30 - name: layer2.1.shortcut; type: Sequential\n",
      "31 - name: layer3; type: Sequential\n",
      "32 - name: layer3.0; type: BasicBlock\n",
      "33 - name: layer3.0.conv1; type: Conv2d\n",
      "34 - name: layer3.0.conv2; type: Conv2d\n",
      "35 - name: layer3.0.bn1; type: BatchNorm1d\n",
      "36 - name: layer3.0.bn2; type: BatchNorm1d\n",
      "37 - name: layer3.0.shortcut; type: Sequential\n",
      "38 - name: layer3.0.shortcut.0; type: Conv2d\n",
      "39 - name: layer3.0.shortcut.1; type: BatchNorm1d\n",
      "40 - name: layer3.1; type: BasicBlock\n",
      "41 - name: layer3.1.conv1; type: Conv2d\n",
      "42 - name: layer3.1.conv2; type: Conv2d\n",
      "43 - name: layer3.1.bn1; type: BatchNorm1d\n",
      "44 - name: layer3.1.bn2; type: BatchNorm1d\n",
      "45 - name: layer3.1.shortcut; type: Sequential\n",
      "46 - name: layer4; type: Sequential\n",
      "47 - name: layer4.0; type: BasicBlock\n",
      "48 - name: layer4.0.conv1; type: Conv2d\n",
      "49 - name: layer4.0.conv2; type: Conv2d\n",
      "50 - name: layer4.0.bn1; type: BatchNorm1d\n",
      "51 - name: layer4.0.bn2; type: BatchNorm1d\n",
      "52 - name: layer4.0.shortcut; type: Sequential\n",
      "53 - name: layer4.0.shortcut.0; type: Conv2d\n",
      "54 - name: layer4.0.shortcut.1; type: BatchNorm1d\n",
      "55 - name: layer4.1; type: BasicBlock\n",
      "56 - name: layer4.1.conv1; type: Conv2d\n",
      "57 - name: layer4.1.conv2; type: Conv2d\n",
      "58 - name: layer4.1.bn1; type: BatchNorm1d\n",
      "59 - name: layer4.1.bn2; type: BatchNorm1d\n",
      "60 - name: layer4.1.shortcut; type: Sequential\n",
      "61 - name: linear; type: Linear\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "    iterate using named_modules?\n",
    "'''\n",
    "\n",
    "# - gets redundant structure\n",
    "# - I think the easiest way to iterate through the whole model is using this list and skipping large layers/blocks of the model and only executing conv/bn/linear etc. \"layers\"\n",
    "# - provides good indicator of when to save input for skipped layer \n",
    "\n",
    "# not sure how to handle sequential layers... going to poke at this here \n",
    "i = 0\n",
    "for name, module in model.named_modules():\n",
    "    print(f'{i} - name: {name}; type: {type(module).__name__}')\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_inputs(num_machines, input):\n",
    "    '''\n",
    "        combines input tensors \n",
    "\n",
    "        Input:\n",
    "            num_machines - number of total machines\n",
    "            input - num_machines x num_machines list with inputs from previous layers collected from each machine. Indexed [destination][origin]\n",
    "\n",
    "        Output:\n",
    "            curr_input - a single tensor for this layer from the combined inputs\n",
    "    '''\n",
    "    curr_input = False \n",
    "    for i in range(num_machines):\n",
    "            if not input[imach][i] == []:\n",
    "                    if not torch.is_tensor(curr_input):\n",
    "                            curr_input = input[imach][i] # initialize curr_input with first input tensor \n",
    "                    else:\n",
    "                            curr_input += input[imach][i]\n",
    "    \n",
    "    return curr_input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.0000e+00, 1.1921e-07, 2.3842e-07],\n",
       "          [1.1921e-07, 1.1921e-07, 2.3842e-07],\n",
       "          [2.3842e-07, 2.3842e-07, 0.0000e+00]]]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    can you split certain operations across machines?\n",
    "    - I think the math says yes but there is some error values a on the order of 1e-7 \n",
    "    - could this be due to non-deterministic tensor operations? or is my math wrong?\n",
    "'''\n",
    "\n",
    "t = torch.rand(1, 3, 10,10)\n",
    "all = torch.sum(t, 1, True)\n",
    "\n",
    "\n",
    "kernel_size = 3\n",
    "\n",
    "full_avg = F.avg_pool2d(all, kernel_size)\n",
    "par_avg = torch.zeros(full_avg.shape)\n",
    "\n",
    "for i in range(t.shape[1]):\n",
    "    par_avg += F.avg_pool2d(t.index_select(1, torch.tensor(i)), kernel_size)\n",
    "\n",
    "torch.abs(full_avg - par_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Conv1 layer test\n",
    "'''\n",
    "\n",
    "# DIFFERENCE SHOULD BE 0 NOT 1E-7\n",
    "\n",
    "N_in = 1\n",
    "split_1 = nn.Conv2d(N_in,\n",
    "            model.conv1.weight.shape[0], # TODO does this need to be an int? (currently tensor)\n",
    "            kernel_size= model.conv1.kernel_size,\n",
    "            stride=model.conv1.stride,\n",
    "            padding=model.conv1.padding, \n",
    "            bias=False) # TODO: add bias during input collecting step on next layer \n",
    "split_1.weight = torch.nn.Parameter(model.conv1.weight.index_select(1, torch.tensor([0])))  \n",
    "out_split1 = split_1(input_tensor.index_select(1, torch.tensor([0])))\n",
    "\n",
    "split_2 = split_1\n",
    "split_2.weight = torch.nn.Parameter(model.conv1.weight.index_select(1, torch.tensor([1])))  \n",
    "out_split2 = split_2(input_tensor.index_select(1, torch.tensor([1])))\n",
    "\n",
    "split_3 = split_1\n",
    "split_3.weight = torch.nn.Parameter(model.conv1.weight.index_select(1, torch.tensor([2])))  \n",
    "out_split3 = split_3(input_tensor.index_select(1, torch.tensor([2])))\n",
    "\n",
    "split_out = torch.add(torch.add(out_split1, out_split2), out_split3)\n",
    "full_out = model.conv1(input_tensor)\n",
    "\n",
    "diff_output = torch.abs(full_out - split_out)\n",
    "max_diff = torch.max(diff_output)\n",
    "max_diff.sci_mode = True\n",
    "print(max_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split_out_from_module(input_tensor, model, module_select, configs, num_machines, module_names):\n",
    "    '''\n",
    "        assumes module is conv layer\n",
    "    '''\n",
    "\n",
    "    input_channel_map = configs['partition'][module_names[module_select] + '.weight']['channel_id']\n",
    "\n",
    "    dummy, module = next((x for i,x in enumerate(model.named_modules()) if i==module_select))\n",
    "\n",
    "    split_modules = [None]*num_machines\n",
    "    split_outputs = [None]*num_machines\n",
    "    for imachine in range(num_machines):\n",
    "\n",
    "        input_channels = torch.tensor(input_channel_map[imachine])\n",
    "        N_in = len(input_channels)\n",
    "        split_modules[imachine] = nn.Conv2d(N_in,\n",
    "                    module.weight.shape[0], # TODO does this need to be an int? (currently tensor)\n",
    "                    kernel_size= module.kernel_size,\n",
    "                    stride=module.stride,\n",
    "                    padding=module.padding, \n",
    "                    bias=False) # TODO: add bias during input collecting step on next layer \n",
    "        split_modules[imachine].weight = torch.nn.Parameter(module.weight.index_select(1, input_channels))  \n",
    "        split_outputs[imachine] = split_modules[imachine](input_tensor.index_select(1, input_channels))\n",
    "\n",
    "    return (split_modules, split_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cap_nb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
