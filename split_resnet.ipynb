{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    Load RESNET model and split it\\n        1. layer by layer\\n        2. [TODO] vertically \\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    Load RESNET model and split it\n",
    "        1. layer by layer\n",
    "        2. [TODO] vertically \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\natet\\anaconda3\\envs\\cap_nb\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from source.core.engine import MoP\n",
    "import source.core.run_partition as run_p\n",
    "from os import environ\n",
    "from source.utils.dataset import *\n",
    "from source.utils.misc import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "from source.models import resnet\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from source.utils import io\n",
    "from source.utils import testers\n",
    "from source.core import engine\n",
    "import json\n",
    "import itertools\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device :  \n",
      "model :  resnet18\n",
      "data_code :  cifar10\n",
      "num_classes :  10\n",
      "model_file :  test.pt\n",
      "epochs :  0\n",
      "batch_size :  128\n",
      "optimizer :  sgd\n",
      "lr_scheduler :  default\n",
      "learning_rate :  0.01\n",
      "seed :  1234\n",
      "sparsity_type :  kernel\n",
      "prune_ratio :  1\n",
      "admm :  True\n",
      "admm_epochs :  3\n",
      "rho :  0.0001\n",
      "multi_rho :  True\n",
      "retrain_bs :  128\n",
      "retrain_lr :  0.005\n",
      "retrain_ep :  50\n",
      "retrain_opt :  default\n",
      "xentropy_weight :  1.0\n",
      "warmup :  False\n",
      "warmup_lr :  0.001\n",
      "warmup_epochs :  10\n",
      "mix_up :  True\n",
      "alpha :  0.3\n",
      "smooth :  False\n",
      "smooth_eps :  0\n",
      "save_last_model_only :  False\n",
      "num_partition :  1\n",
      "layer_type :  regular\n",
      "bn_type :  masked\n",
      "par_first_layer :  False\n",
      "comm_outsize :  False\n",
      "lambda_comm :  0\n",
      "lambda_comp :  0\n",
      "distill_model :  \n",
      "distill_loss :  kl\n",
      "distill_temp :  30\n",
      "distill_alpha :  1\n"
     ]
    }
   ],
   "source": [
    "# setup config\n",
    "dataset='cifar10'\n",
    "environ[\"config\"] = f\"config/{dataset}.yaml\"\n",
    "\n",
    "configs = run_p.main()\n",
    "\n",
    "configs[\"device\"] = \"cpu\"\n",
    "configs['load_model'] = \"cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001.pt\"\n",
    "configs[\"num_partition\"] = '4' #'resnet18-v2.yaml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (linear): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# load data and load or train model\n",
    "model = get_model_from_code(configs).to(configs['device']) # grabs model architecture from ./source/models/escnet.py\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "   1st section of resnet model \n",
    "'''\n",
    "class ResnetBlockOne(nn.Module):\n",
    "    def __init__(self, block, num_blocks, conv_layer, bn_layer, num_classes=10, num_filters=512, bn_partition=[1]*9):\n",
    "        super(ResnetBlockOne, self).__init__()\n",
    "\n",
    "        self.in_planes = 64\n",
    "        self.conv_layer = conv_layer\n",
    "        self.bn_layer = bn_layer\n",
    "        self.shrink = num_filters/512\n",
    "        self.bn_partition = bn_partition\n",
    "        \n",
    "        self.conv1 = conv_layer(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        num_bn = self.bn_partition.pop(0)\n",
    "        self.bn1 = bn_layer(64) if num_bn==1 else bn_layer(64, num_bn)\n",
    "\n",
    "        self.layer1 = self._make_layer(block, int(64*self.shrink),  num_blocks[0], stride=1)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        # TODO: find better way to implement this method using inheretence and getting from ResNet class\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, self.conv_layer, self.bn_layer, stride, self.bn_partition.pop(0)))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # override the the foward pass to only include the first modules \n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "'''\n",
    "   2nd section of resnet model \n",
    "'''\n",
    "class ResnetBlockTwo(nn.Module):\n",
    "    def __init__(self, block, num_blocks, conv_layer, bn_layer, num_classes=10, num_filters=512, bn_partition=[1]*9):\n",
    "        super(ResnetBlockTwo, self).__init__()\n",
    "\n",
    "        self.in_planes = 64\n",
    "        self.conv_layer = conv_layer\n",
    "        self.bn_layer = bn_layer\n",
    "        self.shrink = num_filters/512\n",
    "        self.bn_partition = bn_partition\n",
    "\n",
    "        self.layer2 = self._make_layer(block, int(128*self.shrink), num_blocks[1], stride=2)\n",
    "    \n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        # TODO: find better way to implement this method using inheretence and getting from ResNet class\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, self.conv_layer, self.bn_layer, stride, self.bn_partition.pop(0)))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer2(x)\n",
    "        return out\n",
    "\n",
    "'''\n",
    "    3rd section of resenet model \n",
    "'''\n",
    "class ResnetBlockThree(nn.Module):\n",
    "    def __init__(self, block, num_blocks, conv_layer, bn_layer, num_classes=10, num_filters=512, bn_partition=[1]*9):\n",
    "        super(ResnetBlockThree, self).__init__()\n",
    "\n",
    "        self.in_planes = 128\n",
    "        self.conv_layer = conv_layer\n",
    "        self.bn_layer = bn_layer\n",
    "        self.shrink = num_filters/512\n",
    "        self.bn_partition = bn_partition\n",
    "\n",
    "        self.layer3 = self._make_layer(block, int(256*self.shrink), num_blocks[2], stride=2)\n",
    "    \n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        # TODO: find better way to implement this method using inheretence and getting from ResNet class\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, self.conv_layer, self.bn_layer, stride, self.bn_partition.pop(0)))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer3(x)\n",
    "        return out\n",
    "\n",
    "'''\n",
    "    4-th section of resenet model \n",
    "'''\n",
    "class ResnetBlockFour(nn.Module):\n",
    "    def __init__(self, block, num_blocks, conv_layer, bn_layer, num_classes=10, num_filters=512, bn_partition=[1]*9):\n",
    "        super(ResnetBlockFour, self).__init__()\n",
    "\n",
    "        self.in_planes = 256\n",
    "        self.conv_layer = conv_layer\n",
    "        self.bn_layer = bn_layer\n",
    "        self.shrink = num_filters/512\n",
    "        self.bn_partition = bn_partition\n",
    "\n",
    "        self.layer4 = self._make_layer(block, num_filters, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(num_filters*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        # TODO: find better way to implement this method using inheretence and getting from ResNet class\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, self.conv_layer, self.bn_layer, stride, self.bn_partition.pop(0)))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer4(x)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResnetBlockFour(\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (linear): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# get framework for each layer of resnet\n",
    "\n",
    "# inputs can be found from looking at ./source/util/misc/get_model_from_code\n",
    "num_classes=configs['num_classes']\n",
    "bn_layers = get_bn_layers('regular') # basic block layer\n",
    "conv_layers = get_layers(configs['layer_type'])\n",
    "\n",
    "# resnet18 inputs from resnet.py\n",
    "layer_1 =  ResnetBlockOne(resnet.BasicBlock, [2,2,2,2],conv_layers, bn_layers, num_classes=num_classes) # also includes bn1 and conv1 \n",
    "layer_2 =  ResnetBlockTwo(resnet.BasicBlock, [2,2,2,2],conv_layers, bn_layers, num_classes=num_classes) # also includes bn1 and conv1 \n",
    "layer_3 =  ResnetBlockThree(resnet.BasicBlock, [2,2,2,2],conv_layers, bn_layers, num_classes=num_classes) # also includes bn1 and conv1 \n",
    "layer_4 =  ResnetBlockFour(resnet.BasicBlock, [2,2,2,2],conv_layers, bn_layers, num_classes=num_classes) # also includes bn1 and conv1 \n",
    "\n",
    "print(layer_4)\n",
    "#block2 = |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not found:  layer2.0.conv1.weight\n",
      "not found:  layer2.0.conv2.weight\n",
      "not found:  layer2.0.bn1.weight\n",
      "not found:  layer2.0.bn1.bias\n",
      "not found:  layer2.0.bn1.running_mean\n",
      "not found:  layer2.0.bn1.running_var\n",
      "not found:  layer2.0.bn1.num_batches_tracked\n",
      "not found:  layer2.0.bn2.weight\n",
      "not found:  layer2.0.bn2.bias\n",
      "not found:  layer2.0.bn2.running_mean\n",
      "not found:  layer2.0.bn2.running_var\n",
      "not found:  layer2.0.bn2.num_batches_tracked\n",
      "not found:  layer2.0.shortcut.0.weight\n",
      "not found:  layer2.0.shortcut.1.weight\n",
      "not found:  layer2.0.shortcut.1.bias\n",
      "not found:  layer2.0.shortcut.1.running_mean\n",
      "not found:  layer2.0.shortcut.1.running_var\n",
      "not found:  layer2.0.shortcut.1.num_batches_tracked\n",
      "not found:  layer2.1.conv1.weight\n",
      "not found:  layer2.1.conv2.weight\n",
      "not found:  layer2.1.bn1.weight\n",
      "not found:  layer2.1.bn1.bias\n",
      "not found:  layer2.1.bn1.running_mean\n",
      "not found:  layer2.1.bn1.running_var\n",
      "not found:  layer2.1.bn1.num_batches_tracked\n",
      "not found:  layer2.1.bn2.weight\n",
      "not found:  layer2.1.bn2.bias\n",
      "not found:  layer2.1.bn2.running_mean\n",
      "not found:  layer2.1.bn2.running_var\n",
      "not found:  layer2.1.bn2.num_batches_tracked\n",
      "not found:  layer3.0.conv1.weight\n",
      "not found:  layer3.0.conv2.weight\n",
      "not found:  layer3.0.bn1.weight\n",
      "not found:  layer3.0.bn1.bias\n",
      "not found:  layer3.0.bn1.running_mean\n",
      "not found:  layer3.0.bn1.running_var\n",
      "not found:  layer3.0.bn1.num_batches_tracked\n",
      "not found:  layer3.0.bn2.weight\n",
      "not found:  layer3.0.bn2.bias\n",
      "not found:  layer3.0.bn2.running_mean\n",
      "not found:  layer3.0.bn2.running_var\n",
      "not found:  layer3.0.bn2.num_batches_tracked\n",
      "not found:  layer3.0.shortcut.0.weight\n",
      "not found:  layer3.0.shortcut.1.weight\n",
      "not found:  layer3.0.shortcut.1.bias\n",
      "not found:  layer3.0.shortcut.1.running_mean\n",
      "not found:  layer3.0.shortcut.1.running_var\n",
      "not found:  layer3.0.shortcut.1.num_batches_tracked\n",
      "not found:  layer3.1.conv1.weight\n",
      "not found:  layer3.1.conv2.weight\n",
      "not found:  layer3.1.bn1.weight\n",
      "not found:  layer3.1.bn1.bias\n",
      "not found:  layer3.1.bn1.running_mean\n",
      "not found:  layer3.1.bn1.running_var\n",
      "not found:  layer3.1.bn1.num_batches_tracked\n",
      "not found:  layer3.1.bn2.weight\n",
      "not found:  layer3.1.bn2.bias\n",
      "not found:  layer3.1.bn2.running_mean\n",
      "not found:  layer3.1.bn2.running_var\n",
      "not found:  layer3.1.bn2.num_batches_tracked\n",
      "not found:  layer4.0.conv1.weight\n",
      "not found:  layer4.0.conv2.weight\n",
      "not found:  layer4.0.bn1.weight\n",
      "not found:  layer4.0.bn1.bias\n",
      "not found:  layer4.0.bn1.running_mean\n",
      "not found:  layer4.0.bn1.running_var\n",
      "not found:  layer4.0.bn1.num_batches_tracked\n",
      "not found:  layer4.0.bn2.weight\n",
      "not found:  layer4.0.bn2.bias\n",
      "not found:  layer4.0.bn2.running_mean\n",
      "not found:  layer4.0.bn2.running_var\n",
      "not found:  layer4.0.bn2.num_batches_tracked\n",
      "not found:  layer4.0.shortcut.0.weight\n",
      "not found:  layer4.0.shortcut.1.weight\n",
      "not found:  layer4.0.shortcut.1.bias\n",
      "not found:  layer4.0.shortcut.1.running_mean\n",
      "not found:  layer4.0.shortcut.1.running_var\n",
      "not found:  layer4.0.shortcut.1.num_batches_tracked\n",
      "not found:  layer4.1.conv1.weight\n",
      "not found:  layer4.1.conv2.weight\n",
      "not found:  layer4.1.bn1.weight\n",
      "not found:  layer4.1.bn1.bias\n",
      "not found:  layer4.1.bn1.running_mean\n",
      "not found:  layer4.1.bn1.running_var\n",
      "not found:  layer4.1.bn1.num_batches_tracked\n",
      "not found:  layer4.1.bn2.weight\n",
      "not found:  layer4.1.bn2.bias\n",
      "not found:  layer4.1.bn2.running_mean\n",
      "not found:  layer4.1.bn2.running_var\n",
      "not found:  layer4.1.bn2.num_batches_tracked\n",
      "not found:  linear.weight\n",
      "not found:  linear.bias\n",
      "not found:  conv1.weight\n",
      "not found:  bn1.weight\n",
      "not found:  bn1.bias\n",
      "not found:  bn1.running_mean\n",
      "not found:  bn1.running_var\n",
      "not found:  bn1.num_batches_tracked\n",
      "not found:  layer1.0.conv1.weight\n",
      "not found:  layer1.0.conv2.weight\n",
      "not found:  layer1.0.bn1.weight\n",
      "not found:  layer1.0.bn1.bias\n",
      "not found:  layer1.0.bn1.running_mean\n",
      "not found:  layer1.0.bn1.running_var\n",
      "not found:  layer1.0.bn1.num_batches_tracked\n",
      "not found:  layer1.0.bn2.weight\n",
      "not found:  layer1.0.bn2.bias\n",
      "not found:  layer1.0.bn2.running_mean\n",
      "not found:  layer1.0.bn2.running_var\n",
      "not found:  layer1.0.bn2.num_batches_tracked\n",
      "not found:  layer1.1.conv1.weight\n",
      "not found:  layer1.1.conv2.weight\n",
      "not found:  layer1.1.bn1.weight\n",
      "not found:  layer1.1.bn1.bias\n",
      "not found:  layer1.1.bn1.running_mean\n",
      "not found:  layer1.1.bn1.running_var\n",
      "not found:  layer1.1.bn1.num_batches_tracked\n",
      "not found:  layer1.1.bn2.weight\n",
      "not found:  layer1.1.bn2.bias\n",
      "not found:  layer1.1.bn2.running_mean\n",
      "not found:  layer1.1.bn2.running_var\n",
      "not found:  layer1.1.bn2.num_batches_tracked\n",
      "not found:  layer3.0.conv1.weight\n",
      "not found:  layer3.0.conv2.weight\n",
      "not found:  layer3.0.bn1.weight\n",
      "not found:  layer3.0.bn1.bias\n",
      "not found:  layer3.0.bn1.running_mean\n",
      "not found:  layer3.0.bn1.running_var\n",
      "not found:  layer3.0.bn1.num_batches_tracked\n",
      "not found:  layer3.0.bn2.weight\n",
      "not found:  layer3.0.bn2.bias\n",
      "not found:  layer3.0.bn2.running_mean\n",
      "not found:  layer3.0.bn2.running_var\n",
      "not found:  layer3.0.bn2.num_batches_tracked\n",
      "not found:  layer3.0.shortcut.0.weight\n",
      "not found:  layer3.0.shortcut.1.weight\n",
      "not found:  layer3.0.shortcut.1.bias\n",
      "not found:  layer3.0.shortcut.1.running_mean\n",
      "not found:  layer3.0.shortcut.1.running_var\n",
      "not found:  layer3.0.shortcut.1.num_batches_tracked\n",
      "not found:  layer3.1.conv1.weight\n",
      "not found:  layer3.1.conv2.weight\n",
      "not found:  layer3.1.bn1.weight\n",
      "not found:  layer3.1.bn1.bias\n",
      "not found:  layer3.1.bn1.running_mean\n",
      "not found:  layer3.1.bn1.running_var\n",
      "not found:  layer3.1.bn1.num_batches_tracked\n",
      "not found:  layer3.1.bn2.weight\n",
      "not found:  layer3.1.bn2.bias\n",
      "not found:  layer3.1.bn2.running_mean\n",
      "not found:  layer3.1.bn2.running_var\n",
      "not found:  layer3.1.bn2.num_batches_tracked\n",
      "not found:  layer4.0.conv1.weight\n",
      "not found:  layer4.0.conv2.weight\n",
      "not found:  layer4.0.bn1.weight\n",
      "not found:  layer4.0.bn1.bias\n",
      "not found:  layer4.0.bn1.running_mean\n",
      "not found:  layer4.0.bn1.running_var\n",
      "not found:  layer4.0.bn1.num_batches_tracked\n",
      "not found:  layer4.0.bn2.weight\n",
      "not found:  layer4.0.bn2.bias\n",
      "not found:  layer4.0.bn2.running_mean\n",
      "not found:  layer4.0.bn2.running_var\n",
      "not found:  layer4.0.bn2.num_batches_tracked\n",
      "not found:  layer4.0.shortcut.0.weight\n",
      "not found:  layer4.0.shortcut.1.weight\n",
      "not found:  layer4.0.shortcut.1.bias\n",
      "not found:  layer4.0.shortcut.1.running_mean\n",
      "not found:  layer4.0.shortcut.1.running_var\n",
      "not found:  layer4.0.shortcut.1.num_batches_tracked\n",
      "not found:  layer4.1.conv1.weight\n",
      "not found:  layer4.1.conv2.weight\n",
      "not found:  layer4.1.bn1.weight\n",
      "not found:  layer4.1.bn1.bias\n",
      "not found:  layer4.1.bn1.running_mean\n",
      "not found:  layer4.1.bn1.running_var\n",
      "not found:  layer4.1.bn1.num_batches_tracked\n",
      "not found:  layer4.1.bn2.weight\n",
      "not found:  layer4.1.bn2.bias\n",
      "not found:  layer4.1.bn2.running_mean\n",
      "not found:  layer4.1.bn2.running_var\n",
      "not found:  layer4.1.bn2.num_batches_tracked\n",
      "not found:  linear.weight\n",
      "not found:  linear.bias\n",
      "not found:  conv1.weight\n",
      "not found:  bn1.weight\n",
      "not found:  bn1.bias\n",
      "not found:  bn1.running_mean\n",
      "not found:  bn1.running_var\n",
      "not found:  bn1.num_batches_tracked\n",
      "not found:  layer1.0.conv1.weight\n",
      "not found:  layer1.0.conv2.weight\n",
      "not found:  layer1.0.bn1.weight\n",
      "not found:  layer1.0.bn1.bias\n",
      "not found:  layer1.0.bn1.running_mean\n",
      "not found:  layer1.0.bn1.running_var\n",
      "not found:  layer1.0.bn1.num_batches_tracked\n",
      "not found:  layer1.0.bn2.weight\n",
      "not found:  layer1.0.bn2.bias\n",
      "not found:  layer1.0.bn2.running_mean\n",
      "not found:  layer1.0.bn2.running_var\n",
      "not found:  layer1.0.bn2.num_batches_tracked\n",
      "not found:  layer1.1.conv1.weight\n",
      "not found:  layer1.1.conv2.weight\n",
      "not found:  layer1.1.bn1.weight\n",
      "not found:  layer1.1.bn1.bias\n",
      "not found:  layer1.1.bn1.running_mean\n",
      "not found:  layer1.1.bn1.running_var\n",
      "not found:  layer1.1.bn1.num_batches_tracked\n",
      "not found:  layer1.1.bn2.weight\n",
      "not found:  layer1.1.bn2.bias\n",
      "not found:  layer1.1.bn2.running_mean\n",
      "not found:  layer1.1.bn2.running_var\n",
      "not found:  layer1.1.bn2.num_batches_tracked\n",
      "not found:  layer2.0.conv1.weight\n",
      "not found:  layer2.0.conv2.weight\n",
      "not found:  layer2.0.bn1.weight\n",
      "not found:  layer2.0.bn1.bias\n",
      "not found:  layer2.0.bn1.running_mean\n",
      "not found:  layer2.0.bn1.running_var\n",
      "not found:  layer2.0.bn1.num_batches_tracked\n",
      "not found:  layer2.0.bn2.weight\n",
      "not found:  layer2.0.bn2.bias\n",
      "not found:  layer2.0.bn2.running_mean\n",
      "not found:  layer2.0.bn2.running_var\n",
      "not found:  layer2.0.bn2.num_batches_tracked\n",
      "not found:  layer2.0.shortcut.0.weight\n",
      "not found:  layer2.0.shortcut.1.weight\n",
      "not found:  layer2.0.shortcut.1.bias\n",
      "not found:  layer2.0.shortcut.1.running_mean\n",
      "not found:  layer2.0.shortcut.1.running_var\n",
      "not found:  layer2.0.shortcut.1.num_batches_tracked\n",
      "not found:  layer2.1.conv1.weight\n",
      "not found:  layer2.1.conv2.weight\n",
      "not found:  layer2.1.bn1.weight\n",
      "not found:  layer2.1.bn1.bias\n",
      "not found:  layer2.1.bn1.running_mean\n",
      "not found:  layer2.1.bn1.running_var\n",
      "not found:  layer2.1.bn1.num_batches_tracked\n",
      "not found:  layer2.1.bn2.weight\n",
      "not found:  layer2.1.bn2.bias\n",
      "not found:  layer2.1.bn2.running_mean\n",
      "not found:  layer2.1.bn2.running_var\n",
      "not found:  layer2.1.bn2.num_batches_tracked\n",
      "not found:  layer4.0.conv1.weight\n",
      "not found:  layer4.0.conv2.weight\n",
      "not found:  layer4.0.bn1.weight\n",
      "not found:  layer4.0.bn1.bias\n",
      "not found:  layer4.0.bn1.running_mean\n",
      "not found:  layer4.0.bn1.running_var\n",
      "not found:  layer4.0.bn1.num_batches_tracked\n",
      "not found:  layer4.0.bn2.weight\n",
      "not found:  layer4.0.bn2.bias\n",
      "not found:  layer4.0.bn2.running_mean\n",
      "not found:  layer4.0.bn2.running_var\n",
      "not found:  layer4.0.bn2.num_batches_tracked\n",
      "not found:  layer4.0.shortcut.0.weight\n",
      "not found:  layer4.0.shortcut.1.weight\n",
      "not found:  layer4.0.shortcut.1.bias\n",
      "not found:  layer4.0.shortcut.1.running_mean\n",
      "not found:  layer4.0.shortcut.1.running_var\n",
      "not found:  layer4.0.shortcut.1.num_batches_tracked\n",
      "not found:  layer4.1.conv1.weight\n",
      "not found:  layer4.1.conv2.weight\n",
      "not found:  layer4.1.bn1.weight\n",
      "not found:  layer4.1.bn1.bias\n",
      "not found:  layer4.1.bn1.running_mean\n",
      "not found:  layer4.1.bn1.running_var\n",
      "not found:  layer4.1.bn1.num_batches_tracked\n",
      "not found:  layer4.1.bn2.weight\n",
      "not found:  layer4.1.bn2.bias\n",
      "not found:  layer4.1.bn2.running_mean\n",
      "not found:  layer4.1.bn2.running_var\n",
      "not found:  layer4.1.bn2.num_batches_tracked\n",
      "not found:  linear.weight\n",
      "not found:  linear.bias\n",
      "not found:  conv1.weight\n",
      "not found:  bn1.weight\n",
      "not found:  bn1.bias\n",
      "not found:  bn1.running_mean\n",
      "not found:  bn1.running_var\n",
      "not found:  bn1.num_batches_tracked\n",
      "not found:  layer1.0.conv1.weight\n",
      "not found:  layer1.0.conv2.weight\n",
      "not found:  layer1.0.bn1.weight\n",
      "not found:  layer1.0.bn1.bias\n",
      "not found:  layer1.0.bn1.running_mean\n",
      "not found:  layer1.0.bn1.running_var\n",
      "not found:  layer1.0.bn1.num_batches_tracked\n",
      "not found:  layer1.0.bn2.weight\n",
      "not found:  layer1.0.bn2.bias\n",
      "not found:  layer1.0.bn2.running_mean\n",
      "not found:  layer1.0.bn2.running_var\n",
      "not found:  layer1.0.bn2.num_batches_tracked\n",
      "not found:  layer1.1.conv1.weight\n",
      "not found:  layer1.1.conv2.weight\n",
      "not found:  layer1.1.bn1.weight\n",
      "not found:  layer1.1.bn1.bias\n",
      "not found:  layer1.1.bn1.running_mean\n",
      "not found:  layer1.1.bn1.running_var\n",
      "not found:  layer1.1.bn1.num_batches_tracked\n",
      "not found:  layer1.1.bn2.weight\n",
      "not found:  layer1.1.bn2.bias\n",
      "not found:  layer1.1.bn2.running_mean\n",
      "not found:  layer1.1.bn2.running_var\n",
      "not found:  layer1.1.bn2.num_batches_tracked\n",
      "not found:  layer2.0.conv1.weight\n",
      "not found:  layer2.0.conv2.weight\n",
      "not found:  layer2.0.bn1.weight\n",
      "not found:  layer2.0.bn1.bias\n",
      "not found:  layer2.0.bn1.running_mean\n",
      "not found:  layer2.0.bn1.running_var\n",
      "not found:  layer2.0.bn1.num_batches_tracked\n",
      "not found:  layer2.0.bn2.weight\n",
      "not found:  layer2.0.bn2.bias\n",
      "not found:  layer2.0.bn2.running_mean\n",
      "not found:  layer2.0.bn2.running_var\n",
      "not found:  layer2.0.bn2.num_batches_tracked\n",
      "not found:  layer2.0.shortcut.0.weight\n",
      "not found:  layer2.0.shortcut.1.weight\n",
      "not found:  layer2.0.shortcut.1.bias\n",
      "not found:  layer2.0.shortcut.1.running_mean\n",
      "not found:  layer2.0.shortcut.1.running_var\n",
      "not found:  layer2.0.shortcut.1.num_batches_tracked\n",
      "not found:  layer2.1.conv1.weight\n",
      "not found:  layer2.1.conv2.weight\n",
      "not found:  layer2.1.bn1.weight\n",
      "not found:  layer2.1.bn1.bias\n",
      "not found:  layer2.1.bn1.running_mean\n",
      "not found:  layer2.1.bn1.running_var\n",
      "not found:  layer2.1.bn1.num_batches_tracked\n",
      "not found:  layer2.1.bn2.weight\n",
      "not found:  layer2.1.bn2.bias\n",
      "not found:  layer2.1.bn2.running_mean\n",
      "not found:  layer2.1.bn2.running_var\n",
      "not found:  layer2.1.bn2.num_batches_tracked\n",
      "not found:  layer3.0.conv1.weight\n",
      "not found:  layer3.0.conv2.weight\n",
      "not found:  layer3.0.bn1.weight\n",
      "not found:  layer3.0.bn1.bias\n",
      "not found:  layer3.0.bn1.running_mean\n",
      "not found:  layer3.0.bn1.running_var\n",
      "not found:  layer3.0.bn1.num_batches_tracked\n",
      "not found:  layer3.0.bn2.weight\n",
      "not found:  layer3.0.bn2.bias\n",
      "not found:  layer3.0.bn2.running_mean\n",
      "not found:  layer3.0.bn2.running_var\n",
      "not found:  layer3.0.bn2.num_batches_tracked\n",
      "not found:  layer3.0.shortcut.0.weight\n",
      "not found:  layer3.0.shortcut.1.weight\n",
      "not found:  layer3.0.shortcut.1.bias\n",
      "not found:  layer3.0.shortcut.1.running_mean\n",
      "not found:  layer3.0.shortcut.1.running_var\n",
      "not found:  layer3.0.shortcut.1.num_batches_tracked\n",
      "not found:  layer3.1.conv1.weight\n",
      "not found:  layer3.1.conv2.weight\n",
      "not found:  layer3.1.bn1.weight\n",
      "not found:  layer3.1.bn1.bias\n",
      "not found:  layer3.1.bn1.running_mean\n",
      "not found:  layer3.1.bn1.running_var\n",
      "not found:  layer3.1.bn1.num_batches_tracked\n",
      "not found:  layer3.1.bn2.weight\n",
      "not found:  layer3.1.bn2.bias\n",
      "not found:  layer3.1.bn2.running_mean\n",
      "not found:  layer3.1.bn2.running_var\n",
      "not found:  layer3.1.bn2.num_batches_tracked\n"
     ]
    }
   ],
   "source": [
    "split_model = [layer_1, layer_2, layer_3, layer_4]\n",
    "\n",
    "# load model params into dictionary\n",
    "state_dict = torch.load(io.get_model_path(\"{}\".format(configs[\"load_model\"])), map_location=configs['device'])\n",
    "\n",
    "# add params to split\n",
    "for l in split_model:\n",
    "    l = io.load_state_dict(l, \n",
    "                    state_dict['model_state_dict'] if 'model_state_dict' in state_dict \n",
    "                    else state_dict['state_dict'] if 'state_dict' in state_dict else state_dict,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['conv1.weight', 'bn1.weight', 'bn1.bias', 'bn1.running_mean', 'bn1.running_var', 'bn1.num_batches_tracked', 'layer1.0.conv1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn1.weight', 'layer1.0.bn1.bias', 'layer1.0.bn1.running_mean', 'layer1.0.bn1.running_var', 'layer1.0.bn1.num_batches_tracked', 'layer1.0.bn2.weight', 'layer1.0.bn2.bias', 'layer1.0.bn2.running_mean', 'layer1.0.bn2.running_var', 'layer1.0.bn2.num_batches_tracked', 'layer1.1.conv1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn1.weight', 'layer1.1.bn1.bias', 'layer1.1.bn1.running_mean', 'layer1.1.bn1.running_var', 'layer1.1.bn1.num_batches_tracked', 'layer1.1.bn2.weight', 'layer1.1.bn2.bias', 'layer1.1.bn2.running_mean', 'layer1.1.bn2.running_var', 'layer1.1.bn2.num_batches_tracked', 'layer2.0.conv1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn1.weight', 'layer2.0.bn1.bias', 'layer2.0.bn1.running_mean', 'layer2.0.bn1.running_var', 'layer2.0.bn1.num_batches_tracked', 'layer2.0.bn2.weight', 'layer2.0.bn2.bias', 'layer2.0.bn2.running_mean', 'layer2.0.bn2.running_var', 'layer2.0.bn2.num_batches_tracked', 'layer2.0.shortcut.0.weight', 'layer2.0.shortcut.1.weight', 'layer2.0.shortcut.1.bias', 'layer2.0.shortcut.1.running_mean', 'layer2.0.shortcut.1.running_var', 'layer2.0.shortcut.1.num_batches_tracked', 'layer2.1.conv1.weight', 'layer2.1.conv2.weight', 'layer2.1.bn1.weight', 'layer2.1.bn1.bias', 'layer2.1.bn1.running_mean', 'layer2.1.bn1.running_var', 'layer2.1.bn1.num_batches_tracked', 'layer2.1.bn2.weight', 'layer2.1.bn2.bias', 'layer2.1.bn2.running_mean', 'layer2.1.bn2.running_var', 'layer2.1.bn2.num_batches_tracked', 'layer3.0.conv1.weight', 'layer3.0.conv2.weight', 'layer3.0.bn1.weight', 'layer3.0.bn1.bias', 'layer3.0.bn1.running_mean', 'layer3.0.bn1.running_var', 'layer3.0.bn1.num_batches_tracked', 'layer3.0.bn2.weight', 'layer3.0.bn2.bias', 'layer3.0.bn2.running_mean', 'layer3.0.bn2.running_var', 'layer3.0.bn2.num_batches_tracked', 'layer3.0.shortcut.0.weight', 'layer3.0.shortcut.1.weight', 'layer3.0.shortcut.1.bias', 'layer3.0.shortcut.1.running_mean', 'layer3.0.shortcut.1.running_var', 'layer3.0.shortcut.1.num_batches_tracked', 'layer3.1.conv1.weight', 'layer3.1.conv2.weight', 'layer3.1.bn1.weight', 'layer3.1.bn1.bias', 'layer3.1.bn1.running_mean', 'layer3.1.bn1.running_var', 'layer3.1.bn1.num_batches_tracked', 'layer3.1.bn2.weight', 'layer3.1.bn2.bias', 'layer3.1.bn2.running_mean', 'layer3.1.bn2.running_var', 'layer3.1.bn2.num_batches_tracked', 'layer4.0.conv1.weight', 'layer4.0.conv2.weight', 'layer4.0.bn1.weight', 'layer4.0.bn1.bias', 'layer4.0.bn1.running_mean', 'layer4.0.bn1.running_var', 'layer4.0.bn1.num_batches_tracked', 'layer4.0.bn2.weight', 'layer4.0.bn2.bias', 'layer4.0.bn2.running_mean', 'layer4.0.bn2.running_var', 'layer4.0.bn2.num_batches_tracked', 'layer4.0.shortcut.0.weight', 'layer4.0.shortcut.1.weight', 'layer4.0.shortcut.1.bias', 'layer4.0.shortcut.1.running_mean', 'layer4.0.shortcut.1.running_var', 'layer4.0.shortcut.1.num_batches_tracked', 'layer4.1.conv1.weight', 'layer4.1.conv2.weight', 'layer4.1.bn1.weight', 'layer4.1.bn1.bias', 'layer4.1.bn1.running_mean', 'layer4.1.bn1.running_var', 'layer4.1.bn1.num_batches_tracked', 'layer4.1.bn2.weight', 'layer4.1.bn2.bias', 'layer4.1.bn2.running_mean', 'layer4.1.bn2.running_var', 'layer4.1.bn2.num_batches_tracked', 'linear.weight', 'linear.bias'])\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "# look at state dict keys\n",
    "print(state_dict.keys())\n",
    "for i in split_model:\n",
    "    print(len(l.state_dict().keys()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load weights into full model\n",
    "model = io.load_state_dict(model, \n",
    "                    state_dict['model_state_dict'] if 'model_state_dict' in state_dict \n",
    "                    else state_dict['state_dict'] if 'state_dict' in state_dict else state_dict,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches: 1000/1000\n",
      "histogram tensor([  -23.9638,  -911.0007,   284.2019,  -974.2556,   462.6697, -1298.4489,\n",
      "         3057.1187,  -917.3174,  -234.2508, -1036.4056])\n"
     ]
    }
   ],
   "source": [
    "# compare outputs\n",
    "\n",
    "input = torch.rand(1000, 3, 32, 32, device=torch.device(configs['device'])) # 1k images, 3 channels, 32x32 image (cifar100) \n",
    "\n",
    "# put models into eval mode and on device\n",
    "model.eval()\n",
    "model.to(configs['device'])\n",
    "for l in split_model:\n",
    "    l.eval()\n",
    "    l.to(configs['device'])\n",
    "\n",
    "# make inference \n",
    "with torch.no_grad():\n",
    "        output_full = model(input)\n",
    "\n",
    "        output_split = input\n",
    "        for l in split_model:\n",
    "                output_split = l(output_split)\n",
    "\n",
    "\n",
    "match_count = (torch.argmax(output_split, axis=1) == torch.argmax(output_full, axis=1)).sum().item()\n",
    "label_hist = output_full.sum(0)\n",
    "print(f'Matches: {match_count}/{output_full.size(0)}')\n",
    "print(f'histogram {label_hist}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-1.2916e-04, -2.4399e-04, -3.8814e-05,  ..., -6.5398e-04,\n",
       "           -5.9876e-04, -3.8099e-04],\n",
       "          [-3.3047e-04, -4.6868e-04, -1.0998e-04,  ..., -7.0495e-04,\n",
       "           -6.9534e-04, -4.6422e-04],\n",
       "          [-4.2830e-04, -3.5908e-04, -3.8658e-04,  ..., -5.2300e-04,\n",
       "           -5.2077e-04, -4.1023e-04],\n",
       "          ...,\n",
       "          [-1.3249e-04, -4.5732e-04, -3.7884e-04,  ..., -2.9950e-04,\n",
       "           -5.9811e-04, -6.3014e-04],\n",
       "          [-2.9823e-04, -7.0803e-04, -7.0867e-04,  ..., -4.8670e-04,\n",
       "           -5.0802e-04, -7.9599e-04],\n",
       "          [-5.1725e-04, -6.4022e-04, -5.7712e-04,  ..., -5.1575e-04,\n",
       "           -8.4950e-04, -5.2601e-04]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "         [[ 2.7669e-04,  2.1422e-04,  4.0371e-04,  ...,  5.9810e-04,\n",
       "            4.2786e-04,  4.1975e-04],\n",
       "          [ 3.1897e-04,  3.9239e-05,  2.6188e-04,  ...,  3.5061e-04,\n",
       "            5.5192e-04,  5.7154e-05],\n",
       "          [ 3.0840e-04,  2.3344e-04,  3.1526e-04,  ...,  9.0517e-05,\n",
       "            3.1246e-05,  2.3400e-04],\n",
       "          ...,\n",
       "          [ 3.9347e-04, -1.9993e-05,  3.0466e-04,  ...,  3.6397e-04,\n",
       "            4.2430e-04,  4.2684e-04],\n",
       "          [ 5.7322e-05,  4.0119e-04,  4.3544e-04,  ...,  6.8884e-04,\n",
       "            2.6947e-04,  4.4793e-04],\n",
       "          [ 1.1299e-04, -1.8670e-04,  4.0168e-05,  ..., -1.6453e-04,\n",
       "           -1.4170e-04,  6.5381e-05]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[-1.2610e-04, -2.7476e-04, -5.7521e-04,  ..., -4.0643e-04,\n",
       "           -6.7255e-04, -8.4744e-04],\n",
       "          [-1.5663e-04, -4.9121e-04, -8.6369e-04,  ..., -7.8966e-04,\n",
       "           -1.0046e-03, -1.1265e-03],\n",
       "          [-3.2462e-04, -7.8633e-04, -6.7670e-04,  ..., -9.3650e-04,\n",
       "           -1.3000e-03, -9.6620e-04],\n",
       "          ...,\n",
       "          [-3.7246e-04, -5.1757e-04, -2.6562e-04,  ..., -8.7568e-04,\n",
       "           -3.8606e-04, -4.3984e-04],\n",
       "          [-3.0639e-04, -4.0772e-04, -6.0149e-04,  ..., -8.7870e-04,\n",
       "           -5.0964e-04, -7.7972e-04],\n",
       "          [-4.6549e-04, -5.9113e-04, -6.6057e-04,  ..., -7.6135e-04,\n",
       "           -4.7240e-04, -5.7940e-04]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "         [[ 1.4762e-04,  4.2753e-04,  4.6551e-04,  ...,  1.5412e-04,\n",
       "            1.6736e-04,  9.5257e-05],\n",
       "          [ 4.9985e-04,  4.0049e-04, -1.8543e-04,  ...,  2.3024e-04,\n",
       "            5.8833e-04,  1.1290e-04],\n",
       "          [-1.2470e-04,  1.5269e-04,  2.5263e-04,  ...,  2.6848e-04,\n",
       "            3.7179e-04,  2.5622e-04],\n",
       "          ...,\n",
       "          [ 3.3017e-04,  6.7618e-05,  4.4862e-04,  ...,  6.7150e-04,\n",
       "            3.8152e-04,  5.8856e-04],\n",
       "          [ 1.2516e-04, -4.2692e-05,  5.7286e-05,  ...,  2.5144e-04,\n",
       "            4.8066e-04,  9.8094e-05],\n",
       "          [ 6.4038e-05, -2.2451e-04,  6.9508e-05,  ..., -1.7818e-04,\n",
       "           -1.4736e-04,  1.1823e-04]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[-3.8098e-04, -7.7442e-04, -7.0358e-04,  ..., -4.9068e-04,\n",
       "           -3.1678e-04, -3.2147e-04],\n",
       "          [-7.3379e-04, -7.3649e-04, -6.0044e-04,  ..., -5.1863e-04,\n",
       "           -4.2544e-04, -4.6435e-04],\n",
       "          [-5.2649e-04, -4.9703e-04, -3.0911e-04,  ..., -3.3009e-04,\n",
       "           -4.6401e-04, -6.5060e-04],\n",
       "          ...,\n",
       "          [-4.9814e-04, -6.8393e-04, -9.5799e-04,  ..., -5.0130e-04,\n",
       "           -3.4436e-04, -6.3754e-04],\n",
       "          [-3.9808e-04, -6.1303e-04, -6.4881e-04,  ..., -5.4848e-04,\n",
       "           -7.4914e-04, -6.5864e-04],\n",
       "          [-4.8016e-04, -4.2310e-04, -5.7796e-04,  ..., -6.5029e-04,\n",
       "           -8.9112e-04, -4.8671e-04]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "         [[ 4.6348e-04,  6.2649e-04,  6.7516e-04,  ...,  5.1748e-04,\n",
       "            6.0404e-04,  4.4095e-04],\n",
       "          [ 5.4320e-04,  5.0829e-04,  1.3795e-04,  ...,  3.6367e-04,\n",
       "            2.1559e-04,  1.3038e-04],\n",
       "          [-4.1125e-05, -1.0842e-05,  1.0637e-04,  ...,  9.4612e-06,\n",
       "            1.6724e-04,  5.7977e-04],\n",
       "          ...,\n",
       "          [ 3.6376e-04,  1.3800e-04,  1.2994e-04,  ...,  1.6672e-04,\n",
       "            1.2787e-04,  3.2914e-04],\n",
       "          [ 3.4030e-04, -4.9473e-05,  2.7942e-04,  ...,  1.6643e-04,\n",
       "            5.9819e-04,  3.1056e-04],\n",
       "          [-2.0017e-04,  7.5097e-05, -1.4376e-04,  ..., -3.3332e-06,\n",
       "           -1.5766e-04,  2.7513e-05]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[-4.1269e-04, -6.3050e-04, -2.4886e-04,  ..., -6.1545e-04,\n",
       "           -6.3309e-04, -6.7646e-04],\n",
       "          [-6.0272e-04, -7.5440e-04, -6.1658e-04,  ..., -7.6836e-04,\n",
       "           -1.0328e-03, -9.2740e-04],\n",
       "          [-5.6534e-04, -6.0348e-04, -7.0215e-04,  ..., -8.5441e-04,\n",
       "           -9.8553e-04, -9.5564e-04],\n",
       "          ...,\n",
       "          [-4.8706e-04, -1.0343e-03, -7.1847e-04,  ..., -7.1031e-04,\n",
       "           -9.7921e-04, -7.0233e-04],\n",
       "          [-6.3780e-04, -1.1564e-03, -8.0895e-04,  ..., -8.4002e-04,\n",
       "           -6.9410e-04, -8.1004e-04],\n",
       "          [-7.7955e-04, -7.4479e-04, -5.2242e-04,  ..., -8.4389e-04,\n",
       "           -8.1999e-04, -5.2989e-04]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "         [[ 1.8075e-04,  1.4446e-04,  1.1491e-04,  ...,  2.1716e-04,\n",
       "            3.8190e-04,  3.3215e-04],\n",
       "          [ 3.5567e-04,  1.6115e-04,  4.9358e-04,  ...,  1.3561e-04,\n",
       "            3.2764e-04,  1.3380e-04],\n",
       "          [ 1.4519e-04,  4.7040e-04,  3.8792e-05,  ...,  4.6104e-05,\n",
       "            1.7099e-04,  4.2157e-04],\n",
       "          ...,\n",
       "          [ 5.1349e-04,  3.2646e-04, -7.7062e-05,  ...,  3.6570e-04,\n",
       "            3.7695e-04,  2.4069e-04],\n",
       "          [ 6.9125e-05,  1.8508e-04,  4.0580e-04,  ...,  3.4047e-04,\n",
       "            1.5292e-04,  1.8096e-04],\n",
       "          [-2.0015e-04,  1.9221e-04, -2.2157e-04,  ..., -8.3119e-05,\n",
       "           -7.0372e-05,  5.4270e-05]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[-6.0059e-05, -6.0043e-04, -4.9981e-04,  ..., -7.6735e-04,\n",
       "           -5.2253e-04, -6.2787e-04],\n",
       "          [-6.1444e-04, -7.5060e-04, -5.8002e-04,  ..., -9.3242e-04,\n",
       "           -7.5742e-04, -6.9761e-04],\n",
       "          [-7.0114e-04, -7.8626e-04, -2.1579e-04,  ..., -5.5532e-04,\n",
       "           -4.6296e-04, -7.9343e-04],\n",
       "          ...,\n",
       "          [-6.3732e-04, -8.2468e-04, -7.1445e-04,  ..., -4.6007e-04,\n",
       "           -5.2698e-04, -8.3205e-04],\n",
       "          [-2.3322e-04, -5.0767e-04, -8.7495e-04,  ..., -4.7680e-04,\n",
       "           -1.0486e-03, -7.3951e-04],\n",
       "          [-2.1224e-04, -5.5344e-04, -6.7573e-04,  ..., -6.5280e-04,\n",
       "           -7.7229e-04, -5.3090e-04]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "         [[ 2.7204e-04,  5.0460e-04,  3.5481e-04,  ...,  1.8662e-04,\n",
       "            1.6069e-04,  3.2960e-04],\n",
       "          [ 6.7171e-04,  4.7770e-04,  6.0235e-04,  ...,  8.7129e-05,\n",
       "            3.7166e-04,  9.8297e-06],\n",
       "          [-1.4731e-04,  9.0351e-05,  1.1051e-04,  ...,  3.0645e-04,\n",
       "            3.8118e-04,  4.9487e-04],\n",
       "          ...,\n",
       "          [ 6.4045e-06,  6.5451e-04,  1.3994e-04,  ...,  3.4947e-04,\n",
       "            5.9642e-04,  1.6560e-04],\n",
       "          [ 7.6364e-04,  1.1609e-04,  4.0326e-04,  ...,  4.7842e-04,\n",
       "            1.3672e-04,  1.7508e-04],\n",
       "          [-2.8385e-04, -4.4104e-05,  5.9028e-06,  ..., -1.9151e-04,\n",
       "           -3.5939e-05,  5.7197e-05]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[-1.5710e-04, -6.3209e-04, -3.8237e-04,  ..., -8.1675e-04,\n",
       "           -6.0356e-04, -4.7490e-04],\n",
       "          [-3.9442e-04, -7.9313e-04, -7.4075e-04,  ..., -1.1399e-03,\n",
       "           -7.1988e-04, -5.9926e-04],\n",
       "          [-5.0771e-04, -7.4464e-04, -4.5737e-04,  ..., -8.9486e-04,\n",
       "           -4.6147e-04, -7.7489e-04],\n",
       "          ...,\n",
       "          [-4.6939e-04, -4.1143e-04, -3.5822e-04,  ..., -4.6138e-04,\n",
       "           -4.0807e-04, -5.8517e-04],\n",
       "          [-3.3182e-04, -4.6241e-04, -8.4473e-04,  ..., -8.7836e-04,\n",
       "           -6.0367e-04, -5.9059e-04],\n",
       "          [-4.3816e-04, -6.7109e-04, -7.8594e-04,  ..., -7.5900e-04,\n",
       "           -3.9825e-04, -5.0492e-04]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "         [[ 5.4577e-04,  3.6457e-04,  4.1440e-04,  ...,  4.9924e-04,\n",
       "            5.2652e-04,  3.3574e-04],\n",
       "          [ 2.8133e-04, -3.4830e-05,  3.3841e-04,  ...,  3.1073e-04,\n",
       "            1.1283e-04, -2.4032e-05],\n",
       "          [ 3.4420e-04, -6.0618e-05,  4.9048e-04,  ...,  3.2460e-05,\n",
       "            2.0957e-04,  6.3397e-04],\n",
       "          ...,\n",
       "          [ 3.3086e-04,  7.7614e-04,  1.6420e-04,  ...,  3.9960e-04,\n",
       "            2.4744e-04,  3.2705e-04],\n",
       "          [ 4.2778e-04,  2.8398e-05,  3.6697e-04,  ...,  1.7352e-04,\n",
       "            1.3439e-04,  4.3237e-04],\n",
       "          [-1.9055e-04, -5.6831e-05,  1.1224e-04,  ..., -4.2038e-06,\n",
       "           -7.1837e-05,  1.1700e-05]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]]]], grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# go layer by layer to identify mismatch\n",
    "split_model[0].conv1(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    Test I/O Logic\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    Test I/O Logic\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE\n",
    "\n",
    "# make dir name \n",
    "time_stamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "if len(configs['load_model']) == 0:\n",
    "    folder_name='{}-{}-{}-np{}-pr{}-lcm{}-{}'.format( \n",
    "                configs['data-code'], \n",
    "                configs['model'], \n",
    "                configs['sparsity-type'], \n",
    "                configs['num_partition'], \n",
    "                configs['prune-ratio'], \n",
    "                configs['lambda-comm'],\n",
    "                time_stamp)\n",
    "else:\n",
    "    folder_name = '{}-{}'.format(configs['load_model'][:-3],time_stamp)\n",
    "\n",
    "# make folder \n",
    "folder_path = os.path.join(os.getcwd(), 'assets', 'models',folder_name)\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "\n",
    "# save weights\n",
    "index = 0\n",
    "for l in split_model:\n",
    "    fpath = os.path.join(os.getcwd(), 'assets', 'models', folder_path, f'layer_model_{index}.pth')\n",
    "    torch.save(l.state_dict(), fpath)\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split models:\n",
      "['cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240615-135016', 'cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240615-141807', 'cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240622-092931', 'cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240622-125523', 'cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240623-182335', 'cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240623-182910', 'cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240624-204135', 'cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240625-073155', 'cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240626-090056', 'cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240626-202301', 'cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240626-213627', 'cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240626-213724', 'cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240626-214353', 'cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240627-105309', 'cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240629-090932', 'cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240629-095759', 'cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240701-181315', 'cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240701-220925', 'cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240702-082928', 'cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240702-083133', 'cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240708-092933', 'cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240708-100959', 'cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240708-101251', 'cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240708-104751', 'cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240708-110342', 'cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240708-111919', 'cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240708-112135', 'cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240713-112843', 'cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240713-122622', 'cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240713-140146', 'cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240713-140307', 'cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240713-150826', 'cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240713-160054', 'cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240713-160303', 'cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240713-161733', 'cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240713-190111', 'cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240714-122801', 'vsplit-cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240708-103841']\n",
      "\n",
      "loading split model cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240615-135016\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 32, 32]           1,728\n",
      "       BatchNorm2d-2           [-1, 64, 32, 32]             128\n",
      "            Conv2d-3           [-1, 64, 32, 32]          36,864\n",
      "       BatchNorm2d-4           [-1, 64, 32, 32]             128\n",
      "            Conv2d-5           [-1, 64, 32, 32]          36,864\n",
      "       BatchNorm2d-6           [-1, 64, 32, 32]             128\n",
      "        BasicBlock-7           [-1, 64, 32, 32]               0\n",
      "            Conv2d-8           [-1, 64, 32, 32]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 32, 32]             128\n",
      "           Conv2d-10           [-1, 64, 32, 32]          36,864\n",
      "      BatchNorm2d-11           [-1, 64, 32, 32]             128\n",
      "       BasicBlock-12           [-1, 64, 32, 32]               0\n",
      "================================================================\n",
      "Total params: 149,824\n",
      "Trainable params: 149,824\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 6.00\n",
      "Params size (MB): 0.57\n",
      "Estimated Total Size (MB): 6.58\n",
      "----------------------------------------------------------------\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 128, 16, 16]          73,728\n",
      "       BatchNorm2d-2          [-1, 128, 16, 16]             256\n",
      "            Conv2d-3          [-1, 128, 16, 16]         147,456\n",
      "       BatchNorm2d-4          [-1, 128, 16, 16]             256\n",
      "            Conv2d-5          [-1, 128, 16, 16]           8,192\n",
      "       BatchNorm2d-6          [-1, 128, 16, 16]             256\n",
      "        BasicBlock-7          [-1, 128, 16, 16]               0\n",
      "            Conv2d-8          [-1, 128, 16, 16]         147,456\n",
      "       BatchNorm2d-9          [-1, 128, 16, 16]             256\n",
      "           Conv2d-10          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-11          [-1, 128, 16, 16]             256\n",
      "       BasicBlock-12          [-1, 128, 16, 16]               0\n",
      "================================================================\n",
      "Total params: 525,568\n",
      "Trainable params: 525,568\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.25\n",
      "Forward/backward pass size (MB): 3.00\n",
      "Params size (MB): 2.00\n",
      "Estimated Total Size (MB): 5.25\n",
      "----------------------------------------------------------------\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 256, 8, 8]         294,912\n",
      "       BatchNorm2d-2            [-1, 256, 8, 8]             512\n",
      "            Conv2d-3            [-1, 256, 8, 8]         589,824\n",
      "       BatchNorm2d-4            [-1, 256, 8, 8]             512\n",
      "            Conv2d-5            [-1, 256, 8, 8]          32,768\n",
      "       BatchNorm2d-6            [-1, 256, 8, 8]             512\n",
      "        BasicBlock-7            [-1, 256, 8, 8]               0\n",
      "            Conv2d-8            [-1, 256, 8, 8]         589,824\n",
      "       BatchNorm2d-9            [-1, 256, 8, 8]             512\n",
      "           Conv2d-10            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-11            [-1, 256, 8, 8]             512\n",
      "       BasicBlock-12            [-1, 256, 8, 8]               0\n",
      "================================================================\n",
      "Total params: 2,099,712\n",
      "Trainable params: 2,099,712\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.12\n",
      "Forward/backward pass size (MB): 1.50\n",
      "Params size (MB): 8.01\n",
      "Estimated Total Size (MB): 9.63\n",
      "----------------------------------------------------------------\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 512, 4, 4]       1,179,648\n",
      "       BatchNorm2d-2            [-1, 512, 4, 4]           1,024\n",
      "            Conv2d-3            [-1, 512, 4, 4]       2,359,296\n",
      "       BatchNorm2d-4            [-1, 512, 4, 4]           1,024\n",
      "            Conv2d-5            [-1, 512, 4, 4]         131,072\n",
      "       BatchNorm2d-6            [-1, 512, 4, 4]           1,024\n",
      "        BasicBlock-7            [-1, 512, 4, 4]               0\n",
      "            Conv2d-8            [-1, 512, 4, 4]       2,359,296\n",
      "       BatchNorm2d-9            [-1, 512, 4, 4]           1,024\n",
      "           Conv2d-10            [-1, 512, 4, 4]       2,359,296\n",
      "      BatchNorm2d-11            [-1, 512, 4, 4]           1,024\n",
      "       BasicBlock-12            [-1, 512, 4, 4]               0\n",
      "           Linear-13                   [-1, 10]           5,130\n",
      "================================================================\n",
      "Total params: 8,398,858\n",
      "Trainable params: 8,398,858\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.06\n",
      "Forward/backward pass size (MB): 0.75\n",
      "Params size (MB): 32.04\n",
      "Estimated Total Size (MB): 32.85\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# LOAD\n",
    "\n",
    "select = 0\n",
    "\n",
    "layer_output_sizes = [(3,32,32), (64,32,32), (128,16,16), (256,8,8)]\n",
    "\n",
    "model_path = os.path.join(os.getcwd(), 'assets', 'models')\n",
    "filenames = os.listdir(model_path)\n",
    "\n",
    "# get dirs\n",
    "split_model_names = []\n",
    "for filename in filenames: # loop through all the files and folders\n",
    "    if os.path.isdir(os.path.join(model_path, filename)): # check whether the current object is a folder or not\n",
    "        split_model_names.append(filename)\n",
    "\n",
    "print('Split models:')\n",
    "print(split_model_names)\n",
    "print()\n",
    "\n",
    "model_name = split_model_names[select] \n",
    "print(f'loading split model {model_name}')\n",
    "\n",
    "index = 0\n",
    "for l in split_model:\n",
    "    layer_state_dict = torch.load(os.path.join(model_path, model_name, f'layer_model_{index}.pth'))\n",
    "    l = io.load_state_dict(l, layer_state_dict)\n",
    "\n",
    "\n",
    "    summary(l, layer_output_sizes[index],device=configs['device'])\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference time per data is 23.405790ms.\n",
      "conv1.weight 1024\n",
      "layer1.0.conv1.weight 1024\n",
      "layer1.0.conv2.weight 1024\n",
      "layer1.1.conv1.weight 1024\n",
      "layer1.1.conv2.weight 1024\n",
      "layer2.0.conv1.weight 256\n",
      "layer2.0.conv2.weight 256\n",
      "layer2.0.shortcut.0.weight 256\n",
      "layer2.1.conv1.weight 256\n",
      "layer2.1.conv2.weight 256\n",
      "layer3.0.conv1.weight 64\n",
      "layer3.0.conv2.weight 64\n",
      "layer3.0.shortcut.0.weight 64\n",
      "layer3.1.conv1.weight 64\n",
      "layer3.1.conv2.weight 64\n",
      "layer4.0.conv1.weight 16\n",
      "layer4.0.conv2.weight 16\n",
      "layer4.0.shortcut.0.weight 16\n",
      "layer4.1.conv1.weight 16\n",
      "layer4.1.conv2.weight 16\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    add partitions and communications to configs\n",
    "'''\n",
    "\n",
    "# gets random test input (with correct size)\n",
    "input_var = engine.get_input_from_code(configs)\n",
    "#print(input_var)\n",
    "\n",
    "# Config partitions and prune_ratio\n",
    "configs['num_partition'] = '4'#'./config/resnet18-v2.yaml'\n",
    "configs = engine.partition_generator(configs, model)\n",
    "            \n",
    "# Compute output size of each layer\n",
    "configs['partition'] = engine.featuremap_summary(model, configs['partition'], input_var)\n",
    "        \n",
    "# Setup communication costs\n",
    "configs['comm_costs'] = engine.set_communication_cost(model, configs['partition'],)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\natet\\Desktop\\graduate school\\thesis\\CaP\\assets\\figs\\cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001\n",
      "name: conv1.weight, img size: (3, 64) weight size: (64, 3)\n",
      "name: layer1.0.conv1.weight, img size: (64, 64) weight size: (64, 64)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    inspect IO per layer \n",
    "'''\n",
    "\n",
    "savepath=io.get_fig_path(\"{}\".format('.'.join(configs[\"load_model\"].split('.')[:-1])))\n",
    "print(savepath)\n",
    "\n",
    "# this function looks for model layers named in \"confgis['partition']\" (other layers are ignored)\n",
    "# -> \"conv\" and shortcut layers are the only ones \"split\"\n",
    "# -> total 20/49 (20/62?) layers are split\n",
    "counter = 0\n",
    "for name, W in model.named_parameters():\n",
    "        if name in configs['partition']:\n",
    "            #print(f'{counter} | {name}')\n",
    "            counter +=1\n",
    "\n",
    "# Plot model\n",
    "layer_id = (0,1,2) # inspect these layers\n",
    "testers.plot_layer(model, configs['partition'], layer_id=layer_id, savepath=savepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split model \n",
    "\n",
    "# make copies of model per machine\n",
    "num_machines = max(configs['partition']['bn_partition']) # TODO: double check this makes sense\n",
    "model_machines = [model]*num_machines\n",
    "\n",
    "module_names =  [module[0] for i, module in enumerate(model.named_modules())]\n",
    "num_total_modules = len(module_names)\n",
    "\n",
    "split_module_names = list(configs['partition'].keys())\n",
    "\n",
    "for aname in module_names:\n",
    "    print(f'{aname}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 5, 6, 11, 12, 18, 19, 26, 27, 33, 34, 41, 42, 48, 49, 56, 57]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Setup datastrcuts to ID layers executed with 'extra' functionality\n",
    "'''\n",
    "\n",
    "# module numbering is based on module.named_parameters()\n",
    "relu_modules = [2, 7,8,13,14,20,2428,29,35,3943,44,50,54,58,59] # execute relu on this  layer \n",
    "split_module_indexes =  [i for i in range(len(module_names)) if 'conv' in module_names[i] ]\n",
    "\n",
    "print(split_module_indexes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nonzero_channels(atensor, dim=1):\n",
    "    return torch.unique(torch.nonzero(atensor, as_tuple=True)[dim]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing module 1: conv1\n",
      "\tExecuting on machine 0\n",
      "\t\t received input channels tensor([0, 1, 2])\n",
      "\t\t-No input assigned to this machine. Skipping...\n",
      "\tExecuting on machine 1\n",
      "\t\t received input channels tensor([0, 1, 2])\n",
      "\t\t sending C_out tensor([16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 31]) to machine 1\n",
      "\t\t sending C_out tensor([56, 62]) to machine 3\n",
      "\tExecuting on machine 2\n",
      "\t\t received input channels tensor([0, 1, 2])\n",
      "\t\t sending C_out tensor([ 0, 12]) to machine 0\n",
      "\t\t sending C_out tensor([38, 42]) to machine 2\n",
      "\t\t sending C_out tensor([54, 57]) to machine 3\n",
      "\tExecuting on machine 3\n",
      "\t\t received input channels tensor([0, 1, 2])\n",
      "\t\t sending C_out tensor([ 4,  8, 14]) to machine 0\n",
      "\t\t sending C_out tensor([16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 31]) to machine 1\n",
      "\t\t sending C_out tensor([36, 42, 45, 46, 47]) to machine 2\n",
      "\t\t sending C_out tensor([54, 57]) to machine 3\n",
      "Finished execution of layer 1\n",
      "\n",
      "Executing module 2: bn1\n",
      "\tExecuting on machine 0\n",
      "\t\t received input channels tensor([ 0,  4,  8, 12, 14])\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 1\n",
      "\t\t received input channels tensor([16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 31])\n",
      "\t\t-Applying ReLU\n",
      "\t\t sending C_out tensor([16, 18, 19, 20, 21, 22, 23, 24, 25, 26, 29, 31]) to machine 1\n",
      "\tExecuting on machine 2\n",
      "\t\t received input channels tensor([36, 38, 42, 45, 46, 47])\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 3\n",
      "\t\t received input channels tensor([54, 56, 57, 62])\n",
      "\t\t-Applying ReLU\n",
      "\t\t sending C_out tensor([54, 57]) to machine 3\n",
      "Finished execution of layer 2\n",
      "\n",
      "Executing module 3: layer1\n",
      "\tExecuting on machine 0\n",
      "\t\t-No input sent to this machine. Skipping module\n",
      "\tExecuting on machine 1\n",
      "\t\t received input channels tensor([16, 18, 19, 20, 21, 22, 23, 24, 25, 26, 29, 31])\n",
      "\t\t-Skipping module Sequential\n",
      "\tExecuting on machine 2\n",
      "\t\t-No input sent to this machine. Skipping module\n",
      "\tExecuting on machine 3\n",
      "\t\t received input channels tensor([54, 57])\n",
      "\t\t-Skipping module Sequential\n",
      "Finished execution of layer 3\n",
      "\n",
      "Executing module 4: layer1.0\n",
      "\tExecuting on machine 0\n",
      "\t\t-No input sent to this machine. Skipping module\n",
      "\tExecuting on machine 1\n",
      "\t\t received input channels tensor([16, 18, 19, 20, 21, 22, 23, 24, 25, 26, 29, 31])\n",
      "\t\t-Saving input for later...\n",
      "\tExecuting on machine 2\n",
      "\t\t-No input sent to this machine. Skipping module\n",
      "\tExecuting on machine 3\n",
      "\t\t received input channels tensor([54, 57])\n",
      "\t\t-Saving input for later...\n",
      "Finished execution of layer 4\n",
      "\n",
      "Executing module 5: layer1.0.conv1\n",
      "\tExecuting on machine 0\n",
      "\t\t-No input sent to this machine. Skipping module\n",
      "\tExecuting on machine 1\n",
      "\t\t received input channels tensor([16, 18, 19, 20, 21, 22, 23, 24, 25, 26, 29, 31])\n",
      "\t\t sending C_out tensor([ 0,  2,  3,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15]) to machine 0\n",
      "\t\t sending C_out tensor([16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]) to machine 1\n",
      "\t\t sending C_out tensor([32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]) to machine 2\n",
      "\t\t sending C_out tensor([48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]) to machine 3\n",
      "\tExecuting on machine 2\n",
      "\t\t-No input sent to this machine. Skipping module\n",
      "\tExecuting on machine 3\n",
      "\t\t received input channels tensor([54, 57])\n",
      "\t\t sending C_out tensor([ 0,  1,  4,  5, 10, 11, 13, 14]) to machine 0\n",
      "\t\t sending C_out tensor([16, 19, 21, 25, 30]) to machine 1\n",
      "\t\t sending C_out tensor([32, 33, 36, 38, 39, 41, 42, 44, 45, 46, 47]) to machine 2\n",
      "\t\t sending C_out tensor([51]) to machine 3\n",
      "Finished execution of layer 5\n",
      "\n",
      "Executing module 6: layer1.0.conv2\n",
      "\tExecuting on machine 0\n",
      "\t\t received input channels tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15])\n",
      "\t\t sending C_out tensor([ 4,  9, 13, 14, 15]) to machine 0\n",
      "\t\t sending C_out tensor([16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]) to machine 1\n",
      "\t\t sending C_out tensor([32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]) to machine 2\n",
      "\t\t sending C_out tensor([48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]) to machine 3\n",
      "\tExecuting on machine 1\n",
      "\t\t received input channels tensor([16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31])\n",
      "\t\t sending C_out tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15]) to machine 0\n",
      "\t\t sending C_out tensor([16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 31]) to machine 1\n",
      "\t\t sending C_out tensor([32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]) to machine 2\n",
      "\t\t sending C_out tensor([48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]) to machine 3\n",
      "\tExecuting on machine 2\n",
      "\t\t received input channels tensor([32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47])\n",
      "\t\t sending C_out tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15]) to machine 0\n",
      "\t\t sending C_out tensor([16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]) to machine 1\n",
      "\t\t sending C_out tensor([34, 36, 40, 42, 43, 45, 46, 47]) to machine 2\n",
      "\t\t sending C_out tensor([48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]) to machine 3\n",
      "\tExecuting on machine 3\n",
      "\t\t received input channels tensor([48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63])\n",
      "\t\t sending C_out tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15]) to machine 0\n",
      "\t\t sending C_out tensor([16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]) to machine 1\n",
      "\t\t sending C_out tensor([32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 45, 46, 47]) to machine 2\n",
      "\t\t sending C_out tensor([48, 49, 54, 57, 59, 61]) to machine 3\n",
      "Finished execution of layer 6\n",
      "\n",
      "Executing module 7: layer1.0.bn1\n",
      "\tExecuting on machine 0\n",
      "\t\t received input channels tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15])\n",
      "\t\t-Applying ReLU\n",
      "\t\t sending C_out tensor([10]) to machine 0\n",
      "\tExecuting on machine 1\n",
      "\t\t received input channels tensor([16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31])\n",
      "\t\t-Applying ReLU\n",
      "\t\t sending C_out tensor([16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 31]) to machine 1\n",
      "\tExecuting on machine 2\n",
      "\t\t received input channels tensor([32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47])\n",
      "\t\t-Applying ReLU\n",
      "\t\t sending C_out tensor([36]) to machine 2\n",
      "\tExecuting on machine 3\n",
      "\t\t received input channels tensor([48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63])\n",
      "\t\t-Applying ReLU\n",
      "\t\t sending C_out tensor([54, 57]) to machine 3\n",
      "Finished execution of layer 7\n",
      "\n",
      "Executing module 8: layer1.0.bn2\n",
      "\tExecuting on machine 0\n",
      "\t\t received input channels tensor([10])\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 1\n",
      "\t\t received input channels tensor([16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 31])\n",
      "\t\t-Applying ReLU\n",
      "\t\t sending C_out tensor([16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 31]) to machine 1\n",
      "\tExecuting on machine 2\n",
      "\t\t received input channels tensor([36])\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 3\n",
      "\t\t received input channels tensor([54, 57])\n",
      "\t\t-Applying ReLU\n",
      "\t\t sending C_out tensor([54]) to machine 3\n",
      "Finished execution of layer 8\n",
      "\n",
      "Executing module 9: layer1.0.shortcut\n",
      "\tExecuting on machine 0\n",
      "\t\t-No input sent to this machine. Skipping module\n",
      "\tExecuting on machine 1\n",
      "\t\t received input channels tensor([16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 31])\n",
      "\t\t-Skipping module Sequential\n",
      "\tExecuting on machine 2\n",
      "\t\t-No input sent to this machine. Skipping module\n",
      "\tExecuting on machine 3\n",
      "\t\t received input channels tensor([54])\n",
      "\t\t-Skipping module Sequential\n",
      "Finished execution of layer 9\n",
      "\n",
      "Executing module 10: layer1.1\n",
      "\tExecuting on machine 0\n",
      "\t\t-No input sent to this machine. Skipping module\n",
      "\tExecuting on machine 1\n",
      "\t\t received input channels tensor([16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 31])\n",
      "\t\t-Saving input for later...\n",
      "\tExecuting on machine 2\n",
      "\t\t-No input sent to this machine. Skipping module\n",
      "\tExecuting on machine 3\n",
      "\t\t received input channels tensor([54])\n",
      "\t\t-Saving input for later...\n",
      "Finished execution of layer 10\n",
      "\n",
      "Executing module 11: layer1.1.conv1\n",
      "\tExecuting on machine 0\n",
      "\t\t-No input sent to this machine. Skipping module\n",
      "\tExecuting on machine 1\n",
      "\t\t received input channels tensor([16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 31])\n",
      "\t\t sending C_out tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15]) to machine 0\n",
      "\t\t sending C_out tensor([16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]) to machine 1\n",
      "\t\t sending C_out tensor([32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]) to machine 2\n",
      "\t\t sending C_out tensor([48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]) to machine 3\n",
      "\tExecuting on machine 2\n",
      "\t\t-No input sent to this machine. Skipping module\n",
      "\tExecuting on machine 3\n",
      "\t\t received input channels tensor([54])\n",
      "\t\t sending C_out tensor([ 1,  2,  3,  6, 10, 11]) to machine 0\n",
      "\t\t sending C_out tensor([23, 30]) to machine 1\n",
      "\t\t sending C_out tensor([41]) to machine 2\n",
      "\t\t sending C_out tensor([61]) to machine 3\n",
      "Finished execution of layer 11\n",
      "\n",
      "Executing module 12: layer1.1.conv2\n",
      "\tExecuting on machine 0\n",
      "\t\t received input channels tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15])\n",
      "\t\t sending C_out tensor([ 0,  5,  9, 11, 13]) to machine 0\n",
      "\t\t sending C_out tensor([16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]) to machine 1\n",
      "\t\t sending C_out tensor([32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]) to machine 2\n",
      "\t\t sending C_out tensor([48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]) to machine 3\n",
      "\tExecuting on machine 1\n",
      "\t\t received input channels tensor([16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31])\n",
      "\t\t sending C_out tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15]) to machine 0\n",
      "\t\t sending C_out tensor([16, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 31]) to machine 1\n",
      "\t\t sending C_out tensor([32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]) to machine 2\n",
      "\t\t sending C_out tensor([48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]) to machine 3\n",
      "\tExecuting on machine 2\n",
      "\t\t received input channels tensor([32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47])\n",
      "\t\t sending C_out tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15]) to machine 0\n",
      "\t\t sending C_out tensor([16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]) to machine 1\n",
      "\t\t sending C_out tensor([34, 35, 36, 46]) to machine 2\n",
      "\t\t sending C_out tensor([48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]) to machine 3\n",
      "\tExecuting on machine 3\n",
      "\t\t received input channels tensor([48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63])\n",
      "\t\t sending C_out tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15]) to machine 0\n",
      "\t\t sending C_out tensor([16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]) to machine 1\n",
      "\t\t sending C_out tensor([32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]) to machine 2\n",
      "\t\t sending C_out tensor([48, 52, 56, 57, 62]) to machine 3\n",
      "Finished execution of layer 12\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    mock run through inference using split models \n",
    "'''\n",
    "\n",
    "# TODO: reduce size of communicated tensors to only what is necessary \n",
    "# TODO: also check bias for nonzero\n",
    "# TODO: come up with more general scheme to handle residual layers\n",
    "\n",
    "# channel_id == INPUTS\n",
    "# filter_id  == OUTPUTS\n",
    "\n",
    "# setup input \n",
    "N_batch = 1\n",
    "input_tensor = torch.rand(N_batch, 3, 3, 3, device=torch.device(configs['device'])) # 1k images, 3 channels, 32x32 image (cifar100) \n",
    "\n",
    "# broadcast input_tensor to different machines\n",
    "# TODO: find a better datastructure for this\n",
    "#input = np.empty((num_machines, num_machines), dtype=torch.Tensor)\n",
    "input = [None]*num_machines\n",
    "input = [input[:] for i in range(num_machines)]\n",
    "for imach in range(num_machines):\n",
    "    input[imach][0] = input_tensor\n",
    "\n",
    "# put models into eval mode and on device\n",
    "model.eval()\n",
    "model.to(configs['device'])\n",
    "\n",
    "residual_input = {} # use this to keep track of inputs stored in machine memory for residule layers\n",
    "add_bias = False # add bias for previous conv layer \n",
    "\n",
    "# make inference \n",
    "with torch.no_grad():\n",
    "        # iterate through layers 1 module at a time \n",
    "        for imodule in range(13):#range(num_total_modules): # 16 <=> layer_1 block \n",
    "\n",
    "                if imodule in [0]:\n",
    "                        continue\n",
    "\n",
    "                # initialize output for ilayer\n",
    "                #output = np.empty((num_machines, num_machines), dtype=torch.Tensor) # square list indexed as: output[destination/RX machine][origin/TX machine]\n",
    "                # TODO: find a better datastructure for this \n",
    "                output = [None]*num_machines\n",
    "                output = [output[:] for i in range(num_machines)]\n",
    "                \n",
    "                send_module_outputs = True\n",
    "\n",
    "                print(f'Executing module {imodule}: {module_names[imodule]}')\n",
    "\n",
    "                # iterate through each machine (done in parallel later)\n",
    "                for imach in range(num_machines):\n",
    "                        print(f'\\tExecuting on machine {imach}')\n",
    "                        \n",
    "                        add_residual = False\n",
    "\n",
    "                        # get the current module\n",
    "                        # TODO: this is very bad for latency. Only load module if you have to \n",
    "                        curr_name, curr_module = next((x for i,x in enumerate(model.named_modules()) if i==imodule)) \n",
    "\n",
    "                        # combine inputs from machines\n",
    "                        curr_input = False \n",
    "                        rx_count = 0\n",
    "                        for i in range(num_machines):\n",
    "                                if not input[imach][i] == None:\n",
    "                                        if not torch.is_tensor(curr_input):\n",
    "                                                curr_input = input[imach][i] # initialize curr_input with first input tensor \n",
    "                                        else:\n",
    "                                                curr_input += input[imach][i]\n",
    "                                        rx_count += 1\n",
    "                        if add_bias:\n",
    "                                # TODO: check if this works (this is not required for resnet18 because no bias on conv layers)\n",
    "                                dummy, prev_module = next((x for i,x in enumerate(model.named_modules()) if i==imodule-1))\n",
    "                                bias = prev_module.bias \n",
    "                                curr_input += bias/rx_count\n",
    "\n",
    "                        # skip this machine+module if there is no input to compute \n",
    "                        if not torch.is_tensor(curr_input):\n",
    "                                print('\\t\\t-No input sent to this machine. Skipping module')\n",
    "                                continue\n",
    "                        \n",
    "                        # debug\n",
    "                        print(f'\\t\\t received input channels {get_nonzero_channels(curr_input)}')\n",
    "\n",
    "                        # update communication I/O for this layer  \n",
    "                        # TODO: revist this implementation\n",
    "                        split_param_name = curr_name + '.weight'\n",
    "                        if split_param_name in split_module_names:\n",
    "\n",
    "                                # skip if machine doesnt expect input\n",
    "                                if len(configs['partition'][split_param_name]['channel_id'][imach]) == 0:\n",
    "                                        print(f'\\t\\t-No input assigned to this machine. Skipping...')\n",
    "                                        continue\n",
    "                                \n",
    "                                # TODO: reconsider implementation \n",
    "                                # What input channels does this machine compute?\n",
    "                                input_channels = torch.tensor(configs['partition'][split_param_name]['channel_id'][imach],\n",
    "                                        device=torch.device(configs['device']))\n",
    "                                N_in = len(input_channels) # TODO: is this used?\n",
    "\n",
    "                                # Where to send output (map of output channels to different machines)\n",
    "                                output_channel_map = configs['partition'][split_param_name]['filter_id']\n",
    "                        else:\n",
    "                                # assume no communnication needs to happen \n",
    "                                input_channels = get_nonzero_channels(curr_input)\n",
    "                                output_channel_map = [None]*num_machines\n",
    "                                for i in range(num_machines):\n",
    "                                        if i == imach:\n",
    "                                                output_channel_map[i] =  input_channels.numpy()\n",
    "                                        else:\n",
    "                                                output_channel_map[i] = np.array([])\n",
    "\n",
    "                        # reduce computation-- make vertically split layer \n",
    "                        # TODO: generalize this to more than conv layers \n",
    "                        if type(curr_module) == nn.Conv2d:\n",
    "                                split_layer = nn.Conv2d(N_in,\n",
    "                                                curr_module.weight.shape[0], # TODO does this need to be an int? (currently tensor)\n",
    "                                                kernel_size= curr_module.kernel_size,\n",
    "                                                stride=curr_module.stride,\n",
    "                                                padding=curr_module.padding, \n",
    "                                                bias=False) # TODO: add bias during input collecting step on next layer \n",
    "\n",
    "                                # write parameters to split layer \n",
    "                                split_layer.weight = torch.nn.Parameter(curr_module.weight.index_select(1, input_channels))\n",
    "\n",
    "                                # TODO: add support for splitting bias\n",
    "\n",
    "                                # handle if this is the start of the residual layer\n",
    "                                if 'shortcut' in curr_name:\n",
    "                                        residual_input[str(imach)]['block_out'] = curr_input\n",
    "                                        curr_input = residual_input[str(imach)]['block_in']\n",
    "\n",
    "                        elif type(curr_module) == nn.BatchNorm2d:\n",
    "                                split_layer = nn.BatchNorm2d(N_in, \n",
    "                                                curr_module.eps,\n",
    "                                                momentum=curr_module.momentum, \n",
    "                                                affine=curr_module.affine, \n",
    "                                                track_running_stats=curr_module.track_running_stats)\n",
    "\n",
    "                                # write parameters to split layer \n",
    "                                split_layer.weight = torch.nn.Parameter(curr_module.weight.index_select(0, input_channels))\n",
    "                                split_layer.running_mean = torch.nn.Parameter(curr_module.running_mean.index_select(0, input_channels))\n",
    "                                split_layer.running_var = torch.nn.Parameter(curr_module.running_var.index_select(0, input_channels))\n",
    "\n",
    "                                if not curr_module.bias == None:\n",
    "                                        split_layer.bias = torch.nn.Parameter(curr_module.bias.index_select(0, input_channels))\n",
    "                                \n",
    "                                \n",
    "                                if 'shortcut' in curr_name:\n",
    "                                        add_residual = True\n",
    "\n",
    "                                # TODO: revise implementation to only compute necessary C_in to C_out \n",
    "                                # assume mach-Cout map from previous conv layer can be used as inputs for this bn layer\n",
    "                                #input_channels = output_channel_map[imach]\n",
    "\n",
    "                        elif type(curr_module) == nn.Linear:\n",
    "                                # TODO: assumes there is a bias \n",
    "                                split_layer = nn.Linear(N_in, \n",
    "                                                curr_module.weight.shape[0])\n",
    "\n",
    "                                # write parameters to split layer \n",
    "                                split_layer.weight = torch.nn.Parameter(curr_module.weight.index_select(1, input_channels))\n",
    "\n",
    "                                # TODO: double check bias is applied correctly\n",
    "                                if not curr_module.bias == None:\n",
    "                                        split_layer.bias = curr_module.bias\n",
    "\n",
    "                                # prep for linear layer\n",
    "                                # TODO: assumes this always happens before linear layer \n",
    "                                # bn takes one in channel C_in_i and produces one out channel C_out_j. No communication is needed. \n",
    "                                curr_input = F.avg_pool2d(curr_input, 4)\n",
    "                                curr_input = curr_input.view(curr_input.size(0), -1)\n",
    "\n",
    "                        elif type(curr_module) == resnet.BasicBlock:\n",
    "                                # save input for later \n",
    "                                residual_input[str(imach)] = {}\n",
    "                                residual_input[str(imach)]['block_in'] = curr_input\n",
    "                                print('\\t\\t-Saving input for later...')\n",
    "                                send_module_outputs = False\n",
    "                                continue\n",
    "                        else:\n",
    "                                print(f'\\t\\t-Skipping module {type(curr_module).__name__}')\n",
    "                                send_module_outputs = False\n",
    "                                continue\n",
    "                        \n",
    "                        # make sure layer is in eval mode\n",
    "                        # TODO: if you set model.eval() can we skip this, also only required for bn layers? Maybe \n",
    "                        split_layer.eval()\n",
    "\n",
    "                        # eval split\n",
    "                        out_tensor = split_layer(curr_input.index_select(1, input_channels))\n",
    "                        if type(curr_module) == nn.BatchNorm2d:\n",
    "                                tmp_out_tensor = torch.zeros(curr_input.shape)\n",
    "                                tmp_out_tensor[:,input_channels,:,:] = out_tensor\n",
    "                                out_tensor = tmp_out_tensor\n",
    "\n",
    "                        # debug\n",
    "                        nonzero_out_tensor = torch.unique(torch.nonzero(out_tensor, as_tuple=True)[1])\n",
    "\n",
    "\n",
    "                        # check if this is residual layer\n",
    "                        if add_residual:\n",
    "                                print('\\t\\t-adding residual')\n",
    "                                out_tensor += residual_input[str(imach)]['block_out']\n",
    "\n",
    "                                # erase stored \n",
    "                                residual_input[str(imach)] = {}\n",
    "\n",
    "                        # apply ReLU after batch layers\n",
    "                        if imodule in relu_modules:\n",
    "                                print('\\t\\t-Applying ReLU')\n",
    "                                out_tensor = F.relu(out_tensor)\n",
    "\n",
    "                        # look at which C_out need to be computed and sent\n",
    "                        #nonzero_Cout = torch.unique(torch.nonzero(split_layer.weight, as_tuple=True)[0]) # find nonzero dimensions in output channels\n",
    "                        nonzero_Cout = get_nonzero_channels(out_tensor)\n",
    "\n",
    "                        # communicate\n",
    "                        out_channel_array = torch.arange(out_tensor.shape[1])\n",
    "                        for rx_mach in range(num_machines):\n",
    "                                # only add to output if communication is necessary \n",
    "\n",
    "                                # Get output channels for current rx machine? TODO: consider removing, this just maps C_out's to machine\n",
    "                                output_channels = torch.tensor(output_channel_map[rx_mach],\n",
    "                                        device=torch.device(configs['device']))\n",
    "\n",
    "                                # TODO: is there a faster way to do this? Consider putting larger array 1st... just not sure which one that'd be\n",
    "                                nonzero_out_channels = nonzero_Cout[torch.isin(nonzero_Cout, output_channels)]\n",
    "                                if nonzero_out_channels.nelement() > 0:\n",
    "                                        communication_mask = torch.isin(out_channel_array, nonzero_out_channels)\n",
    "\n",
    "                                        # TODO: this is inefficient, redo. Probbably need to send a tensor and some info what output channels are being sent\n",
    "                                        tmp_out = torch.zeros(out_tensor.shape) \n",
    "                                        tmp_out[:,communication_mask,:,:] = out_tensor[:,communication_mask,:,:]\n",
    "                                        output[rx_mach][imach] = tmp_out\n",
    "\n",
    "                                        # debug\n",
    "                                        print(f'\\t\\t sending C_out {nonzero_out_channels} to machine {rx_mach}')\n",
    "\n",
    "                # send to next layer  \n",
    "                if send_module_outputs:      \n",
    "                        input = output\n",
    "                print(f'Finished execution of layer {imodule}')\n",
    "                print()\n",
    "\n",
    "# collect outputs -- assumes ends with Linear layer. Not sure how generalizable this is\n",
    "# if loop stops on module that doesnt calculate anything use input struct \n",
    "if send_module_outputs:\n",
    "        tmp_output = output\n",
    "else:\n",
    "        tmp_output = input \n",
    "need_to_init  = True\n",
    "for rx_mach in range(num_machines):\n",
    "        for tx_mach in range(num_machines):\n",
    "                if not tmp_output[rx_mach][tx_mach] == None:\n",
    "                        if need_to_init:\n",
    "                                final_output = tmp_output[rx_mach][tx_mach]\n",
    "                                need_to_init = False\n",
    "                        else:\n",
    "                                # TODO: += causes assignment issues, switched to x = x+y which might be more more inefficent memory wise ... \n",
    "                                final_output = final_output + tmp_output[rx_mach][tx_mach] \n",
    "                                nz_channels = get_nonzero_channels(final_output)\n",
    "                                #print(f'({rx_mach},{tx_mach}) {nz_channels}')\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine 0\n",
      "[]\n",
      "0\n",
      "Machine 1\n",
      "[]\n",
      "0\n",
      "Machine 2\n",
      "[]\n",
      "0\n",
      "Machine 3\n",
      "[]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "'''Sanity Check C_out'''\n",
    "\n",
    "# emulate the rx machine receiving its inputs and summing them \n",
    "# does the nonzero C_in channels make sense?\n",
    "\n",
    "for rx_mach in range(num_machines):\n",
    "    \n",
    "    need_to_init = True\n",
    "    for tx_mach in range(num_machines):\n",
    "\n",
    "        if not output[rx_mach][tx_mach] == None:\n",
    "            if need_to_init:\n",
    "                    tmp_output = output[rx_mach][tx_mach]\n",
    "                    need_to_init = False\n",
    "            else:\n",
    "                    tmp_output = tmp_output+ output[rx_mach][tx_mach]\n",
    "    \n",
    "    print(f'Machine {rx_mach}')\n",
    "    if need_to_init:\n",
    "        nz_channels = []      \n",
    "    else:\n",
    "        nz_channels = get_nonzero_channels(tmp_output)\n",
    "    print(nz_channels)     \n",
    "    print(len(nz_channels))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Compare I/O for one layer \n",
    "'''\n",
    "\n",
    "def compare_IO(model, module_list, input, split_output):\n",
    "    ''' This function is used for debugging. It computes \n",
    "    the output for a particular layer in the model using the input argument \n",
    "    and compares it to the split output'''\n",
    "\n",
    "\n",
    "    for imodule in module_list:\n",
    "        curr_name, curr_module = next((x for i,x in enumerate(model.named_modules()) if i==imodule)) \n",
    "        print(curr_name)\n",
    "\n",
    "        curr_module.eval()\n",
    "\n",
    "        input = curr_module(input)\n",
    "        if imodule in relu_modules:\n",
    "            print('Apllying ReLU')\n",
    "            input = F.relu(input)\n",
    "    full_output = input \n",
    "\n",
    "    io_match = torch.all(torch.eq(split_output, full_output))\n",
    "\n",
    "    return (io_match, full_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1\n",
      "bn1\n",
      "Apllying ReLU\n",
      "layer1.0.conv1\n",
      "layer1.0.conv2\n",
      "layer1.0.bn1\n",
      "Apllying ReLU\n",
      "layer1.0.bn2\n",
      "Apllying ReLU\n",
      "layer1.0.shortcut\n",
      "layer1.1.conv1\n",
      "layer1.1.conv2\n",
      "torch.return_types.max(\n",
      "values=tensor([4.7684e-06], grad_fn=<MaxBackward0>),\n",
      "indices=tensor([166]))\n"
     ]
    }
   ],
   "source": [
    "''' Single Layer Test'''\n",
    "io_match, full_output= compare_IO(model, [1,2,5,6,7,8, 9,11,12,13], input_tensor, final_output)\n",
    "\n",
    "# module 9 sequential seems to be messing stuff up \n",
    "\n",
    "\n",
    "#print(io_match)\n",
    "diff_output = torch.abs(full_output - final_output)\n",
    "\n",
    "print(torch.max(torch.reshape(diff_output, (N_batch, -1)), dim=1))\n",
    "#plt.hist(diff_output.reshape((-1,)))\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResnetBlockOne(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "torch.return_types.max(\n",
      "values=tensor([3.7672], grad_fn=<MaxBackward0>),\n",
      "indices=tensor([241]))\n"
     ]
    }
   ],
   "source": [
    "''' \n",
    "    Check Model output 1 horizontal block at a time \n",
    "'''\n",
    "\n",
    "# imodule = 16\n",
    "\n",
    "\n",
    "print(layer_1)\n",
    "layer_1_output = layer_1(input_tensor)\n",
    "\n",
    "diff_output = torch.abs(layer_1_output - final_output)\n",
    "\n",
    "print(torch.max(torch.reshape(diff_output, (N_batch, -1)), dim=1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'layer1.1.shortcut.weight'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m split_output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(full_output\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m imachine \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_machines):\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# get input channels\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m     input_channels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[43mconfigs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpartition\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43msplit_param_name\u001b[49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchannel_id\u001b[39m\u001b[38;5;124m'\u001b[39m][imach],\n\u001b[0;32m     12\u001b[0m                                         device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(configs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[0;32m     13\u001b[0m     N_in \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(input_channels) \u001b[38;5;66;03m# TODO: is this used?\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;66;03m# make split layer \u001b[39;00m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'layer1.1.shortcut.weight'"
     ]
    }
   ],
   "source": [
    "''' Outside framework test'''\n",
    "\n",
    "imodule = 1\n",
    "curr_name, curr_module = next((x for i,x in enumerate(model.named_modules()) if i==imodule)) \n",
    "\n",
    "full_output = curr_module(input_tensor)\n",
    "\n",
    "split_output = torch.zeros(full_output.shape)\n",
    "for imachine in range(num_machines):\n",
    "    # get input channels\n",
    "    input_channels = torch.tensor(configs['partition'][split_param_name]['channel_id'][imach],\n",
    "                                        device=torch.device(configs['device']))\n",
    "    N_in = len(input_channels) # TODO: is this used?\n",
    "\n",
    "    # make split layer \n",
    "    split_layer = nn.Conv2d(N_in,\n",
    "                    curr_module.weight.shape[0], # TODO does this need to be an int? (currently tensor)\n",
    "                    kernel_size= curr_module.kernel_size,\n",
    "                    stride=curr_module.stride,\n",
    "                    padding=curr_module.padding, \n",
    "                    bias=False) # TODO: add bias during input collecting step on next layer \n",
    "    split_layer.weight = torch.nn.Parameter(curr_module.weight.index_select(1, input_channels))\n",
    "\n",
    "    # eval and add\n",
    "    split_output += split_layer(input_tensor.index_select(1, input_channels))\n",
    "\n",
    "# compare \n",
    "io_match = torch.all(torch.eq(split_output, full_output))\n",
    "diff_output = torch.abs(full_output - final_output)\n",
    "print(io_match)\n",
    "print(torch.max(diff_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in_tensor\n",
      "tensor([[[[ 0.,  1.,  2.],\n",
      "          [ 3.,  4.,  5.],\n",
      "          [ 6.,  7.,  8.]],\n",
      "\n",
      "         [[ 9., 10., 11.],\n",
      "          [12., 13., 14.],\n",
      "          [15., 16., 17.]]]], dtype=torch.float64)\n",
      "\n",
      "kernals\n",
      "Parameter containing:\n",
      "tensor([[[[2., 1.],\n",
      "          [1., 0.]],\n",
      "\n",
      "         [[2., 1.],\n",
      "          [1., 2.]]]], dtype=torch.float64, requires_grad=True)\n",
      "\n",
      "out tensor\n",
      "tensor([[[[ 70.,  80.],\n",
      "          [100., 110.]]]], dtype=torch.float64, grad_fn=<ConvolutionBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''2D conv test'''\n",
    "\n",
    "in_tensor = torch.arange(18, dtype=float).reshape([1,2,3,3])\n",
    "print('in_tensor')\n",
    "print(in_tensor)\n",
    "print()\n",
    "\n",
    "kernals = torch.randint(0,3,(1,2,2,2),  dtype=float)\n",
    "kernals = torch.nn.Parameter(kernals)\n",
    "print('kernals')\n",
    "print(kernals)\n",
    "print()\n",
    "\n",
    "conv1 = nn.Conv2d(2,1, (2,2), padding=0, bias=False)\n",
    "conv1.weight = kernals\n",
    "\n",
    "out_tensor = conv1(in_tensor)\n",
    "print('out tensor')\n",
    "print(out_tensor)\n",
    "print()\n",
    "\n",
    "\n",
    "# verified this is the same as documentation \n",
    "# error computing 1st conv layer is probably from missing a computation?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (32) must match the size of tensor b (1000) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[208], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03m    Compare vertical+horizontal split model with full model\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# TODO: finish\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m match_count \u001b[38;5;241m=\u001b[39m (\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfinal_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_full\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m      7\u001b[0m label_hist \u001b[38;5;241m=\u001b[39m output_full\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMatches: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmatch_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_full\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (32) must match the size of tensor b (1000) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Compare vertical+horizontal split model with full model\n",
    "'''\n",
    "\n",
    "# TODO: finish\n",
    "match_count = (torch.argmax(final_output, axis=1) == torch.argmax(output_full, axis=1)).sum().item()\n",
    "label_hist = output_full.sum(0)\n",
    "print(f'Matches: {match_count}/{output_full.size(0)}')\n",
    "print(f'histogram {label_hist}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'layer_names' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m imach \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m      6\u001b[0m ilayer\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m----> 7\u001b[0m input_channels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(configs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpartition\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[43mlayer_names\u001b[49m[ilayer]][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchannel_id\u001b[39m\u001b[38;5;124m'\u001b[39m][imach],\n\u001b[0;32m      8\u001b[0m                               device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(configs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[0;32m      9\u001b[0m output_channels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(configs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpartition\u001b[39m\u001b[38;5;124m'\u001b[39m][layer_names[ilayer]][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilter_id\u001b[39m\u001b[38;5;124m'\u001b[39m][imach],\n\u001b[0;32m     10\u001b[0m                                device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(configs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput channels \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_channels\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'layer_names' is not defined"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Test partial execution of a single layer\n",
    "'''\n",
    "\n",
    "imach = 1\n",
    "ilayer= 0\n",
    "input_channels = torch.tensor(configs['partition'][layer_names[ilayer]]['channel_id'][imach],\n",
    "                              device=torch.device(configs['device']))\n",
    "output_channels = torch.tensor(configs['partition'][layer_names[ilayer]]['filter_id'][imach],\n",
    "                               device=torch.device(configs['device']))\n",
    "print(f'Input channels {input_channels}')\n",
    "print(f'Output channels {output_channels}')\n",
    "\n",
    "# TODO: generalize this to more than conv layers \n",
    "a_layer = model.conv1\n",
    "split_layer = nn.Conv2d(len(input_channels),\n",
    "                    len(output_channels),\n",
    "                    kernel_size= a_layer.kernel_size,\n",
    "                    stride=a_layer.stride,\n",
    "                    padding=a_layer.padding, \n",
    "                    bias=a_layer.bias)\n",
    "\n",
    "split_layer.parameters = a_layer.weight.index_select(0, output_channels).index_select(1, input_channels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - 0\n",
      "1 - 1\n",
      "2 - 2\n",
      "3 - 3\n",
      "4 - 4\n",
      "5 - 5\n",
      "6 - 6\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    iterate using children method?\n",
    "'''\n",
    "\n",
    "# creates nested structure for each layer/block \n",
    "i = 0\n",
    "for name,module in enumerate( model_machines[imach].children()):\n",
    "    print(f'{i} - {name}')\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - name: ; type: ResNet\n",
      "1 - name: conv1; type: Conv2d\n",
      "2 - name: bn1; type: BatchNorm1d\n",
      "3 - name: layer1; type: Sequential\n",
      "4 - name: layer1.0; type: BasicBlock\n",
      "5 - name: layer1.0.conv1; type: Conv2d\n",
      "6 - name: layer1.0.conv2; type: Conv2d\n",
      "7 - name: layer1.0.bn1; type: BatchNorm1d\n",
      "8 - name: layer1.0.bn2; type: BatchNorm1d\n",
      "9 - name: layer1.0.shortcut; type: Sequential\n",
      "10 - name: layer1.1; type: BasicBlock\n",
      "11 - name: layer1.1.conv1; type: Conv2d\n",
      "12 - name: layer1.1.conv2; type: Conv2d\n",
      "13 - name: layer1.1.bn1; type: BatchNorm1d\n",
      "14 - name: layer1.1.bn2; type: BatchNorm1d\n",
      "15 - name: layer1.1.shortcut; type: Sequential\n",
      "16 - name: layer2; type: Sequential\n",
      "17 - name: layer2.0; type: BasicBlock\n",
      "18 - name: layer2.0.conv1; type: Conv2d\n",
      "19 - name: layer2.0.conv2; type: Conv2d\n",
      "20 - name: layer2.0.bn1; type: BatchNorm1d\n",
      "21 - name: layer2.0.bn2; type: BatchNorm1d\n",
      "22 - name: layer2.0.shortcut; type: Sequential\n",
      "23 - name: layer2.0.shortcut.0; type: Conv2d\n",
      "24 - name: layer2.0.shortcut.1; type: BatchNorm1d\n",
      "25 - name: layer2.1; type: BasicBlock\n",
      "26 - name: layer2.1.conv1; type: Conv2d\n",
      "27 - name: layer2.1.conv2; type: Conv2d\n",
      "28 - name: layer2.1.bn1; type: BatchNorm1d\n",
      "29 - name: layer2.1.bn2; type: BatchNorm1d\n",
      "30 - name: layer2.1.shortcut; type: Sequential\n",
      "31 - name: layer3; type: Sequential\n",
      "32 - name: layer3.0; type: BasicBlock\n",
      "33 - name: layer3.0.conv1; type: Conv2d\n",
      "34 - name: layer3.0.conv2; type: Conv2d\n",
      "35 - name: layer3.0.bn1; type: BatchNorm1d\n",
      "36 - name: layer3.0.bn2; type: BatchNorm1d\n",
      "37 - name: layer3.0.shortcut; type: Sequential\n",
      "38 - name: layer3.0.shortcut.0; type: Conv2d\n",
      "39 - name: layer3.0.shortcut.1; type: BatchNorm1d\n",
      "40 - name: layer3.1; type: BasicBlock\n",
      "41 - name: layer3.1.conv1; type: Conv2d\n",
      "42 - name: layer3.1.conv2; type: Conv2d\n",
      "43 - name: layer3.1.bn1; type: BatchNorm1d\n",
      "44 - name: layer3.1.bn2; type: BatchNorm1d\n",
      "45 - name: layer3.1.shortcut; type: Sequential\n",
      "46 - name: layer4; type: Sequential\n",
      "47 - name: layer4.0; type: BasicBlock\n",
      "48 - name: layer4.0.conv1; type: Conv2d\n",
      "49 - name: layer4.0.conv2; type: Conv2d\n",
      "50 - name: layer4.0.bn1; type: BatchNorm1d\n",
      "51 - name: layer4.0.bn2; type: BatchNorm1d\n",
      "52 - name: layer4.0.shortcut; type: Sequential\n",
      "53 - name: layer4.0.shortcut.0; type: Conv2d\n",
      "54 - name: layer4.0.shortcut.1; type: BatchNorm1d\n",
      "55 - name: layer4.1; type: BasicBlock\n",
      "56 - name: layer4.1.conv1; type: Conv2d\n",
      "57 - name: layer4.1.conv2; type: Conv2d\n",
      "58 - name: layer4.1.bn1; type: BatchNorm1d\n",
      "59 - name: layer4.1.bn2; type: BatchNorm1d\n",
      "60 - name: layer4.1.shortcut; type: Sequential\n",
      "61 - name: linear; type: Linear\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "    iterate using named_modules?\n",
    "'''\n",
    "\n",
    "# - gets redundant structure\n",
    "# - I think the easiest way to iterate through the whole model is using this list and skipping large layers/blocks of the model and only executing conv/bn/linear etc. \"layers\"\n",
    "# - provides good indicator of when to save input for skipped layer \n",
    "\n",
    "# not sure how to handle sequential layers... going to poke at this here \n",
    "i = 0\n",
    "for name, module in model.named_modules():\n",
    "    print(f'{i} - name: {name}; type: {type(module).__name__}')\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_inputs(num_machines, input):\n",
    "    '''\n",
    "        combines input tensors \n",
    "\n",
    "        Input:\n",
    "            num_machines - number of total machines\n",
    "            input - num_machines x num_machines list with inputs from previous layers collected from each machine. Indexed [destination][origin]\n",
    "\n",
    "        Output:\n",
    "            curr_input - a single tensor for this layer from the combined inputs\n",
    "    '''\n",
    "    curr_input = False \n",
    "    for i in range(num_machines):\n",
    "            if not input[imach][i] == []:\n",
    "                    if not torch.is_tensor(curr_input):\n",
    "                            curr_input = input[imach][i] # initialize curr_input with first input tensor \n",
    "                    else:\n",
    "                            curr_input += input[imach][i]\n",
    "    \n",
    "    return curr_input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.0000e+00, 1.1921e-07, 2.3842e-07],\n",
       "          [1.1921e-07, 1.1921e-07, 2.3842e-07],\n",
       "          [2.3842e-07, 2.3842e-07, 0.0000e+00]]]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    can you split certain operations across machines?\n",
    "    - I think the math says yes but there is some error values a on the order of 1e-7 \n",
    "    - could this be due to non-deterministic tensor operations? or is my math wrong?\n",
    "'''\n",
    "\n",
    "t = torch.rand(1, 3, 10,10)\n",
    "all = torch.sum(t, 1, True)\n",
    "\n",
    "\n",
    "kernel_size = 3\n",
    "\n",
    "full_avg = F.avg_pool2d(all, kernel_size)\n",
    "par_avg = torch.zeros(full_avg.shape)\n",
    "\n",
    "for i in range(t.shape[1]):\n",
    "    par_avg += F.avg_pool2d(t.index_select(1, torch.tensor(i)), kernel_size)\n",
    "\n",
    "torch.abs(full_avg - par_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Conv1 layer test\n",
    "'''\n",
    "\n",
    "# DIFFERENCE SHOULD BE 0 NOT 1E-7\n",
    "\n",
    "N_in = 1\n",
    "\n",
    "split_1 = nn.Conv2d(N_in,\n",
    "            model.conv1.weight.shape[0], # TODO does this need to be an int? (currently tensor)\n",
    "            kernel_size= model.conv1.kernel_size,\n",
    "            stride=model.conv1.stride,\n",
    "            padding=model.conv1.padding, \n",
    "            bias=False) # TODO: add bias during input collecting step on next layer \n",
    "split_1.weight = torch.nn.Parameter(model.conv1.weight.index_select(1, torch.tensor([0])))  \n",
    "\n",
    "split_2 = split_1\n",
    "split_2.weight = torch.nn.Parameter(model.conv1.weight.index_select(1, torch.tensor([1])))  \n",
    "\n",
    "split_3 = split_1\n",
    "split_3.weight = torch.nn.Parameter(model.conv1.weight.index_select(1, torch.tensor([2])))  \n",
    "\n",
    "split_out = split_1(input_tensor.index_select(1, torch.tensor([0]))) + split_2(input_tensor.index_select(1, torch.tensor([1]))) + split_3(input_tensor.index_select(1, torch.tensor([2])))\n",
    "full_out = model.conv1(input_tensor)\n",
    "\n",
    "diff_output = torch.abs(full_out - full_out)\n",
    "print(torch.max(diff_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cap_nb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
