{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    Load RESNET model and split it\\n        1. layer by layer\\n        2. [TODO] vertically \\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    Load RESNET model and split it\n",
    "        1. layer by layer\n",
    "        2. [TODO] vertically \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\natet\\anaconda3\\envs\\cap_nb\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from source.core.engine import MoP\n",
    "import source.core.run_partition as run_p\n",
    "from os import environ\n",
    "from source.utils.dataset import *\n",
    "from source.utils.misc import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "from source.models import resnet\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from source.utils import io\n",
    "from source.utils import testers\n",
    "from source.core import engine\n",
    "import json\n",
    "import itertools\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device :  \n",
      "model :  resnet18\n",
      "data_code :  cifar10\n",
      "num_classes :  10\n",
      "model_file :  test.pt\n",
      "epochs :  0\n",
      "batch_size :  128\n",
      "optimizer :  sgd\n",
      "lr_scheduler :  default\n",
      "learning_rate :  0.01\n",
      "seed :  1234\n",
      "sparsity_type :  kernel\n",
      "prune_ratio :  1\n",
      "admm :  True\n",
      "admm_epochs :  3\n",
      "rho :  0.0001\n",
      "multi_rho :  True\n",
      "retrain_bs :  128\n",
      "retrain_lr :  0.005\n",
      "retrain_ep :  50\n",
      "retrain_opt :  default\n",
      "xentropy_weight :  1.0\n",
      "warmup :  False\n",
      "warmup_lr :  0.001\n",
      "warmup_epochs :  10\n",
      "mix_up :  True\n",
      "alpha :  0.3\n",
      "smooth :  False\n",
      "smooth_eps :  0\n",
      "save_last_model_only :  False\n",
      "num_partition :  1\n",
      "layer_type :  regular\n",
      "bn_type :  masked\n",
      "par_first_layer :  False\n",
      "comm_outsize :  False\n",
      "lambda_comm :  0\n",
      "lambda_comp :  0\n",
      "distill_model :  \n",
      "distill_loss :  kl\n",
      "distill_temp :  30\n",
      "distill_alpha :  1\n"
     ]
    }
   ],
   "source": [
    "# setup config\n",
    "dataset='cifar10'\n",
    "environ[\"config\"] = f\"config/{dataset}.yaml\"\n",
    "\n",
    "configs = run_p.main()\n",
    "\n",
    "configs[\"device\"] = \"cpu\"\n",
    "configs['load_model'] = \"cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001.pt\"\n",
    "configs[\"num_partition\"] = '4' #'resnet18-v2.yaml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (linear): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# load data and load or train model\n",
    "model = get_model_from_code(configs).to(configs['device']) # grabs model architecture from ./source/models/escnet.py\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "   1st section of resnet model \n",
    "'''\n",
    "class ResnetBlockOne(nn.Module):\n",
    "    def __init__(self, block, num_blocks, conv_layer, bn_layer, num_classes=10, num_filters=512, bn_partition=[1]*9):\n",
    "        super(ResnetBlockOne, self).__init__()\n",
    "\n",
    "        self.in_planes = 64\n",
    "        self.conv_layer = conv_layer\n",
    "        self.bn_layer = bn_layer\n",
    "        self.shrink = num_filters/512\n",
    "        self.bn_partition = bn_partition\n",
    "        \n",
    "        self.conv1 = conv_layer(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        num_bn = self.bn_partition.pop(0)\n",
    "        self.bn1 = bn_layer(64) if num_bn==1 else bn_layer(64, num_bn)\n",
    "\n",
    "        self.layer1 = self._make_layer(block, int(64*self.shrink),  num_blocks[0], stride=1)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        # TODO: find better way to implement this method using inheretence and getting from ResNet class\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, self.conv_layer, self.bn_layer, stride, self.bn_partition.pop(0)))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # override the the foward pass to only include the first modules \n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "'''\n",
    "   2nd section of resnet model \n",
    "'''\n",
    "class ResnetBlockTwo(nn.Module):\n",
    "    def __init__(self, block, num_blocks, conv_layer, bn_layer, num_classes=10, num_filters=512, bn_partition=[1]*9):\n",
    "        super(ResnetBlockTwo, self).__init__()\n",
    "\n",
    "        self.in_planes = 64\n",
    "        self.conv_layer = conv_layer\n",
    "        self.bn_layer = bn_layer\n",
    "        self.shrink = num_filters/512\n",
    "        self.bn_partition = bn_partition\n",
    "\n",
    "        self.layer2 = self._make_layer(block, int(128*self.shrink), num_blocks[1], stride=2)\n",
    "    \n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        # TODO: find better way to implement this method using inheretence and getting from ResNet class\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, self.conv_layer, self.bn_layer, stride, self.bn_partition.pop(0)))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer2(x)\n",
    "        return out\n",
    "\n",
    "'''\n",
    "    3rd section of resenet model \n",
    "'''\n",
    "class ResnetBlockThree(nn.Module):\n",
    "    def __init__(self, block, num_blocks, conv_layer, bn_layer, num_classes=10, num_filters=512, bn_partition=[1]*9):\n",
    "        super(ResnetBlockThree, self).__init__()\n",
    "\n",
    "        self.in_planes = 128\n",
    "        self.conv_layer = conv_layer\n",
    "        self.bn_layer = bn_layer\n",
    "        self.shrink = num_filters/512\n",
    "        self.bn_partition = bn_partition\n",
    "\n",
    "        self.layer3 = self._make_layer(block, int(256*self.shrink), num_blocks[2], stride=2)\n",
    "    \n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        # TODO: find better way to implement this method using inheretence and getting from ResNet class\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, self.conv_layer, self.bn_layer, stride, self.bn_partition.pop(0)))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer3(x)\n",
    "        return out\n",
    "\n",
    "'''\n",
    "    4-th section of resenet model \n",
    "'''\n",
    "class ResnetBlockFour(nn.Module):\n",
    "    def __init__(self, block, num_blocks, conv_layer, bn_layer, num_classes=10, num_filters=512, bn_partition=[1]*9):\n",
    "        super(ResnetBlockFour, self).__init__()\n",
    "\n",
    "        self.in_planes = 256\n",
    "        self.conv_layer = conv_layer\n",
    "        self.bn_layer = bn_layer\n",
    "        self.shrink = num_filters/512\n",
    "        self.bn_partition = bn_partition\n",
    "\n",
    "        self.layer4 = self._make_layer(block, num_filters, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(num_filters*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        # TODO: find better way to implement this method using inheretence and getting from ResNet class\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, self.conv_layer, self.bn_layer, stride, self.bn_partition.pop(0)))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer4(x)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResnetBlockFour(\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (linear): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# get framework for each layer of resnet\n",
    "\n",
    "# inputs can be found from looking at ./source/util/misc/get_model_from_code\n",
    "num_classes=configs['num_classes']\n",
    "bn_layers = get_bn_layers('regular') # basic block layer\n",
    "conv_layers = get_layers(configs['layer_type'])\n",
    "\n",
    "# resnet18 inputs from resnet.py\n",
    "layer_1 =  ResnetBlockOne(resnet.BasicBlock, [2,2,2,2],conv_layers, bn_layers, num_classes=num_classes) # also includes bn1 and conv1 \n",
    "layer_2 =  ResnetBlockTwo(resnet.BasicBlock, [2,2,2,2],conv_layers, bn_layers, num_classes=num_classes) # also includes bn1 and conv1 \n",
    "layer_3 =  ResnetBlockThree(resnet.BasicBlock, [2,2,2,2],conv_layers, bn_layers, num_classes=num_classes) # also includes bn1 and conv1 \n",
    "layer_4 =  ResnetBlockFour(resnet.BasicBlock, [2,2,2,2],conv_layers, bn_layers, num_classes=num_classes) # also includes bn1 and conv1 \n",
    "\n",
    "print(layer_4)\n",
    "#block2 = |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not found:  layer2.0.conv1.weight\n",
      "not found:  layer2.0.conv2.weight\n",
      "not found:  layer2.0.bn1.weight\n",
      "not found:  layer2.0.bn1.bias\n",
      "not found:  layer2.0.bn1.running_mean\n",
      "not found:  layer2.0.bn1.running_var\n",
      "not found:  layer2.0.bn1.num_batches_tracked\n",
      "not found:  layer2.0.bn2.weight\n",
      "not found:  layer2.0.bn2.bias\n",
      "not found:  layer2.0.bn2.running_mean\n",
      "not found:  layer2.0.bn2.running_var\n",
      "not found:  layer2.0.bn2.num_batches_tracked\n",
      "not found:  layer2.0.shortcut.0.weight\n",
      "not found:  layer2.0.shortcut.1.weight\n",
      "not found:  layer2.0.shortcut.1.bias\n",
      "not found:  layer2.0.shortcut.1.running_mean\n",
      "not found:  layer2.0.shortcut.1.running_var\n",
      "not found:  layer2.0.shortcut.1.num_batches_tracked\n",
      "not found:  layer2.1.conv1.weight\n",
      "not found:  layer2.1.conv2.weight\n",
      "not found:  layer2.1.bn1.weight\n",
      "not found:  layer2.1.bn1.bias\n",
      "not found:  layer2.1.bn1.running_mean\n",
      "not found:  layer2.1.bn1.running_var\n",
      "not found:  layer2.1.bn1.num_batches_tracked\n",
      "not found:  layer2.1.bn2.weight\n",
      "not found:  layer2.1.bn2.bias\n",
      "not found:  layer2.1.bn2.running_mean\n",
      "not found:  layer2.1.bn2.running_var\n",
      "not found:  layer2.1.bn2.num_batches_tracked\n",
      "not found:  layer3.0.conv1.weight\n",
      "not found:  layer3.0.conv2.weight\n",
      "not found:  layer3.0.bn1.weight\n",
      "not found:  layer3.0.bn1.bias\n",
      "not found:  layer3.0.bn1.running_mean\n",
      "not found:  layer3.0.bn1.running_var\n",
      "not found:  layer3.0.bn1.num_batches_tracked\n",
      "not found:  layer3.0.bn2.weight\n",
      "not found:  layer3.0.bn2.bias\n",
      "not found:  layer3.0.bn2.running_mean\n",
      "not found:  layer3.0.bn2.running_var\n",
      "not found:  layer3.0.bn2.num_batches_tracked\n",
      "not found:  layer3.0.shortcut.0.weight\n",
      "not found:  layer3.0.shortcut.1.weight\n",
      "not found:  layer3.0.shortcut.1.bias\n",
      "not found:  layer3.0.shortcut.1.running_mean\n",
      "not found:  layer3.0.shortcut.1.running_var\n",
      "not found:  layer3.0.shortcut.1.num_batches_tracked\n",
      "not found:  layer3.1.conv1.weight\n",
      "not found:  layer3.1.conv2.weight\n",
      "not found:  layer3.1.bn1.weight\n",
      "not found:  layer3.1.bn1.bias\n",
      "not found:  layer3.1.bn1.running_mean\n",
      "not found:  layer3.1.bn1.running_var\n",
      "not found:  layer3.1.bn1.num_batches_tracked\n",
      "not found:  layer3.1.bn2.weight\n",
      "not found:  layer3.1.bn2.bias\n",
      "not found:  layer3.1.bn2.running_mean\n",
      "not found:  layer3.1.bn2.running_var\n",
      "not found:  layer3.1.bn2.num_batches_tracked\n",
      "not found:  layer4.0.conv1.weight\n",
      "not found:  layer4.0.conv2.weight\n",
      "not found:  layer4.0.bn1.weight\n",
      "not found:  layer4.0.bn1.bias\n",
      "not found:  layer4.0.bn1.running_mean\n",
      "not found:  layer4.0.bn1.running_var\n",
      "not found:  layer4.0.bn1.num_batches_tracked\n",
      "not found:  layer4.0.bn2.weight\n",
      "not found:  layer4.0.bn2.bias\n",
      "not found:  layer4.0.bn2.running_mean\n",
      "not found:  layer4.0.bn2.running_var\n",
      "not found:  layer4.0.bn2.num_batches_tracked\n",
      "not found:  layer4.0.shortcut.0.weight\n",
      "not found:  layer4.0.shortcut.1.weight\n",
      "not found:  layer4.0.shortcut.1.bias\n",
      "not found:  layer4.0.shortcut.1.running_mean\n",
      "not found:  layer4.0.shortcut.1.running_var\n",
      "not found:  layer4.0.shortcut.1.num_batches_tracked\n",
      "not found:  layer4.1.conv1.weight\n",
      "not found:  layer4.1.conv2.weight\n",
      "not found:  layer4.1.bn1.weight\n",
      "not found:  layer4.1.bn1.bias\n",
      "not found:  layer4.1.bn1.running_mean\n",
      "not found:  layer4.1.bn1.running_var\n",
      "not found:  layer4.1.bn1.num_batches_tracked\n",
      "not found:  layer4.1.bn2.weight\n",
      "not found:  layer4.1.bn2.bias\n",
      "not found:  layer4.1.bn2.running_mean\n",
      "not found:  layer4.1.bn2.running_var\n",
      "not found:  layer4.1.bn2.num_batches_tracked\n",
      "not found:  linear.weight\n",
      "not found:  linear.bias\n",
      "not found:  conv1.weight\n",
      "not found:  bn1.weight\n",
      "not found:  bn1.bias\n",
      "not found:  bn1.running_mean\n",
      "not found:  bn1.running_var\n",
      "not found:  bn1.num_batches_tracked\n",
      "not found:  layer1.0.conv1.weight\n",
      "not found:  layer1.0.conv2.weight\n",
      "not found:  layer1.0.bn1.weight\n",
      "not found:  layer1.0.bn1.bias\n",
      "not found:  layer1.0.bn1.running_mean\n",
      "not found:  layer1.0.bn1.running_var\n",
      "not found:  layer1.0.bn1.num_batches_tracked\n",
      "not found:  layer1.0.bn2.weight\n",
      "not found:  layer1.0.bn2.bias\n",
      "not found:  layer1.0.bn2.running_mean\n",
      "not found:  layer1.0.bn2.running_var\n",
      "not found:  layer1.0.bn2.num_batches_tracked\n",
      "not found:  layer1.1.conv1.weight\n",
      "not found:  layer1.1.conv2.weight\n",
      "not found:  layer1.1.bn1.weight\n",
      "not found:  layer1.1.bn1.bias\n",
      "not found:  layer1.1.bn1.running_mean\n",
      "not found:  layer1.1.bn1.running_var\n",
      "not found:  layer1.1.bn1.num_batches_tracked\n",
      "not found:  layer1.1.bn2.weight\n",
      "not found:  layer1.1.bn2.bias\n",
      "not found:  layer1.1.bn2.running_mean\n",
      "not found:  layer1.1.bn2.running_var\n",
      "not found:  layer1.1.bn2.num_batches_tracked\n",
      "not found:  layer3.0.conv1.weight\n",
      "not found:  layer3.0.conv2.weight\n",
      "not found:  layer3.0.bn1.weight\n",
      "not found:  layer3.0.bn1.bias\n",
      "not found:  layer3.0.bn1.running_mean\n",
      "not found:  layer3.0.bn1.running_var\n",
      "not found:  layer3.0.bn1.num_batches_tracked\n",
      "not found:  layer3.0.bn2.weight\n",
      "not found:  layer3.0.bn2.bias\n",
      "not found:  layer3.0.bn2.running_mean\n",
      "not found:  layer3.0.bn2.running_var\n",
      "not found:  layer3.0.bn2.num_batches_tracked\n",
      "not found:  layer3.0.shortcut.0.weight\n",
      "not found:  layer3.0.shortcut.1.weight\n",
      "not found:  layer3.0.shortcut.1.bias\n",
      "not found:  layer3.0.shortcut.1.running_mean\n",
      "not found:  layer3.0.shortcut.1.running_var\n",
      "not found:  layer3.0.shortcut.1.num_batches_tracked\n",
      "not found:  layer3.1.conv1.weight\n",
      "not found:  layer3.1.conv2.weight\n",
      "not found:  layer3.1.bn1.weight\n",
      "not found:  layer3.1.bn1.bias\n",
      "not found:  layer3.1.bn1.running_mean\n",
      "not found:  layer3.1.bn1.running_var\n",
      "not found:  layer3.1.bn1.num_batches_tracked\n",
      "not found:  layer3.1.bn2.weight\n",
      "not found:  layer3.1.bn2.bias\n",
      "not found:  layer3.1.bn2.running_mean\n",
      "not found:  layer3.1.bn2.running_var\n",
      "not found:  layer3.1.bn2.num_batches_tracked\n",
      "not found:  layer4.0.conv1.weight\n",
      "not found:  layer4.0.conv2.weight\n",
      "not found:  layer4.0.bn1.weight\n",
      "not found:  layer4.0.bn1.bias\n",
      "not found:  layer4.0.bn1.running_mean\n",
      "not found:  layer4.0.bn1.running_var\n",
      "not found:  layer4.0.bn1.num_batches_tracked\n",
      "not found:  layer4.0.bn2.weight\n",
      "not found:  layer4.0.bn2.bias\n",
      "not found:  layer4.0.bn2.running_mean\n",
      "not found:  layer4.0.bn2.running_var\n",
      "not found:  layer4.0.bn2.num_batches_tracked\n",
      "not found:  layer4.0.shortcut.0.weight\n",
      "not found:  layer4.0.shortcut.1.weight\n",
      "not found:  layer4.0.shortcut.1.bias\n",
      "not found:  layer4.0.shortcut.1.running_mean\n",
      "not found:  layer4.0.shortcut.1.running_var\n",
      "not found:  layer4.0.shortcut.1.num_batches_tracked\n",
      "not found:  layer4.1.conv1.weight\n",
      "not found:  layer4.1.conv2.weight\n",
      "not found:  layer4.1.bn1.weight\n",
      "not found:  layer4.1.bn1.bias\n",
      "not found:  layer4.1.bn1.running_mean\n",
      "not found:  layer4.1.bn1.running_var\n",
      "not found:  layer4.1.bn1.num_batches_tracked\n",
      "not found:  layer4.1.bn2.weight\n",
      "not found:  layer4.1.bn2.bias\n",
      "not found:  layer4.1.bn2.running_mean\n",
      "not found:  layer4.1.bn2.running_var\n",
      "not found:  layer4.1.bn2.num_batches_tracked\n",
      "not found:  linear.weight\n",
      "not found:  linear.bias\n",
      "not found:  conv1.weight\n",
      "not found:  bn1.weight\n",
      "not found:  bn1.bias\n",
      "not found:  bn1.running_mean\n",
      "not found:  bn1.running_var\n",
      "not found:  bn1.num_batches_tracked\n",
      "not found:  layer1.0.conv1.weight\n",
      "not found:  layer1.0.conv2.weight\n",
      "not found:  layer1.0.bn1.weight\n",
      "not found:  layer1.0.bn1.bias\n",
      "not found:  layer1.0.bn1.running_mean\n",
      "not found:  layer1.0.bn1.running_var\n",
      "not found:  layer1.0.bn1.num_batches_tracked\n",
      "not found:  layer1.0.bn2.weight\n",
      "not found:  layer1.0.bn2.bias\n",
      "not found:  layer1.0.bn2.running_mean\n",
      "not found:  layer1.0.bn2.running_var\n",
      "not found:  layer1.0.bn2.num_batches_tracked\n",
      "not found:  layer1.1.conv1.weight\n",
      "not found:  layer1.1.conv2.weight\n",
      "not found:  layer1.1.bn1.weight\n",
      "not found:  layer1.1.bn1.bias\n",
      "not found:  layer1.1.bn1.running_mean\n",
      "not found:  layer1.1.bn1.running_var\n",
      "not found:  layer1.1.bn1.num_batches_tracked\n",
      "not found:  layer1.1.bn2.weight\n",
      "not found:  layer1.1.bn2.bias\n",
      "not found:  layer1.1.bn2.running_mean\n",
      "not found:  layer1.1.bn2.running_var\n",
      "not found:  layer1.1.bn2.num_batches_tracked\n",
      "not found:  layer2.0.conv1.weight\n",
      "not found:  layer2.0.conv2.weight\n",
      "not found:  layer2.0.bn1.weight\n",
      "not found:  layer2.0.bn1.bias\n",
      "not found:  layer2.0.bn1.running_mean\n",
      "not found:  layer2.0.bn1.running_var\n",
      "not found:  layer2.0.bn1.num_batches_tracked\n",
      "not found:  layer2.0.bn2.weight\n",
      "not found:  layer2.0.bn2.bias\n",
      "not found:  layer2.0.bn2.running_mean\n",
      "not found:  layer2.0.bn2.running_var\n",
      "not found:  layer2.0.bn2.num_batches_tracked\n",
      "not found:  layer2.0.shortcut.0.weight\n",
      "not found:  layer2.0.shortcut.1.weight\n",
      "not found:  layer2.0.shortcut.1.bias\n",
      "not found:  layer2.0.shortcut.1.running_mean\n",
      "not found:  layer2.0.shortcut.1.running_var\n",
      "not found:  layer2.0.shortcut.1.num_batches_tracked\n",
      "not found:  layer2.1.conv1.weight\n",
      "not found:  layer2.1.conv2.weight\n",
      "not found:  layer2.1.bn1.weight\n",
      "not found:  layer2.1.bn1.bias\n",
      "not found:  layer2.1.bn1.running_mean\n",
      "not found:  layer2.1.bn1.running_var\n",
      "not found:  layer2.1.bn1.num_batches_tracked\n",
      "not found:  layer2.1.bn2.weight\n",
      "not found:  layer2.1.bn2.bias\n",
      "not found:  layer2.1.bn2.running_mean\n",
      "not found:  layer2.1.bn2.running_var\n",
      "not found:  layer2.1.bn2.num_batches_tracked\n",
      "not found:  layer4.0.conv1.weight\n",
      "not found:  layer4.0.conv2.weight\n",
      "not found:  layer4.0.bn1.weight\n",
      "not found:  layer4.0.bn1.bias\n",
      "not found:  layer4.0.bn1.running_mean\n",
      "not found:  layer4.0.bn1.running_var\n",
      "not found:  layer4.0.bn1.num_batches_tracked\n",
      "not found:  layer4.0.bn2.weight\n",
      "not found:  layer4.0.bn2.bias\n",
      "not found:  layer4.0.bn2.running_mean\n",
      "not found:  layer4.0.bn2.running_var\n",
      "not found:  layer4.0.bn2.num_batches_tracked\n",
      "not found:  layer4.0.shortcut.0.weight\n",
      "not found:  layer4.0.shortcut.1.weight\n",
      "not found:  layer4.0.shortcut.1.bias\n",
      "not found:  layer4.0.shortcut.1.running_mean\n",
      "not found:  layer4.0.shortcut.1.running_var\n",
      "not found:  layer4.0.shortcut.1.num_batches_tracked\n",
      "not found:  layer4.1.conv1.weight\n",
      "not found:  layer4.1.conv2.weight\n",
      "not found:  layer4.1.bn1.weight\n",
      "not found:  layer4.1.bn1.bias\n",
      "not found:  layer4.1.bn1.running_mean\n",
      "not found:  layer4.1.bn1.running_var\n",
      "not found:  layer4.1.bn1.num_batches_tracked\n",
      "not found:  layer4.1.bn2.weight\n",
      "not found:  layer4.1.bn2.bias\n",
      "not found:  layer4.1.bn2.running_mean\n",
      "not found:  layer4.1.bn2.running_var\n",
      "not found:  layer4.1.bn2.num_batches_tracked\n",
      "not found:  linear.weight\n",
      "not found:  linear.bias\n",
      "not found:  conv1.weight\n",
      "not found:  bn1.weight\n",
      "not found:  bn1.bias\n",
      "not found:  bn1.running_mean\n",
      "not found:  bn1.running_var\n",
      "not found:  bn1.num_batches_tracked\n",
      "not found:  layer1.0.conv1.weight\n",
      "not found:  layer1.0.conv2.weight\n",
      "not found:  layer1.0.bn1.weight\n",
      "not found:  layer1.0.bn1.bias\n",
      "not found:  layer1.0.bn1.running_mean\n",
      "not found:  layer1.0.bn1.running_var\n",
      "not found:  layer1.0.bn1.num_batches_tracked\n",
      "not found:  layer1.0.bn2.weight\n",
      "not found:  layer1.0.bn2.bias\n",
      "not found:  layer1.0.bn2.running_mean\n",
      "not found:  layer1.0.bn2.running_var\n",
      "not found:  layer1.0.bn2.num_batches_tracked\n",
      "not found:  layer1.1.conv1.weight\n",
      "not found:  layer1.1.conv2.weight\n",
      "not found:  layer1.1.bn1.weight\n",
      "not found:  layer1.1.bn1.bias\n",
      "not found:  layer1.1.bn1.running_mean\n",
      "not found:  layer1.1.bn1.running_var\n",
      "not found:  layer1.1.bn1.num_batches_tracked\n",
      "not found:  layer1.1.bn2.weight\n",
      "not found:  layer1.1.bn2.bias\n",
      "not found:  layer1.1.bn2.running_mean\n",
      "not found:  layer1.1.bn2.running_var\n",
      "not found:  layer1.1.bn2.num_batches_tracked\n",
      "not found:  layer2.0.conv1.weight\n",
      "not found:  layer2.0.conv2.weight\n",
      "not found:  layer2.0.bn1.weight\n",
      "not found:  layer2.0.bn1.bias\n",
      "not found:  layer2.0.bn1.running_mean\n",
      "not found:  layer2.0.bn1.running_var\n",
      "not found:  layer2.0.bn1.num_batches_tracked\n",
      "not found:  layer2.0.bn2.weight\n",
      "not found:  layer2.0.bn2.bias\n",
      "not found:  layer2.0.bn2.running_mean\n",
      "not found:  layer2.0.bn2.running_var\n",
      "not found:  layer2.0.bn2.num_batches_tracked\n",
      "not found:  layer2.0.shortcut.0.weight\n",
      "not found:  layer2.0.shortcut.1.weight\n",
      "not found:  layer2.0.shortcut.1.bias\n",
      "not found:  layer2.0.shortcut.1.running_mean\n",
      "not found:  layer2.0.shortcut.1.running_var\n",
      "not found:  layer2.0.shortcut.1.num_batches_tracked\n",
      "not found:  layer2.1.conv1.weight\n",
      "not found:  layer2.1.conv2.weight\n",
      "not found:  layer2.1.bn1.weight\n",
      "not found:  layer2.1.bn1.bias\n",
      "not found:  layer2.1.bn1.running_mean\n",
      "not found:  layer2.1.bn1.running_var\n",
      "not found:  layer2.1.bn1.num_batches_tracked\n",
      "not found:  layer2.1.bn2.weight\n",
      "not found:  layer2.1.bn2.bias\n",
      "not found:  layer2.1.bn2.running_mean\n",
      "not found:  layer2.1.bn2.running_var\n",
      "not found:  layer2.1.bn2.num_batches_tracked\n",
      "not found:  layer3.0.conv1.weight\n",
      "not found:  layer3.0.conv2.weight\n",
      "not found:  layer3.0.bn1.weight\n",
      "not found:  layer3.0.bn1.bias\n",
      "not found:  layer3.0.bn1.running_mean\n",
      "not found:  layer3.0.bn1.running_var\n",
      "not found:  layer3.0.bn1.num_batches_tracked\n",
      "not found:  layer3.0.bn2.weight\n",
      "not found:  layer3.0.bn2.bias\n",
      "not found:  layer3.0.bn2.running_mean\n",
      "not found:  layer3.0.bn2.running_var\n",
      "not found:  layer3.0.bn2.num_batches_tracked\n",
      "not found:  layer3.0.shortcut.0.weight\n",
      "not found:  layer3.0.shortcut.1.weight\n",
      "not found:  layer3.0.shortcut.1.bias\n",
      "not found:  layer3.0.shortcut.1.running_mean\n",
      "not found:  layer3.0.shortcut.1.running_var\n",
      "not found:  layer3.0.shortcut.1.num_batches_tracked\n",
      "not found:  layer3.1.conv1.weight\n",
      "not found:  layer3.1.conv2.weight\n",
      "not found:  layer3.1.bn1.weight\n",
      "not found:  layer3.1.bn1.bias\n",
      "not found:  layer3.1.bn1.running_mean\n",
      "not found:  layer3.1.bn1.running_var\n",
      "not found:  layer3.1.bn1.num_batches_tracked\n",
      "not found:  layer3.1.bn2.weight\n",
      "not found:  layer3.1.bn2.bias\n",
      "not found:  layer3.1.bn2.running_mean\n",
      "not found:  layer3.1.bn2.running_var\n",
      "not found:  layer3.1.bn2.num_batches_tracked\n"
     ]
    }
   ],
   "source": [
    "split_model = [layer_1, layer_2, layer_3, layer_4]\n",
    "\n",
    "# load model params into dictionary\n",
    "state_dict = torch.load(io.get_model_path(\"{}\".format(configs[\"load_model\"])), map_location=configs['device'])\n",
    "\n",
    "# add params to split\n",
    "for l in split_model:\n",
    "    l = io.load_state_dict(l, \n",
    "                    state_dict['model_state_dict'] if 'model_state_dict' in state_dict \n",
    "                    else state_dict['state_dict'] if 'state_dict' in state_dict else state_dict,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['conv1.weight', 'bn1.weight', 'bn1.bias', 'bn1.running_mean', 'bn1.running_var', 'bn1.num_batches_tracked', 'layer1.0.conv1.weight', 'layer1.0.conv2.weight', 'layer1.0.bn1.weight', 'layer1.0.bn1.bias', 'layer1.0.bn1.running_mean', 'layer1.0.bn1.running_var', 'layer1.0.bn1.num_batches_tracked', 'layer1.0.bn2.weight', 'layer1.0.bn2.bias', 'layer1.0.bn2.running_mean', 'layer1.0.bn2.running_var', 'layer1.0.bn2.num_batches_tracked', 'layer1.1.conv1.weight', 'layer1.1.conv2.weight', 'layer1.1.bn1.weight', 'layer1.1.bn1.bias', 'layer1.1.bn1.running_mean', 'layer1.1.bn1.running_var', 'layer1.1.bn1.num_batches_tracked', 'layer1.1.bn2.weight', 'layer1.1.bn2.bias', 'layer1.1.bn2.running_mean', 'layer1.1.bn2.running_var', 'layer1.1.bn2.num_batches_tracked', 'layer2.0.conv1.weight', 'layer2.0.conv2.weight', 'layer2.0.bn1.weight', 'layer2.0.bn1.bias', 'layer2.0.bn1.running_mean', 'layer2.0.bn1.running_var', 'layer2.0.bn1.num_batches_tracked', 'layer2.0.bn2.weight', 'layer2.0.bn2.bias', 'layer2.0.bn2.running_mean', 'layer2.0.bn2.running_var', 'layer2.0.bn2.num_batches_tracked', 'layer2.0.shortcut.0.weight', 'layer2.0.shortcut.1.weight', 'layer2.0.shortcut.1.bias', 'layer2.0.shortcut.1.running_mean', 'layer2.0.shortcut.1.running_var', 'layer2.0.shortcut.1.num_batches_tracked', 'layer2.1.conv1.weight', 'layer2.1.conv2.weight', 'layer2.1.bn1.weight', 'layer2.1.bn1.bias', 'layer2.1.bn1.running_mean', 'layer2.1.bn1.running_var', 'layer2.1.bn1.num_batches_tracked', 'layer2.1.bn2.weight', 'layer2.1.bn2.bias', 'layer2.1.bn2.running_mean', 'layer2.1.bn2.running_var', 'layer2.1.bn2.num_batches_tracked', 'layer3.0.conv1.weight', 'layer3.0.conv2.weight', 'layer3.0.bn1.weight', 'layer3.0.bn1.bias', 'layer3.0.bn1.running_mean', 'layer3.0.bn1.running_var', 'layer3.0.bn1.num_batches_tracked', 'layer3.0.bn2.weight', 'layer3.0.bn2.bias', 'layer3.0.bn2.running_mean', 'layer3.0.bn2.running_var', 'layer3.0.bn2.num_batches_tracked', 'layer3.0.shortcut.0.weight', 'layer3.0.shortcut.1.weight', 'layer3.0.shortcut.1.bias', 'layer3.0.shortcut.1.running_mean', 'layer3.0.shortcut.1.running_var', 'layer3.0.shortcut.1.num_batches_tracked', 'layer3.1.conv1.weight', 'layer3.1.conv2.weight', 'layer3.1.bn1.weight', 'layer3.1.bn1.bias', 'layer3.1.bn1.running_mean', 'layer3.1.bn1.running_var', 'layer3.1.bn1.num_batches_tracked', 'layer3.1.bn2.weight', 'layer3.1.bn2.bias', 'layer3.1.bn2.running_mean', 'layer3.1.bn2.running_var', 'layer3.1.bn2.num_batches_tracked', 'layer4.0.conv1.weight', 'layer4.0.conv2.weight', 'layer4.0.bn1.weight', 'layer4.0.bn1.bias', 'layer4.0.bn1.running_mean', 'layer4.0.bn1.running_var', 'layer4.0.bn1.num_batches_tracked', 'layer4.0.bn2.weight', 'layer4.0.bn2.bias', 'layer4.0.bn2.running_mean', 'layer4.0.bn2.running_var', 'layer4.0.bn2.num_batches_tracked', 'layer4.0.shortcut.0.weight', 'layer4.0.shortcut.1.weight', 'layer4.0.shortcut.1.bias', 'layer4.0.shortcut.1.running_mean', 'layer4.0.shortcut.1.running_var', 'layer4.0.shortcut.1.num_batches_tracked', 'layer4.1.conv1.weight', 'layer4.1.conv2.weight', 'layer4.1.bn1.weight', 'layer4.1.bn1.bias', 'layer4.1.bn1.running_mean', 'layer4.1.bn1.running_var', 'layer4.1.bn1.num_batches_tracked', 'layer4.1.bn2.weight', 'layer4.1.bn2.bias', 'layer4.1.bn2.running_mean', 'layer4.1.bn2.running_var', 'layer4.1.bn2.num_batches_tracked', 'linear.weight', 'linear.bias'])\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "# look at state dict keys\n",
    "print(state_dict.keys())\n",
    "for i in split_model:\n",
    "    print(len(l.state_dict().keys()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load weights into full model\n",
    "model = io.load_state_dict(model, \n",
    "                    state_dict['model_state_dict'] if 'model_state_dict' in state_dict \n",
    "                    else state_dict['state_dict'] if 'state_dict' in state_dict else state_dict,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches: 1000/1000\n",
      "histogram tensor([  -26.5522,  -910.8924,   328.6273,  -977.7976,   448.9310, -1293.6526,\n",
      "         3071.8613,  -943.0629,  -225.1325, -1041.1338])\n"
     ]
    }
   ],
   "source": [
    "# compare outputs\n",
    "\n",
    "input = torch.rand(1000, 3, 32, 32, device=torch.device(configs['device'])) # 1k images, 3 channels, 32x32 image (cifar100) \n",
    "\n",
    "# put models into eval mode and on device\n",
    "model.eval()\n",
    "model.to(configs['device'])\n",
    "for l in split_model:\n",
    "    l.eval()\n",
    "    l.to(configs['device'])\n",
    "\n",
    "# make inference \n",
    "with torch.no_grad():\n",
    "        output_full = model(input)\n",
    "\n",
    "        output_split = input\n",
    "        for l in split_model:\n",
    "                output_split = l(output_split)\n",
    "\n",
    "\n",
    "match_count = (torch.argmax(output_split, axis=1) == torch.argmax(output_full, axis=1)).sum().item()\n",
    "label_hist = output_full.sum(0)\n",
    "print(f'Matches: {match_count}/{output_full.size(0)}')\n",
    "print(f'histogram {label_hist}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-3.3825e-04, -6.1009e-04, -4.1753e-04,  ..., -7.2118e-04,\n",
       "           -5.2659e-04, -3.6228e-04],\n",
       "          [-5.8772e-04, -2.8550e-04, -6.0061e-04,  ..., -9.9537e-04,\n",
       "           -6.4957e-04, -5.8841e-04],\n",
       "          [-3.3169e-04, -4.3085e-04, -2.2344e-04,  ..., -1.0928e-03,\n",
       "           -7.2443e-04, -4.7181e-04],\n",
       "          ...,\n",
       "          [-3.4766e-04, -5.4073e-04, -5.5399e-04,  ..., -6.8473e-04,\n",
       "           -9.4878e-04, -4.7071e-04],\n",
       "          [-6.7092e-04, -7.6038e-04, -6.9611e-04,  ..., -9.6566e-04,\n",
       "           -6.9640e-04, -4.1652e-04],\n",
       "          [-5.6015e-04, -6.0670e-04, -4.8848e-04,  ..., -7.6346e-04,\n",
       "           -5.8965e-04, -1.2903e-04]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "         [[ 1.7728e-04,  3.3555e-04,  4.1816e-04,  ...,  4.2804e-04,\n",
       "            2.1105e-04, -2.1707e-06],\n",
       "          [ 4.2280e-04,  3.8467e-05, -8.2576e-05,  ...,  4.2417e-04,\n",
       "            2.5595e-04,  2.7698e-04],\n",
       "          [ 1.2569e-04,  1.1128e-04,  2.2602e-04,  ...,  2.2822e-04,\n",
       "            4.7665e-04,  3.0631e-04],\n",
       "          ...,\n",
       "          [ 1.3782e-04, -7.5053e-06,  2.0531e-04,  ...,  1.4668e-04,\n",
       "           -2.0620e-05,  2.3860e-04],\n",
       "          [-2.4942e-06,  4.2136e-04,  3.8359e-04,  ...,  3.4418e-04,\n",
       "            3.6726e-04,  1.1075e-04],\n",
       "          [-7.1735e-05,  6.2940e-05, -2.1534e-04,  ..., -6.0518e-05,\n",
       "           -9.2800e-05,  3.3104e-05]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[-3.1441e-04, -4.6347e-04, -5.5031e-04,  ..., -5.5852e-04,\n",
       "           -7.0042e-04, -4.5754e-04],\n",
       "          [-3.9958e-04, -6.9138e-04, -9.5291e-04,  ..., -7.7794e-04,\n",
       "           -7.5755e-04, -5.8507e-04],\n",
       "          [-3.7772e-04, -6.8623e-04, -1.0064e-03,  ..., -8.9323e-04,\n",
       "           -6.6882e-04, -7.8326e-04],\n",
       "          ...,\n",
       "          [-6.8479e-04, -6.3086e-04, -7.3983e-04,  ..., -5.9325e-04,\n",
       "           -5.2529e-04, -5.2762e-04],\n",
       "          [-4.8417e-04, -9.2800e-04, -1.0116e-03,  ..., -5.6759e-04,\n",
       "           -5.7508e-04, -4.4171e-04],\n",
       "          [-6.7873e-04, -6.3232e-04, -7.3964e-04,  ..., -5.3024e-04,\n",
       "           -2.6776e-04, -1.6916e-04]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "         [[ 1.7501e-04,  2.4140e-04,  1.9642e-04,  ...,  2.4922e-04,\n",
       "            2.3864e-04, -8.2868e-05],\n",
       "          [ 1.6638e-04,  3.3225e-05,  3.3739e-04,  ...,  1.4463e-04,\n",
       "            1.3023e-04,  5.0128e-04],\n",
       "          [ 3.9107e-04,  5.1613e-04, -3.7887e-05,  ...,  3.6423e-04,\n",
       "            5.1655e-04,  3.6440e-04],\n",
       "          ...,\n",
       "          [ 2.0505e-04,  3.9804e-04,  2.1682e-04,  ...,  4.2102e-04,\n",
       "            2.9753e-04,  5.5790e-04],\n",
       "          [ 4.1014e-04, -1.9349e-04,  6.0026e-05,  ..., -2.2157e-05,\n",
       "            6.6002e-04,  2.6489e-04],\n",
       "          [-2.8643e-04,  7.3929e-05, -2.8187e-04,  ..., -1.1781e-04,\n",
       "           -1.4869e-04,  6.9514e-05]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[-2.7437e-04, -4.6850e-04, -5.5127e-04,  ..., -2.9815e-04,\n",
       "           -3.4074e-04, -3.1273e-04],\n",
       "          [-6.1916e-04, -5.3737e-04, -4.4127e-04,  ..., -5.9980e-04,\n",
       "           -6.7915e-04, -4.7886e-04],\n",
       "          [-3.3224e-04, -4.8460e-04, -7.0620e-04,  ..., -6.9257e-04,\n",
       "           -6.0760e-04, -7.3820e-04],\n",
       "          ...,\n",
       "          [-3.0021e-04, -5.4183e-04, -3.5403e-04,  ..., -7.0301e-04,\n",
       "           -5.1676e-04, -6.5374e-04],\n",
       "          [-4.1283e-04, -6.1557e-04, -4.0019e-04,  ..., -7.1991e-04,\n",
       "           -7.9885e-04, -4.4635e-04],\n",
       "          [-5.2912e-04, -3.1943e-04, -3.6047e-04,  ..., -6.0252e-04,\n",
       "           -5.6931e-04, -3.9704e-04]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "         [[ 2.4368e-04,  4.7100e-04,  5.2555e-04,  ..., -1.2608e-05,\n",
       "            1.7655e-04,  2.3288e-04],\n",
       "          [ 4.8367e-04,  6.4821e-05,  1.3673e-04,  ...,  2.7515e-04,\n",
       "            3.4007e-04,  2.4607e-04],\n",
       "          [ 2.1029e-04,  1.4597e-04,  4.4765e-04,  ...,  4.4121e-04,\n",
       "            4.8442e-04,  5.3744e-04],\n",
       "          ...,\n",
       "          [ 2.3540e-04,  4.8346e-04,  4.0744e-04,  ...,  6.0028e-05,\n",
       "            4.0691e-04,  4.6749e-04],\n",
       "          [ 3.5484e-04,  2.1218e-04,  3.2451e-04,  ...,  2.4613e-04,\n",
       "            2.4595e-04,  4.3455e-04],\n",
       "          [-1.5253e-04, -3.3926e-05, -2.4030e-04,  ..., -2.0820e-04,\n",
       "           -5.9044e-05,  1.9750e-05]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[-2.8079e-04, -5.2980e-04, -3.9163e-04,  ..., -5.4694e-04,\n",
       "           -7.5270e-04, -3.6309e-04],\n",
       "          [-2.6149e-04, -6.0942e-04, -4.5126e-04,  ..., -8.8332e-04,\n",
       "           -6.0333e-04, -7.0374e-04],\n",
       "          [-6.2461e-04, -4.3619e-04, -3.7710e-04,  ..., -9.3934e-04,\n",
       "           -5.4273e-04, -7.2356e-04],\n",
       "          ...,\n",
       "          [-7.4931e-04, -6.2311e-04, -6.8821e-04,  ..., -7.3262e-04,\n",
       "           -8.4084e-04, -5.5145e-04],\n",
       "          [-5.6493e-04, -7.5673e-04, -7.2392e-04,  ..., -5.5011e-04,\n",
       "           -5.7931e-04, -5.7802e-04],\n",
       "          [-5.7248e-04, -3.6345e-04, -5.0755e-04,  ..., -4.7339e-04,\n",
       "           -5.7912e-04, -4.4708e-04]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "         [[ 4.3599e-04,  2.9566e-04,  1.0982e-04,  ...,  2.7353e-04,\n",
       "            7.2279e-04,  3.6134e-04],\n",
       "          [ 1.2942e-04,  2.3636e-04,  1.2033e-04,  ...,  3.6857e-04,\n",
       "            1.1975e-04,  1.6168e-04],\n",
       "          [ 3.7520e-04,  3.8059e-04,  3.5256e-04,  ..., -2.1015e-04,\n",
       "            1.2519e-04,  6.4287e-04],\n",
       "          ...,\n",
       "          [ 4.6636e-04,  5.3618e-04,  1.8656e-04,  ...,  2.7060e-04,\n",
       "            1.9045e-04, -3.6547e-05],\n",
       "          [ 4.5291e-04,  4.2281e-04,  2.3847e-04,  ...,  5.5871e-04,\n",
       "            5.8624e-05,  4.7538e-04],\n",
       "          [-2.1770e-04, -4.2413e-05, -3.7344e-05,  ..., -9.4822e-05,\n",
       "            5.6208e-05, -6.6406e-05]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[-6.5740e-05, -6.7357e-04, -7.1458e-04,  ..., -3.3455e-04,\n",
       "           -6.4451e-04, -5.7484e-04],\n",
       "          [-3.3957e-04, -1.0023e-03, -7.1845e-04,  ..., -7.8427e-04,\n",
       "           -7.2001e-04, -6.8416e-04],\n",
       "          [-7.3079e-04, -8.4966e-04, -4.1083e-04,  ..., -8.4265e-04,\n",
       "           -9.5198e-04, -3.7014e-04],\n",
       "          ...,\n",
       "          [-2.6360e-04, -4.1473e-04, -6.5226e-04,  ..., -7.4533e-04,\n",
       "           -6.3876e-04, -7.7519e-04],\n",
       "          [-3.7677e-04, -6.6932e-04, -7.6141e-04,  ..., -4.7318e-04,\n",
       "           -8.2068e-04, -8.3161e-04],\n",
       "          [-2.1033e-04, -5.2692e-04, -5.6286e-04,  ..., -6.0114e-04,\n",
       "           -6.9715e-04, -5.5303e-04]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "         [[ 6.1614e-04,  4.9320e-04,  5.2538e-04,  ...,  4.7270e-04,\n",
       "            3.5621e-04,  3.7944e-04],\n",
       "          [ 7.8861e-05,  2.0522e-04,  1.7745e-04,  ...,  2.0572e-04,\n",
       "            5.5821e-05,  3.0363e-04],\n",
       "          [ 2.1585e-04,  4.1227e-04,  1.6122e-04,  ...,  2.4006e-04,\n",
       "            2.9461e-04,  1.9277e-04],\n",
       "          ...,\n",
       "          [ 1.2489e-04,  3.2400e-04,  2.9557e-04,  ...,  2.7140e-04,\n",
       "            6.9148e-04,  2.3691e-04],\n",
       "          [ 1.1862e-04,  3.4650e-04,  3.5675e-04,  ...,  1.3638e-05,\n",
       "            1.3625e-04,  1.9504e-04],\n",
       "          [-1.7153e-04,  5.7718e-06, -1.5374e-04,  ..., -2.1790e-04,\n",
       "            9.5463e-06,  4.7364e-05]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[-3.4298e-04, -8.8093e-04, -5.7658e-04,  ..., -6.2237e-04,\n",
       "           -5.1893e-04, -5.9311e-04],\n",
       "          [-7.2215e-04, -8.1791e-04, -4.7356e-04,  ..., -5.6771e-04,\n",
       "           -8.0026e-04, -8.7307e-04],\n",
       "          [-3.2034e-04, -2.5983e-04, -4.7924e-04,  ..., -5.7610e-04,\n",
       "           -6.1840e-04, -7.3979e-04],\n",
       "          ...,\n",
       "          [-4.8327e-04, -6.0848e-04, -6.7543e-04,  ..., -4.8925e-04,\n",
       "           -6.1652e-04, -6.1878e-04],\n",
       "          [-5.1179e-04, -5.1707e-04, -4.5111e-04,  ..., -7.9102e-04,\n",
       "           -4.2217e-04, -4.1032e-04],\n",
       "          [-2.3457e-04, -4.3511e-04, -4.4798e-04,  ..., -5.2976e-04,\n",
       "           -4.9878e-04, -1.3205e-04]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "         [[ 5.6517e-04,  5.7846e-04,  2.1197e-04,  ...,  3.3367e-04,\n",
       "            3.4833e-04,  2.7888e-04],\n",
       "          [ 1.9850e-04, -1.1230e-04,  4.5690e-04,  ...,  4.6375e-04,\n",
       "            3.6043e-04,  7.8115e-05],\n",
       "          [ 2.5963e-04,  3.1985e-04,  1.5184e-04,  ...,  4.8220e-04,\n",
       "            1.7140e-04,  3.3666e-04],\n",
       "          ...,\n",
       "          [ 5.9748e-04,  3.8616e-04,  4.2728e-04,  ...,  1.3882e-04,\n",
       "            2.0366e-04,  2.4582e-04],\n",
       "          [ 6.4808e-05,  3.3534e-04,  1.0835e-04,  ...,  6.8770e-04,\n",
       "            3.7743e-04,  1.3664e-04],\n",
       "          [ 4.8669e-05, -2.6107e-04, -4.4517e-05,  ..., -5.0030e-05,\n",
       "           -1.1284e-04,  4.6889e-05]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]]]], grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# go layer by layer to identify mismatch\n",
    "split_model[0].conv1(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    Test I/O Logic\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    Test I/O Logic\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE\n",
    "\n",
    "# make dir name \n",
    "time_stamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "if len(configs['load_model']) == 0:\n",
    "    folder_name='{}-{}-{}-np{}-pr{}-lcm{}-{}'.format( \n",
    "                configs['data-code'], \n",
    "                configs['model'], \n",
    "                configs['sparsity-type'], \n",
    "                configs['num_partition'], \n",
    "                configs['prune-ratio'], \n",
    "                configs['lambda-comm'],\n",
    "                time_stamp)\n",
    "else:\n",
    "    folder_name = '{}-{}'.format(configs['load_model'][:-3],time_stamp)\n",
    "\n",
    "# make folder \n",
    "folder_path = os.path.join(os.getcwd(), 'assets', 'models',folder_name)\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "\n",
    "# save weights\n",
    "index = 0\n",
    "for l in split_model:\n",
    "    fpath = os.path.join(os.getcwd(), 'assets', 'models', folder_path, f'layer_model_{index}.pth')\n",
    "    torch.save(l.state_dict(), fpath)\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split models:\n",
      "['cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240615-135016', 'cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240615-141807', 'cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240622-092931', 'cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240622-125523', 'cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240623-182335', 'cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240623-182910', 'cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240624-204135', 'cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240625-073155', 'cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240626-090056', 'cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240626-202301', 'cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240626-213627', 'cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240626-213724', 'cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240626-214353', 'cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240627-105309', 'cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240629-090932', 'cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240629-095759', 'cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240701-181315', 'cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240701-220925', 'cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240702-082928', 'cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240702-083133']\n",
      "\n",
      "loading split model cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001-20240615-135016\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 32, 32]           1,728\n",
      "       BatchNorm2d-2           [-1, 64, 32, 32]             128\n",
      "            Conv2d-3           [-1, 64, 32, 32]          36,864\n",
      "       BatchNorm2d-4           [-1, 64, 32, 32]             128\n",
      "            Conv2d-5           [-1, 64, 32, 32]          36,864\n",
      "       BatchNorm2d-6           [-1, 64, 32, 32]             128\n",
      "        BasicBlock-7           [-1, 64, 32, 32]               0\n",
      "            Conv2d-8           [-1, 64, 32, 32]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 32, 32]             128\n",
      "           Conv2d-10           [-1, 64, 32, 32]          36,864\n",
      "      BatchNorm2d-11           [-1, 64, 32, 32]             128\n",
      "       BasicBlock-12           [-1, 64, 32, 32]               0\n",
      "================================================================\n",
      "Total params: 149,824\n",
      "Trainable params: 149,824\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 6.00\n",
      "Params size (MB): 0.57\n",
      "Estimated Total Size (MB): 6.58\n",
      "----------------------------------------------------------------\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 128, 16, 16]          73,728\n",
      "       BatchNorm2d-2          [-1, 128, 16, 16]             256\n",
      "            Conv2d-3          [-1, 128, 16, 16]         147,456\n",
      "       BatchNorm2d-4          [-1, 128, 16, 16]             256\n",
      "            Conv2d-5          [-1, 128, 16, 16]           8,192\n",
      "       BatchNorm2d-6          [-1, 128, 16, 16]             256\n",
      "        BasicBlock-7          [-1, 128, 16, 16]               0\n",
      "            Conv2d-8          [-1, 128, 16, 16]         147,456\n",
      "       BatchNorm2d-9          [-1, 128, 16, 16]             256\n",
      "           Conv2d-10          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-11          [-1, 128, 16, 16]             256\n",
      "       BasicBlock-12          [-1, 128, 16, 16]               0\n",
      "================================================================\n",
      "Total params: 525,568\n",
      "Trainable params: 525,568\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.25\n",
      "Forward/backward pass size (MB): 3.00\n",
      "Params size (MB): 2.00\n",
      "Estimated Total Size (MB): 5.25\n",
      "----------------------------------------------------------------\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 256, 8, 8]         294,912\n",
      "       BatchNorm2d-2            [-1, 256, 8, 8]             512\n",
      "            Conv2d-3            [-1, 256, 8, 8]         589,824\n",
      "       BatchNorm2d-4            [-1, 256, 8, 8]             512\n",
      "            Conv2d-5            [-1, 256, 8, 8]          32,768\n",
      "       BatchNorm2d-6            [-1, 256, 8, 8]             512\n",
      "        BasicBlock-7            [-1, 256, 8, 8]               0\n",
      "            Conv2d-8            [-1, 256, 8, 8]         589,824\n",
      "       BatchNorm2d-9            [-1, 256, 8, 8]             512\n",
      "           Conv2d-10            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-11            [-1, 256, 8, 8]             512\n",
      "       BasicBlock-12            [-1, 256, 8, 8]               0\n",
      "================================================================\n",
      "Total params: 2,099,712\n",
      "Trainable params: 2,099,712\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.12\n",
      "Forward/backward pass size (MB): 1.50\n",
      "Params size (MB): 8.01\n",
      "Estimated Total Size (MB): 9.63\n",
      "----------------------------------------------------------------\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 512, 4, 4]       1,179,648\n",
      "       BatchNorm2d-2            [-1, 512, 4, 4]           1,024\n",
      "            Conv2d-3            [-1, 512, 4, 4]       2,359,296\n",
      "       BatchNorm2d-4            [-1, 512, 4, 4]           1,024\n",
      "            Conv2d-5            [-1, 512, 4, 4]         131,072\n",
      "       BatchNorm2d-6            [-1, 512, 4, 4]           1,024\n",
      "        BasicBlock-7            [-1, 512, 4, 4]               0\n",
      "            Conv2d-8            [-1, 512, 4, 4]       2,359,296\n",
      "       BatchNorm2d-9            [-1, 512, 4, 4]           1,024\n",
      "           Conv2d-10            [-1, 512, 4, 4]       2,359,296\n",
      "      BatchNorm2d-11            [-1, 512, 4, 4]           1,024\n",
      "       BasicBlock-12            [-1, 512, 4, 4]               0\n",
      "           Linear-13                   [-1, 10]           5,130\n",
      "================================================================\n",
      "Total params: 8,398,858\n",
      "Trainable params: 8,398,858\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.06\n",
      "Forward/backward pass size (MB): 0.75\n",
      "Params size (MB): 32.04\n",
      "Estimated Total Size (MB): 32.85\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# LOAD\n",
    "\n",
    "select = 0\n",
    "\n",
    "layer_output_sizes = [(3,32,32), (64,32,32), (128,16,16), (256,8,8)]\n",
    "\n",
    "model_path = os.path.join(os.getcwd(), 'assets', 'models')\n",
    "filenames = os.listdir(model_path)\n",
    "\n",
    "# get dirs\n",
    "split_model_names = []\n",
    "for filename in filenames: # loop through all the files and folders\n",
    "    if os.path.isdir(os.path.join(model_path, filename)): # check whether the current object is a folder or not\n",
    "        split_model_names.append(filename)\n",
    "\n",
    "print('Split models:')\n",
    "print(split_model_names)\n",
    "print()\n",
    "\n",
    "model_name = split_model_names[select] \n",
    "print(f'loading split model {model_name}')\n",
    "\n",
    "index = 0\n",
    "for l in split_model:\n",
    "    layer_state_dict = torch.load(os.path.join(model_path, model_name, f'layer_model_{index}.pth'))\n",
    "    l = io.load_state_dict(l, layer_state_dict)\n",
    "\n",
    "\n",
    "    summary(l, layer_output_sizes[index],device=configs['device'])\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference time per data is 23.136139ms.\n",
      "conv1.weight 1024\n",
      "layer1.0.conv1.weight 1024\n",
      "layer1.0.conv2.weight 1024\n",
      "layer1.1.conv1.weight 1024\n",
      "layer1.1.conv2.weight 1024\n",
      "layer2.0.conv1.weight 256\n",
      "layer2.0.conv2.weight 256\n",
      "layer2.0.shortcut.0.weight 256\n",
      "layer2.1.conv1.weight 256\n",
      "layer2.1.conv2.weight 256\n",
      "layer3.0.conv1.weight 64\n",
      "layer3.0.conv2.weight 64\n",
      "layer3.0.shortcut.0.weight 64\n",
      "layer3.1.conv1.weight 64\n",
      "layer3.1.conv2.weight 64\n",
      "layer4.0.conv1.weight 16\n",
      "layer4.0.conv2.weight 16\n",
      "layer4.0.shortcut.0.weight 16\n",
      "layer4.1.conv1.weight 16\n",
      "layer4.1.conv2.weight 16\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    add partitions and communications to configs\n",
    "'''\n",
    "\n",
    "# gets random test input (with correct size)\n",
    "input_var = engine.get_input_from_code(configs)\n",
    "#print(input_var)\n",
    "\n",
    "# Config partitions and prune_ratio\n",
    "configs['num_partition'] = '4'#'./config/resnet18-v2.yaml'\n",
    "configs = engine.partition_generator(configs, model)\n",
    "            \n",
    "# Compute output size of each layer\n",
    "configs['partition'] = engine.featuremap_summary(model, configs['partition'], input_var)\n",
    "        \n",
    "# Setup communication costs\n",
    "configs['comm_costs'] = engine.set_communication_cost(model, configs['partition'],)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\natet\\Desktop\\graduate school\\thesis\\CaP\\assets\\figs\\cifar10-resnet18-kernel-npv2-pr0.75-lcm0.001\n",
      "name: conv1.weight, img size: (3, 64) weight size: (64, 3)\n",
      "name: layer1.0.conv1.weight, img size: (64, 64) weight size: (64, 64)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    inspect IO per layer \n",
    "'''\n",
    "\n",
    "savepath=io.get_fig_path(\"{}\".format('.'.join(configs[\"load_model\"].split('.')[:-1])))\n",
    "print(savepath)\n",
    "\n",
    "# this function looks for model layers named in \"confgis['partition']\" (other layers are ignored)\n",
    "# -> \"conv\" and shortcut layers are the only ones \"split\"\n",
    "# -> total 20/49 (20/62?) layers are split\n",
    "counter = 0\n",
    "for name, W in model.named_parameters():\n",
    "        if name in configs['partition']:\n",
    "            #print(f'{counter} | {name}')\n",
    "            counter +=1\n",
    "\n",
    "# Plot model\n",
    "layer_id = (0,1,2) # inspect these layers\n",
    "testers.plot_layer(model, configs['partition'], layer_id=layer_id, savepath=savepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'conv1', 'bn1', 'layer1', 'layer1.0', 'layer1.0.conv1', 'layer1.0.conv2', 'layer1.0.bn1', 'layer1.0.bn2', 'layer1.0.shortcut', 'layer1.1', 'layer1.1.conv1', 'layer1.1.conv2', 'layer1.1.bn1', 'layer1.1.bn2', 'layer1.1.shortcut', 'layer2', 'layer2.0', 'layer2.0.conv1', 'layer2.0.conv2', 'layer2.0.bn1', 'layer2.0.bn2', 'layer2.0.shortcut', 'layer2.0.shortcut.0', 'layer2.0.shortcut.1', 'layer2.1', 'layer2.1.conv1', 'layer2.1.conv2', 'layer2.1.bn1', 'layer2.1.bn2', 'layer2.1.shortcut', 'layer3', 'layer3.0', 'layer3.0.conv1', 'layer3.0.conv2', 'layer3.0.bn1', 'layer3.0.bn2', 'layer3.0.shortcut', 'layer3.0.shortcut.0', 'layer3.0.shortcut.1', 'layer3.1', 'layer3.1.conv1', 'layer3.1.conv2', 'layer3.1.bn1', 'layer3.1.bn2', 'layer3.1.shortcut', 'layer4', 'layer4.0', 'layer4.0.conv1', 'layer4.0.conv2', 'layer4.0.bn1', 'layer4.0.bn2', 'layer4.0.shortcut', 'layer4.0.shortcut.0', 'layer4.0.shortcut.1', 'layer4.1', 'layer4.1.conv1', 'layer4.1.conv2', 'layer4.1.bn1', 'layer4.1.bn2', 'layer4.1.shortcut', 'linear']\n"
     ]
    }
   ],
   "source": [
    "# split model \n",
    "\n",
    "# make copies of model per machine\n",
    "num_machines = max(configs['partition']['bn_partition']) # TODO: double check this makes sense\n",
    "model_machines = [model]*num_machines\n",
    "\n",
    "module_names =  [module[0] for i, module in enumerate(model.named_modules())]\n",
    "num_total_modules = len(module_names)\n",
    "\n",
    "split_module_names = list(configs['partition'].keys())\n",
    "\n",
    "print(module_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Setup datastrcuts to ID layers executed with 'extra' functionality\n",
    "'''\n",
    "\n",
    "# module numbering is based on module.named_parameters()\n",
    "relu_modules = [2, 7,8,13,14,20,2428,29,35,3943,44,50,54,58,59] # execute relu on this  layer \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing module 0: \n",
      "\tExecuting on machine 0\n",
      "\t\t-Skipping module ResNet\n",
      "\tExecuting on machine 1\n",
      "\t\t-Skipping module ResNet\n",
      "\tExecuting on machine 2\n",
      "\t\t-Skipping module ResNet\n",
      "\tExecuting on machine 3\n",
      "\t\t-Skipping module ResNet\n",
      "Finished execution of layer 0\n",
      "\n",
      "Executing module 1: conv1\n",
      "\tExecuting on machine 0\n",
      "\t\t-No input assigned to this machine. Skipping...\n",
      "\tExecuting on machine 1\n",
      "\tExecuting on machine 2\n",
      "\tExecuting on machine 3\n",
      "Finished execution of layer 1\n",
      "\n",
      "Executing module 2: bn1\n",
      "\tExecuting on machine 0\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 1\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 2\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 3\n",
      "\t\t-Applying ReLU\n",
      "Finished execution of layer 2\n",
      "\n",
      "Executing module 3: layer1\n",
      "\tExecuting on machine 0\n",
      "\t\t-Skipping module Sequential\n",
      "\tExecuting on machine 1\n",
      "\t\t-Skipping module Sequential\n",
      "\tExecuting on machine 2\n",
      "\t\t-Skipping module Sequential\n",
      "\tExecuting on machine 3\n",
      "\t\t-Skipping module Sequential\n",
      "Finished execution of layer 3\n",
      "\n",
      "Executing module 4: layer1.0\n",
      "\tExecuting on machine 0\n",
      "\t\t-Saving input for later...\n",
      "\tExecuting on machine 1\n",
      "\t\t-Saving input for later...\n",
      "\tExecuting on machine 2\n",
      "\t\t-Saving input for later...\n",
      "\tExecuting on machine 3\n",
      "\t\t-Saving input for later...\n",
      "Finished execution of layer 4\n",
      "\n",
      "Executing module 5: layer1.0.conv1\n",
      "\tExecuting on machine 0\n",
      "\tExecuting on machine 1\n",
      "\tExecuting on machine 2\n",
      "\tExecuting on machine 3\n",
      "Finished execution of layer 5\n",
      "\n",
      "Executing module 6: layer1.0.conv2\n",
      "\tExecuting on machine 0\n",
      "\tExecuting on machine 1\n",
      "\tExecuting on machine 2\n",
      "\tExecuting on machine 3\n",
      "Finished execution of layer 6\n",
      "\n",
      "Executing module 7: layer1.0.bn1\n",
      "\tExecuting on machine 0\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 1\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 2\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 3\n",
      "\t\t-Applying ReLU\n",
      "Finished execution of layer 7\n",
      "\n",
      "Executing module 8: layer1.0.bn2\n",
      "\tExecuting on machine 0\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 1\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 2\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 3\n",
      "\t\t-Applying ReLU\n",
      "Finished execution of layer 8\n",
      "\n",
      "Executing module 9: layer1.0.shortcut\n",
      "\tExecuting on machine 0\n",
      "\t\t-Skipping module Sequential\n",
      "\tExecuting on machine 1\n",
      "\t\t-Skipping module Sequential\n",
      "\tExecuting on machine 2\n",
      "\t\t-Skipping module Sequential\n",
      "\tExecuting on machine 3\n",
      "\t\t-Skipping module Sequential\n",
      "Finished execution of layer 9\n",
      "\n",
      "Executing module 10: layer1.1\n",
      "\tExecuting on machine 0\n",
      "\t\t-Saving input for later...\n",
      "\tExecuting on machine 1\n",
      "\t\t-Saving input for later...\n",
      "\tExecuting on machine 2\n",
      "\t\t-Saving input for later...\n",
      "\tExecuting on machine 3\n",
      "\t\t-Saving input for later...\n",
      "Finished execution of layer 10\n",
      "\n",
      "Executing module 11: layer1.1.conv1\n",
      "\tExecuting on machine 0\n",
      "\tExecuting on machine 1\n",
      "\tExecuting on machine 2\n",
      "\tExecuting on machine 3\n",
      "Finished execution of layer 11\n",
      "\n",
      "Executing module 12: layer1.1.conv2\n",
      "\tExecuting on machine 0\n",
      "\tExecuting on machine 1\n",
      "\tExecuting on machine 2\n",
      "\tExecuting on machine 3\n",
      "Finished execution of layer 12\n",
      "\n",
      "Executing module 13: layer1.1.bn1\n",
      "\tExecuting on machine 0\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 1\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 2\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 3\n",
      "\t\t-Applying ReLU\n",
      "Finished execution of layer 13\n",
      "\n",
      "Executing module 14: layer1.1.bn2\n",
      "\tExecuting on machine 0\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 1\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 2\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 3\n",
      "\t\t-Applying ReLU\n",
      "Finished execution of layer 14\n",
      "\n",
      "Executing module 15: layer1.1.shortcut\n",
      "\tExecuting on machine 0\n",
      "\t\t-Skipping module Sequential\n",
      "\tExecuting on machine 1\n",
      "\t\t-Skipping module Sequential\n",
      "\tExecuting on machine 2\n",
      "\t\t-Skipping module Sequential\n",
      "\tExecuting on machine 3\n",
      "\t\t-Skipping module Sequential\n",
      "Finished execution of layer 15\n",
      "\n",
      "Executing module 16: layer2\n",
      "\tExecuting on machine 0\n",
      "\t\t-Skipping module Sequential\n",
      "\tExecuting on machine 1\n",
      "\t\t-Skipping module Sequential\n",
      "\tExecuting on machine 2\n",
      "\t\t-Skipping module Sequential\n",
      "\tExecuting on machine 3\n",
      "\t\t-Skipping module Sequential\n",
      "Finished execution of layer 16\n",
      "\n",
      "Executing module 17: layer2.0\n",
      "\tExecuting on machine 0\n",
      "\t\t-Saving input for later...\n",
      "\tExecuting on machine 1\n",
      "\t\t-Saving input for later...\n",
      "\tExecuting on machine 2\n",
      "\t\t-Saving input for later...\n",
      "\tExecuting on machine 3\n",
      "\t\t-Saving input for later...\n",
      "Finished execution of layer 17\n",
      "\n",
      "Executing module 18: layer2.0.conv1\n",
      "\tExecuting on machine 0\n",
      "\tExecuting on machine 1\n",
      "\tExecuting on machine 2\n",
      "\tExecuting on machine 3\n",
      "Finished execution of layer 18\n",
      "\n",
      "Executing module 19: layer2.0.conv2\n",
      "\tExecuting on machine 0\n",
      "\tExecuting on machine 1\n",
      "\tExecuting on machine 2\n",
      "\tExecuting on machine 3\n",
      "Finished execution of layer 19\n",
      "\n",
      "Executing module 20: layer2.0.bn1\n",
      "\tExecuting on machine 0\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 1\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 2\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 3\n",
      "\t\t-Applying ReLU\n",
      "Finished execution of layer 20\n",
      "\n",
      "Executing module 21: layer2.0.bn2\n",
      "\tExecuting on machine 0\n",
      "\tExecuting on machine 1\n",
      "\tExecuting on machine 2\n",
      "\tExecuting on machine 3\n",
      "Finished execution of layer 21\n",
      "\n",
      "Executing module 22: layer2.0.shortcut\n",
      "\tExecuting on machine 0\n",
      "\t\t-Skipping module Sequential\n",
      "\tExecuting on machine 1\n",
      "\t\t-Skipping module Sequential\n",
      "\tExecuting on machine 2\n",
      "\t\t-Skipping module Sequential\n",
      "\tExecuting on machine 3\n",
      "\t\t-Skipping module Sequential\n",
      "Finished execution of layer 22\n",
      "\n",
      "Executing module 23: layer2.0.shortcut.0\n",
      "\tExecuting on machine 0\n",
      "\tExecuting on machine 1\n",
      "\tExecuting on machine 2\n",
      "\tExecuting on machine 3\n",
      "Finished execution of layer 23\n",
      "\n",
      "Executing module 24: layer2.0.shortcut.1\n",
      "\tExecuting on machine 0\n",
      "\t\t-adding residual\n",
      "\tExecuting on machine 1\n",
      "\t\t-adding residual\n",
      "\tExecuting on machine 2\n",
      "\t\t-adding residual\n",
      "\tExecuting on machine 3\n",
      "\t\t-adding residual\n",
      "Finished execution of layer 24\n",
      "\n",
      "Executing module 25: layer2.1\n",
      "\tExecuting on machine 0\n",
      "\t\t-Saving input for later...\n",
      "\tExecuting on machine 1\n",
      "\t\t-Saving input for later...\n",
      "\tExecuting on machine 2\n",
      "\t\t-Saving input for later...\n",
      "\tExecuting on machine 3\n",
      "\t\t-Saving input for later...\n",
      "Finished execution of layer 25\n",
      "\n",
      "Executing module 26: layer2.1.conv1\n",
      "\tExecuting on machine 0\n",
      "\tExecuting on machine 1\n",
      "\tExecuting on machine 2\n",
      "\tExecuting on machine 3\n",
      "Finished execution of layer 26\n",
      "\n",
      "Executing module 27: layer2.1.conv2\n",
      "\tExecuting on machine 0\n",
      "\tExecuting on machine 1\n",
      "\tExecuting on machine 2\n",
      "\tExecuting on machine 3\n",
      "Finished execution of layer 27\n",
      "\n",
      "Executing module 28: layer2.1.bn1\n",
      "\tExecuting on machine 0\n",
      "\tExecuting on machine 1\n",
      "\tExecuting on machine 2\n",
      "\tExecuting on machine 3\n",
      "Finished execution of layer 28\n",
      "\n",
      "Executing module 29: layer2.1.bn2\n",
      "\tExecuting on machine 0\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 1\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 2\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 3\n",
      "\t\t-Applying ReLU\n",
      "Finished execution of layer 29\n",
      "\n",
      "Executing module 30: layer2.1.shortcut\n",
      "\tExecuting on machine 0\n",
      "\t\t-Skipping module Sequential\n",
      "\tExecuting on machine 1\n",
      "\t\t-Skipping module Sequential\n",
      "\tExecuting on machine 2\n",
      "\t\t-Skipping module Sequential\n",
      "\tExecuting on machine 3\n",
      "\t\t-Skipping module Sequential\n",
      "Finished execution of layer 30\n",
      "\n",
      "Executing module 31: layer3\n",
      "\tExecuting on machine 0\n",
      "\t\t-Skipping module Sequential\n",
      "\tExecuting on machine 1\n",
      "\t\t-Skipping module Sequential\n",
      "\tExecuting on machine 2\n",
      "\t\t-Skipping module Sequential\n",
      "\tExecuting on machine 3\n",
      "\t\t-Skipping module Sequential\n",
      "Finished execution of layer 31\n",
      "\n",
      "Executing module 32: layer3.0\n",
      "\tExecuting on machine 0\n",
      "\t\t-Saving input for later...\n",
      "\tExecuting on machine 1\n",
      "\t\t-Saving input for later...\n",
      "\tExecuting on machine 2\n",
      "\t\t-Saving input for later...\n",
      "\tExecuting on machine 3\n",
      "\t\t-Saving input for later...\n",
      "Finished execution of layer 32\n",
      "\n",
      "Executing module 33: layer3.0.conv1\n",
      "\tExecuting on machine 0\n",
      "\tExecuting on machine 1\n",
      "\tExecuting on machine 2\n",
      "\tExecuting on machine 3\n",
      "Finished execution of layer 33\n",
      "\n",
      "Executing module 34: layer3.0.conv2\n",
      "\tExecuting on machine 0\n",
      "\tExecuting on machine 1\n",
      "\tExecuting on machine 2\n",
      "\tExecuting on machine 3\n",
      "Finished execution of layer 34\n",
      "\n",
      "Executing module 35: layer3.0.bn1\n",
      "\tExecuting on machine 0\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 1\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 2\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 3\n",
      "\t\t-Applying ReLU\n",
      "Finished execution of layer 35\n",
      "\n",
      "Executing module 36: layer3.0.bn2\n",
      "\tExecuting on machine 0\n",
      "\tExecuting on machine 1\n",
      "\tExecuting on machine 2\n",
      "\tExecuting on machine 3\n",
      "Finished execution of layer 36\n",
      "\n",
      "Executing module 37: layer3.0.shortcut\n",
      "\tExecuting on machine 0\n",
      "\t\t-Skipping module Sequential\n",
      "\tExecuting on machine 1\n",
      "\t\t-Skipping module Sequential\n",
      "\tExecuting on machine 2\n",
      "\t\t-Skipping module Sequential\n",
      "\tExecuting on machine 3\n",
      "\t\t-Skipping module Sequential\n",
      "Finished execution of layer 37\n",
      "\n",
      "Executing module 38: layer3.0.shortcut.0\n",
      "\tExecuting on machine 0\n",
      "\tExecuting on machine 1\n",
      "\tExecuting on machine 2\n",
      "\tExecuting on machine 3\n",
      "Finished execution of layer 38\n",
      "\n",
      "Executing module 39: layer3.0.shortcut.1\n",
      "\tExecuting on machine 0\n",
      "\t\t-adding residual\n",
      "\tExecuting on machine 1\n",
      "\t\t-adding residual\n",
      "\tExecuting on machine 2\n",
      "\t\t-adding residual\n",
      "\tExecuting on machine 3\n",
      "\t\t-adding residual\n",
      "Finished execution of layer 39\n",
      "\n",
      "Executing module 40: layer3.1\n",
      "\tExecuting on machine 0\n",
      "\t\t-Saving input for later...\n",
      "\tExecuting on machine 1\n",
      "\t\t-Saving input for later...\n",
      "\tExecuting on machine 2\n",
      "\t\t-Saving input for later...\n",
      "\tExecuting on machine 3\n",
      "\t\t-Saving input for later...\n",
      "Finished execution of layer 40\n",
      "\n",
      "Executing module 41: layer3.1.conv1\n",
      "\tExecuting on machine 0\n",
      "\tExecuting on machine 1\n",
      "\tExecuting on machine 2\n",
      "\tExecuting on machine 3\n",
      "Finished execution of layer 41\n",
      "\n",
      "Executing module 42: layer3.1.conv2\n",
      "\tExecuting on machine 0\n",
      "\tExecuting on machine 1\n",
      "\tExecuting on machine 2\n",
      "\tExecuting on machine 3\n",
      "Finished execution of layer 42\n",
      "\n",
      "Executing module 43: layer3.1.bn1\n",
      "\tExecuting on machine 0\n",
      "\tExecuting on machine 1\n",
      "\tExecuting on machine 2\n",
      "\tExecuting on machine 3\n",
      "Finished execution of layer 43\n",
      "\n",
      "Executing module 44: layer3.1.bn2\n",
      "\tExecuting on machine 0\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 1\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 2\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 3\n",
      "\t\t-Applying ReLU\n",
      "Finished execution of layer 44\n",
      "\n",
      "Executing module 45: layer3.1.shortcut\n",
      "\tExecuting on machine 0\n",
      "\t\t-Skipping module Sequential\n",
      "\tExecuting on machine 1\n",
      "\t\t-Skipping module Sequential\n",
      "\tExecuting on machine 2\n",
      "\t\t-Skipping module Sequential\n",
      "\tExecuting on machine 3\n",
      "\t\t-Skipping module Sequential\n",
      "Finished execution of layer 45\n",
      "\n",
      "Executing module 46: layer4\n",
      "\tExecuting on machine 0\n",
      "\t\t-Skipping module Sequential\n",
      "\tExecuting on machine 1\n",
      "\t\t-Skipping module Sequential\n",
      "\tExecuting on machine 2\n",
      "\t\t-Skipping module Sequential\n",
      "\tExecuting on machine 3\n",
      "\t\t-Skipping module Sequential\n",
      "Finished execution of layer 46\n",
      "\n",
      "Executing module 47: layer4.0\n",
      "\tExecuting on machine 0\n",
      "\t\t-Saving input for later...\n",
      "\tExecuting on machine 1\n",
      "\t\t-Saving input for later...\n",
      "\tExecuting on machine 2\n",
      "\t\t-Saving input for later...\n",
      "\tExecuting on machine 3\n",
      "\t\t-Saving input for later...\n",
      "Finished execution of layer 47\n",
      "\n",
      "Executing module 48: layer4.0.conv1\n",
      "\tExecuting on machine 0\n",
      "\tExecuting on machine 1\n",
      "\tExecuting on machine 2\n",
      "\tExecuting on machine 3\n",
      "Finished execution of layer 48\n",
      "\n",
      "Executing module 49: layer4.0.conv2\n",
      "\tExecuting on machine 0\n",
      "\tExecuting on machine 1\n",
      "\tExecuting on machine 2\n",
      "\tExecuting on machine 3\n",
      "Finished execution of layer 49\n",
      "\n",
      "Executing module 50: layer4.0.bn1\n",
      "\tExecuting on machine 0\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 1\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 2\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 3\n",
      "\t\t-Applying ReLU\n",
      "Finished execution of layer 50\n",
      "\n",
      "Executing module 51: layer4.0.bn2\n",
      "\tExecuting on machine 0\n",
      "\tExecuting on machine 1\n",
      "\tExecuting on machine 2\n",
      "\tExecuting on machine 3\n",
      "Finished execution of layer 51\n",
      "\n",
      "Executing module 52: layer4.0.shortcut\n",
      "\tExecuting on machine 0\n",
      "\t\t-Skipping module Sequential\n",
      "\tExecuting on machine 1\n",
      "\t\t-Skipping module Sequential\n",
      "\tExecuting on machine 2\n",
      "\t\t-Skipping module Sequential\n",
      "\tExecuting on machine 3\n",
      "\t\t-Skipping module Sequential\n",
      "Finished execution of layer 52\n",
      "\n",
      "Executing module 53: layer4.0.shortcut.0\n",
      "\tExecuting on machine 0\n",
      "\tExecuting on machine 1\n",
      "\tExecuting on machine 2\n",
      "\tExecuting on machine 3\n",
      "Finished execution of layer 53\n",
      "\n",
      "Executing module 54: layer4.0.shortcut.1\n",
      "\tExecuting on machine 0\n",
      "\t\t-adding residual\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 1\n",
      "\t\t-adding residual\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 2\n",
      "\t\t-adding residual\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 3\n",
      "\t\t-adding residual\n",
      "\t\t-Applying ReLU\n",
      "Finished execution of layer 54\n",
      "\n",
      "Executing module 55: layer4.1\n",
      "\tExecuting on machine 0\n",
      "\t\t-Saving input for later...\n",
      "\tExecuting on machine 1\n",
      "\t\t-Saving input for later...\n",
      "\tExecuting on machine 2\n",
      "\t\t-Saving input for later...\n",
      "\tExecuting on machine 3\n",
      "\t\t-Saving input for later...\n",
      "Finished execution of layer 55\n",
      "\n",
      "Executing module 56: layer4.1.conv1\n",
      "\tExecuting on machine 0\n",
      "\tExecuting on machine 1\n",
      "\tExecuting on machine 2\n",
      "\tExecuting on machine 3\n",
      "Finished execution of layer 56\n",
      "\n",
      "Executing module 57: layer4.1.conv2\n",
      "\tExecuting on machine 0\n",
      "\tExecuting on machine 1\n",
      "\tExecuting on machine 2\n",
      "\tExecuting on machine 3\n",
      "Finished execution of layer 57\n",
      "\n",
      "Executing module 58: layer4.1.bn1\n",
      "\tExecuting on machine 0\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 1\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 2\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 3\n",
      "\t\t-Applying ReLU\n",
      "Finished execution of layer 58\n",
      "\n",
      "Executing module 59: layer4.1.bn2\n",
      "\tExecuting on machine 0\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 1\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 2\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 3\n",
      "\t\t-Applying ReLU\n",
      "Finished execution of layer 59\n",
      "\n",
      "Executing module 60: layer4.1.shortcut\n",
      "\tExecuting on machine 0\n",
      "\t\t-Skipping module Sequential\n",
      "\tExecuting on machine 1\n",
      "\t\t-Skipping module Sequential\n",
      "\tExecuting on machine 2\n",
      "\t\t-Skipping module Sequential\n",
      "\tExecuting on machine 3\n",
      "\t\t-Skipping module Sequential\n",
      "Finished execution of layer 60\n",
      "\n",
      "Executing module 61: linear\n",
      "\tExecuting on machine 0\n",
      "\tExecuting on machine 1\n",
      "\tExecuting on machine 2\n",
      "\tExecuting on machine 3\n",
      "Finished execution of layer 61\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    mock run through inference using split models \n",
    "'''\n",
    "\n",
    "# TODO: reduce size of communicated tensors to only what is necessary \n",
    "# TODO: also check bias for nonzero\n",
    "# TODO: come up with more general scheme to handle residual layers\n",
    "\n",
    "# channel_id == INPUTS\n",
    "# filter_id  == OUTPUTS\n",
    "\n",
    "# setup input \n",
    "N_batch = 1000\n",
    "input_tensor = torch.rand(N_batch, 3, 32, 32, device=torch.device(configs['device'])) # 1k images, 3 channels, 32x32 image (cifar100) \n",
    "input_size = (N_batch, 3, 32, 32)\n",
    "\n",
    "# broadcast input_tensor to different machines\n",
    "input = [[]]*num_machines\n",
    "input = [input]*num_machines\n",
    "for imach in range(num_machines):\n",
    "    input[imach][0] = input_tensor\n",
    "\n",
    "# put models into eval mode and on device\n",
    "model.eval()\n",
    "model.to(configs['device'])\n",
    "for l in model_machines:\n",
    "    l.eval()\n",
    "    l.to(configs['device'])\n",
    "\n",
    "residual_input = {} # use this to keep track of inputs stored in machine memory for residule layers\n",
    "\n",
    "# make inference \n",
    "with torch.no_grad():\n",
    "        # iterate through layers 1 module at a time \n",
    "        for imodule in range(num_total_modules):\n",
    "\n",
    "                # initialize output for ilayer\n",
    "                output = [[]]*num_machines\n",
    "                output = [output]*num_machines # square list indexed as: output[destination/RX machine][origin/TX machine]\n",
    "                \n",
    "                send_module_outputs = True\n",
    "\n",
    "                print(f'Executing module {imodule}: {module_names[imodule]}')\n",
    "\n",
    "                # iterate through each machine (done in parallel later)\n",
    "                for imach in range(num_machines):\n",
    "                        print(f'\\tExecuting on machine {imach}')\n",
    "                        \n",
    "                        add_residual = False\n",
    "\n",
    "                        # get the current module\n",
    "                        # TODO: this is very bad for latency. Only load module if you have to \n",
    "                        curr_name, curr_module = next((x for i,x in enumerate(model.named_modules()) if i==imodule)) \n",
    "\n",
    "                        # update I/O if encounter split layer\n",
    "                        # TODO: revist this implementation\n",
    "                        split_param_name = curr_name + '.weight'\n",
    "                        if split_param_name in split_module_names:\n",
    "\n",
    "                                # skip if machine doesnt expect input\n",
    "                                if len(configs['partition'][split_param_name]['channel_id'][imach]) == 0:\n",
    "                                        print(f'\\t\\t-No input assigned to this machine. Skipping...')\n",
    "                                        continue\n",
    "                                \n",
    "                                # TODO: reconsider implementation \n",
    "                                # What input channels does this machine compute?\n",
    "                                input_channels = torch.tensor(configs['partition'][split_param_name]['channel_id'][imach],\n",
    "                                        device=torch.device(configs['device']))\n",
    "                                N_in = len(input_channels) # TODO: is this used?\n",
    "\n",
    "                                # Where to send output (map of output channels to different machines)\n",
    "                                output_channel_map = configs['partition'][split_param_name]['filter_id']\n",
    "\n",
    "                        # combine inputs from machines\n",
    "                        curr_input = False \n",
    "                        for i in range(num_machines):\n",
    "                                if not input[imach][i] == []:\n",
    "                                        if not torch.is_tensor(curr_input):\n",
    "                                                curr_input = input[imach][i] # initialize curr_input with first input tensor \n",
    "                                        else:\n",
    "                                                curr_input += input[imach][i]\n",
    "\n",
    "                        # skip this machine+module if there is no input to compute \n",
    "                        if not torch.is_tensor(curr_input):\n",
    "                                print('\\t\\t-No input sent to this machine. Skipping module')\n",
    "                                continue\n",
    "\n",
    "                        # reduce computation-- make vertically split layer \n",
    "                        # TODO: generalize this to more than conv layers \n",
    "                        if type(curr_module) == nn.Conv2d:\n",
    "                                split_layer = nn.Conv2d(N_in,\n",
    "                                                curr_module.weight.shape[0], # TODO does this need to be an int? (currently tensor)\n",
    "                                                kernel_size= curr_module.kernel_size,\n",
    "                                                stride=curr_module.stride,\n",
    "                                                padding=curr_module.padding, \n",
    "                                                bias=curr_module.bias)\n",
    "\n",
    "                                # write parameters to split layer \n",
    "                                split_layer.weight = torch.nn.Parameter(curr_module.weight.index_select(1, input_channels))\n",
    "\n",
    "                                # TODO: add support for splitting bias\n",
    "\n",
    "                                # handle if this is the start of the residual layer\n",
    "                                if 'shortcut' in curr_name:\n",
    "                                        residual_input[str(imach)]['block_out'] = curr_input\n",
    "                                        curr_input = residual_input[str(imach)]['block_in']\n",
    "\n",
    "                        elif type(curr_module) == nn.BatchNorm2d:\n",
    "                                split_layer = nn.BatchNorm2d(N_in, \n",
    "                                                curr_module.eps,\n",
    "                                                momentum=curr_module.momentum, \n",
    "                                                affine=curr_module.affine, \n",
    "                                                track_running_stats=curr_module.track_running_stats)\n",
    "\n",
    "                                # write parameters to split layer \n",
    "                                split_layer.weight = torch.nn.Parameter(curr_module.weight.index_select(0, input_channels))\n",
    "                                split_layer.running_mean = torch.nn.Parameter(curr_module.running_mean.index_select(0, input_channels))\n",
    "                                split_layer.running_var = torch.nn.Parameter(curr_module.running_var.index_select(0, input_channels))\n",
    "\n",
    "                                if not curr_module.bias == None:\n",
    "                                        split_layer.bias = torch.nn.Parameter(curr_module.bias.index_select(0, input_channels))\n",
    "                                \n",
    "                                \n",
    "                                if 'shortcut' in curr_name:\n",
    "                                        add_residual = True\n",
    "\n",
    "                                # TODO: revise implementation to only compute necessary C_in to C_out \n",
    "                                # assume mach-Cout map from previous conv layer can be used as inputs for this bn layer\n",
    "                                #input_channels = output_channel_map[imach]\n",
    "\n",
    "                        elif type(curr_module) == nn.Linear:\n",
    "                                # TODO: assumes there is a bias \n",
    "                                split_layer = nn.Linear(N_in, \n",
    "                                                curr_module.weight.shape[0])\n",
    "\n",
    "                                # write parameters to split layer \n",
    "                                split_layer.weight = torch.nn.Parameter(curr_module.weight.index_select(1, input_channels))\n",
    "\n",
    "                                # TODO: double check bias is applied correctly\n",
    "                                if not curr_module.bias == None:\n",
    "                                        split_layer.bias = curr_module.bias\n",
    "\n",
    "                                # prep for linear layer\n",
    "                                # TODO: assumes this always happens before linear layer \n",
    "                                # bn takes one in channel C_in_i and produces one out channel C_out_j. No communication is needed. \n",
    "                                curr_input = F.avg_pool2d(curr_input, 4)\n",
    "                                curr_input = curr_input.view(curr_input.size(0), -1)\n",
    "\n",
    "                        elif type(curr_module) == resnet.BasicBlock:\n",
    "                                # save input for later \n",
    "                                residual_input[str(imach)] = {}\n",
    "                                residual_input[str(imach)]['block_in'] = curr_input\n",
    "                                print('\\t\\t-Saving input for later...')\n",
    "                                send_module_outputs = False\n",
    "                                continue\n",
    "                        else:\n",
    "                                print(f'\\t\\t-Skipping module {type(curr_module).__name__}')\n",
    "                                send_module_outputs = False\n",
    "                                continue\n",
    "\n",
    "                        # look at which C_out need to be computed and sent\n",
    "                        nonzero_Cout = torch.unique(torch.nonzero(split_layer.weight, as_tuple=True)[0]) # find nonzero dimensions in output channels\n",
    "\n",
    "                        # eval and send to next machines \n",
    "                        #print(f'\\t\\t {curr_input.device}\\t\\t{input_channels.device}\\n\\t\\t{split_layer.weight.device}\\n\\t\\t{split_layer.bias}')\n",
    "                        if type(curr_module) == nn.BatchNorm2d:\n",
    "                                # TODO: lazy implementation. Every machine computes entire C_in to C_out but only need to compute C_in from previous layer\n",
    "                                out_tensor = curr_module(curr_input)\n",
    "                        else:\n",
    "                                out_tensor = split_layer(curr_input.index_select(1, input_channels))\n",
    "\n",
    "\n",
    "                        # check if this is residual layer\n",
    "                        if add_residual:\n",
    "                                print('\\t\\t-adding residual')\n",
    "                                out_tensor += residual_input[str(imach)]['block_out']\n",
    "\n",
    "                                # erase stored \n",
    "                                residual_input[str(imach)] = {}\n",
    "\n",
    "                        # apply ReLU after batch layers\n",
    "                        if imodule in relu_modules:\n",
    "                                print('\\t\\t-Applying ReLU')\n",
    "                                F.relu(out_tensor)\n",
    "\n",
    "                        # communicate\n",
    "                        for rx_mach in range(num_machines):\n",
    "                                # only add to output if communication is necessary \n",
    "\n",
    "                                # Where does this machine send outputs? TODO: consider removing, this just maps C_out's to machine\n",
    "                                output_channels = torch.tensor(output_channel_map[rx_mach],\n",
    "                                        device=torch.device(configs['device']))\n",
    "\n",
    "                                # TODO: is there a faster way to do this? Consider putting larger array 1st... just not sure which one that'd be\n",
    "                                is_comms = torch.any(torch.isin(nonzero_Cout, output_channels))\n",
    "                                if is_comms.item():\n",
    "                                        output[rx_mach][imach] = out_tensor\n",
    "\n",
    "                # send to next layer  \n",
    "                if send_module_outputs:      \n",
    "                        input = output\n",
    "                print(f'Finished execution of layer {imodule}')\n",
    "                print()\n",
    "\n",
    "# collect outputs -- assumes ends with Linear layer. Not sure how generalizable this is\n",
    "need_to_init  = True\n",
    "for tx_mach in range(num_machines):\n",
    "        for rx_mach in range(num_machines):\n",
    "                if not input[imach][i] == []:\n",
    "                        if need_to_init:\n",
    "                                final_output = output[rx_mach][tx_mach]\n",
    "                        else:\n",
    "                                final_output += output[rx_mach][tx_mach]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches: 0/1000\n",
      "histogram tensor([  -26.5522,  -910.8924,   328.6273,  -977.7976,   448.9310, -1293.6526,\n",
      "         3071.8613,  -943.0629,  -225.1325, -1041.1338])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Compare vertical+horizontal split model with full model\n",
    "'''\n",
    "\n",
    "# TODO: finish\n",
    "match_count = (torch.argmax(final_output, axis=1) == torch.argmax(output_full, axis=1)).sum().item()\n",
    "label_hist = output_full.sum(0)\n",
    "print(f'Matches: {match_count}/{output_full.size(0)}')\n",
    "print(f'histogram {label_hist}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input channels tensor([0], device='cuda:0', dtype=torch.int32)\n",
      "Output channels tensor([16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31],\n",
      "       device='cuda:0', dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Test partial execution of a single layer\n",
    "'''\n",
    "\n",
    "imach = 1\n",
    "ilayer= 0\n",
    "input_channels = torch.tensor(configs['partition'][layer_names[ilayer]]['channel_id'][imach],\n",
    "                              device=torch.device(configs['device']))\n",
    "output_channels = torch.tensor(configs['partition'][layer_names[ilayer]]['filter_id'][imach],\n",
    "                               device=torch.device(configs['device']))\n",
    "print(f'Input channels {input_channels}')\n",
    "print(f'Output channels {output_channels}')\n",
    "\n",
    "# TODO: generalize this to more than conv layers \n",
    "a_layer = model.conv1\n",
    "split_layer = nn.Conv2d(len(input_channels),\n",
    "                    len(output_channels),\n",
    "                    kernel_size= a_layer.kernel_size,\n",
    "                    stride=a_layer.stride,\n",
    "                    padding=a_layer.padding, \n",
    "                    bias=a_layer.bias)\n",
    "\n",
    "split_layer.parameters = a_layer.weight.index_select(0, output_channels).index_select(1, input_channels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - 0\n",
      "1 - 1\n",
      "2 - 2\n",
      "3 - 3\n",
      "4 - 4\n",
      "5 - 5\n",
      "6 - 6\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    iterate using children method?\n",
    "'''\n",
    "\n",
    "# creates nested structure for each layer/block \n",
    "i = 0\n",
    "for name,module in enumerate( model_machines[imach].children()):\n",
    "    print(f'{i} - {name}')\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - name: ; type: ResNet\n",
      "1 - name: conv1; type: Conv2d\n",
      "2 - name: bn1; type: BatchNorm1d\n",
      "3 - name: layer1; type: Sequential\n",
      "4 - name: layer1.0; type: BasicBlock\n",
      "5 - name: layer1.0.conv1; type: Conv2d\n",
      "6 - name: layer1.0.conv2; type: Conv2d\n",
      "7 - name: layer1.0.bn1; type: BatchNorm1d\n",
      "8 - name: layer1.0.bn2; type: BatchNorm1d\n",
      "9 - name: layer1.0.shortcut; type: Sequential\n",
      "10 - name: layer1.1; type: BasicBlock\n",
      "11 - name: layer1.1.conv1; type: Conv2d\n",
      "12 - name: layer1.1.conv2; type: Conv2d\n",
      "13 - name: layer1.1.bn1; type: BatchNorm1d\n",
      "14 - name: layer1.1.bn2; type: BatchNorm1d\n",
      "15 - name: layer1.1.shortcut; type: Sequential\n",
      "16 - name: layer2; type: Sequential\n",
      "17 - name: layer2.0; type: BasicBlock\n",
      "18 - name: layer2.0.conv1; type: Conv2d\n",
      "19 - name: layer2.0.conv2; type: Conv2d\n",
      "20 - name: layer2.0.bn1; type: BatchNorm1d\n",
      "21 - name: layer2.0.bn2; type: BatchNorm1d\n",
      "22 - name: layer2.0.shortcut; type: Sequential\n",
      "23 - name: layer2.0.shortcut.0; type: Conv2d\n",
      "24 - name: layer2.0.shortcut.1; type: BatchNorm1d\n",
      "25 - name: layer2.1; type: BasicBlock\n",
      "26 - name: layer2.1.conv1; type: Conv2d\n",
      "27 - name: layer2.1.conv2; type: Conv2d\n",
      "28 - name: layer2.1.bn1; type: BatchNorm1d\n",
      "29 - name: layer2.1.bn2; type: BatchNorm1d\n",
      "30 - name: layer2.1.shortcut; type: Sequential\n",
      "31 - name: layer3; type: Sequential\n",
      "32 - name: layer3.0; type: BasicBlock\n",
      "33 - name: layer3.0.conv1; type: Conv2d\n",
      "34 - name: layer3.0.conv2; type: Conv2d\n",
      "35 - name: layer3.0.bn1; type: BatchNorm1d\n",
      "36 - name: layer3.0.bn2; type: BatchNorm1d\n",
      "37 - name: layer3.0.shortcut; type: Sequential\n",
      "38 - name: layer3.0.shortcut.0; type: Conv2d\n",
      "39 - name: layer3.0.shortcut.1; type: BatchNorm1d\n",
      "40 - name: layer3.1; type: BasicBlock\n",
      "41 - name: layer3.1.conv1; type: Conv2d\n",
      "42 - name: layer3.1.conv2; type: Conv2d\n",
      "43 - name: layer3.1.bn1; type: BatchNorm1d\n",
      "44 - name: layer3.1.bn2; type: BatchNorm1d\n",
      "45 - name: layer3.1.shortcut; type: Sequential\n",
      "46 - name: layer4; type: Sequential\n",
      "47 - name: layer4.0; type: BasicBlock\n",
      "48 - name: layer4.0.conv1; type: Conv2d\n",
      "49 - name: layer4.0.conv2; type: Conv2d\n",
      "50 - name: layer4.0.bn1; type: BatchNorm1d\n",
      "51 - name: layer4.0.bn2; type: BatchNorm1d\n",
      "52 - name: layer4.0.shortcut; type: Sequential\n",
      "53 - name: layer4.0.shortcut.0; type: Conv2d\n",
      "54 - name: layer4.0.shortcut.1; type: BatchNorm1d\n",
      "55 - name: layer4.1; type: BasicBlock\n",
      "56 - name: layer4.1.conv1; type: Conv2d\n",
      "57 - name: layer4.1.conv2; type: Conv2d\n",
      "58 - name: layer4.1.bn1; type: BatchNorm1d\n",
      "59 - name: layer4.1.bn2; type: BatchNorm1d\n",
      "60 - name: layer4.1.shortcut; type: Sequential\n",
      "61 - name: linear; type: Linear\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "    iterate using named_modules?\n",
    "'''\n",
    "\n",
    "# - gets redundant structure\n",
    "# - I think the easiest way to iterate through the whole model is using this list and skipping large layers/blocks of the model and only executing conv/bn/linear etc. \"layers\"\n",
    "# - provides good indicator of when to save input for skipped layer \n",
    "\n",
    "# not sure how to handle sequential layers... going to poke at this here \n",
    "i = 0\n",
    "for name, module in model.named_modules():\n",
    "    print(f'{i} - name: {name}; type: {type(module).__name__}')\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('',\n",
       " ResNet(\n",
       "   (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "   (bn1): BatchNorm1d(64, eps=4, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   (layer1): Sequential(\n",
       "     (0): BasicBlock(\n",
       "       (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "       (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "       (bn1): BatchNorm1d(64, eps=4, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       (bn2): BatchNorm1d(64, eps=4, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       (shortcut): Sequential()\n",
       "     )\n",
       "     (1): BasicBlock(\n",
       "       (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "       (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "       (bn1): BatchNorm1d(64, eps=4, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       (bn2): BatchNorm1d(64, eps=4, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       (shortcut): Sequential()\n",
       "     )\n",
       "   )\n",
       "   (layer2): Sequential(\n",
       "     (0): BasicBlock(\n",
       "       (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "       (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "       (bn1): BatchNorm1d(128, eps=4, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       (bn2): BatchNorm1d(128, eps=4, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       (shortcut): Sequential(\n",
       "         (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "         (1): BatchNorm1d(128, eps=4, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       )\n",
       "     )\n",
       "     (1): BasicBlock(\n",
       "       (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "       (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "       (bn1): BatchNorm1d(128, eps=4, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       (bn2): BatchNorm1d(128, eps=4, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       (shortcut): Sequential()\n",
       "     )\n",
       "   )\n",
       "   (layer3): Sequential(\n",
       "     (0): BasicBlock(\n",
       "       (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "       (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "       (bn1): BatchNorm1d(256, eps=4, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       (bn2): BatchNorm1d(256, eps=4, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       (shortcut): Sequential(\n",
       "         (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "         (1): BatchNorm1d(256, eps=4, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       )\n",
       "     )\n",
       "     (1): BasicBlock(\n",
       "       (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "       (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "       (bn1): BatchNorm1d(256, eps=4, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       (bn2): BatchNorm1d(256, eps=4, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       (shortcut): Sequential()\n",
       "     )\n",
       "   )\n",
       "   (layer4): Sequential(\n",
       "     (0): BasicBlock(\n",
       "       (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "       (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "       (bn1): BatchNorm1d(512, eps=4, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       (bn2): BatchNorm1d(512, eps=4, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       (shortcut): Sequential(\n",
       "         (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "         (1): BatchNorm1d(512, eps=4, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       )\n",
       "     )\n",
       "     (1): BasicBlock(\n",
       "       (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "       (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "       (bn1): BatchNorm1d(512, eps=4, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       (bn2): BatchNorm1d(512, eps=4, momentum=0.1, affine=True, track_running_stats=True)\n",
       "       (shortcut): Sequential()\n",
       "     )\n",
       "   )\n",
       "   (linear): Linear(in_features=512, out_features=10, bias=True)\n",
       " ))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    Look at one \"layer\"\n",
    "'''\n",
    "\n",
    "ilayer = 0\n",
    "a_module = next((m for i, m in enumerate(model.named_modules()) if i==ilayer))\n",
    "\n",
    "a_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1.1921e-07, 0.0000e+00, 0.0000e+00],\n",
       "          [1.1921e-07, 0.0000e+00, 0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00]]]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    can you split certain operations across machines?\n",
    "    - I think the math says yes but there is some error values a on the order of 1e-7 \n",
    "    - could this be due to non-deterministic tensor operations? or is my math wrong?\n",
    "'''\n",
    "\n",
    "t = torch.rand(1, 3, 10,10)\n",
    "all = torch.sum(t, 1, True)\n",
    "\n",
    "\n",
    "kernel_size = 3\n",
    "\n",
    "full_avg = F.avg_pool2d(all, kernel_size)\n",
    "par_avg = torch.zeros(full_avg.shape)\n",
    "\n",
    "for i in range(t.shape[1]):\n",
    "    par_avg += F.avg_pool2d(t.index_select(1, torch.tensor(i)), kernel_size)\n",
    "\n",
    "torch.abs(full_avg - par_avg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cap_nb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
