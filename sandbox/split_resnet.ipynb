{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    Load RESNET model and split it\\n        1. layer by layer\\n        2. [TODO] vertically \\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    Load RESNET model and split it\n",
    "        1. layer by layer\n",
    "        2. [TODO] vertically \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\natet\\anaconda3\\envs\\cap_nb\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from source.core.engine import MoP\n",
    "from source.core import run_partition\n",
    "from os import environ\n",
    "from source.utils.dataset import *\n",
    "from source.utils.misc import *\n",
    "from source.utils.split_network import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "from source.models import resnet\n",
    "from source.core.split_manager import SplitManager\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from source.utils import io\n",
    "from source.utils import testers\n",
    "from source.core import engine\n",
    "\n",
    "import json\n",
    "import itertools\n",
    "\n",
    "from torchvision.models.feature_extraction import create_feature_extractor, get_graph_node_names\n",
    "from torchsummary import summary\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device :  \n",
      "model :  resnet18\n",
      "data_code :  cifar10\n",
      "num_classes :  10\n",
      "model_file :  test.pt\n",
      "epochs :  0\n",
      "batch_size :  128\n",
      "optimizer :  sgd\n",
      "lr_scheduler :  default\n",
      "learning_rate :  0.01\n",
      "seed :  1234\n",
      "sparsity_type :  kernel\n",
      "prune_ratio :  1\n",
      "admm :  True\n",
      "admm_epochs :  300\n",
      "rho :  0.0001\n",
      "multi_rho :  True\n",
      "retrain_bs :  128\n",
      "retrain_lr :  0.005\n",
      "retrain_ep :  50\n",
      "retrain_opt :  default\n",
      "xentropy_weight :  1.0\n",
      "warmup :  False\n",
      "warmup_lr :  0.001\n",
      "warmup_epochs :  10\n",
      "mix_up :  True\n",
      "alpha :  0.3\n",
      "smooth :  False\n",
      "smooth_eps :  0\n",
      "save_last_model_only :  False\n",
      "num_partition :  1\n",
      "layer_type :  regular\n",
      "bn_type :  masked\n",
      "par_first_layer :  False\n",
      "comm_outsize :  False\n",
      "lambda_comm :  0\n",
      "lambda_comp :  0\n",
      "distill_model :  \n",
      "distill_loss :  kl\n",
      "distill_temp :  30\n",
      "distill_alpha :  1\n"
     ]
    }
   ],
   "source": [
    "# setup config\n",
    "dataset='cifar10'\n",
    "environ[\"config\"] = f\"config/{dataset}.yaml\"\n",
    "\n",
    "configs = run_p.main()\n",
    "\n",
    "configs[\"device\"] = \"cpu\"\n",
    "configs['load_model'] = \"cifar10-resnet18-kernel-npv0-pr0.75-lcm0.001.pt\"\n",
    "configs[\"num_partition\"] = '4' #'resnet18-v2.yaml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (linear): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# load data and load or train model\n",
    "model = get_model_from_code(configs).to(configs['device']) # grabs model architecture from ./source/models/escnet.py\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load weights into full model\n",
    "state_dict = torch.load(io.get_model_path(\"{}\".format(configs[\"load_model\"])), map_location=configs['device'])\n",
    "model = io.load_state_dict(model, \n",
    "                    state_dict['model_state_dict'] if 'model_state_dict' in state_dict \n",
    "                    else state_dict['state_dict'] if 'state_dict' in state_dict else state_dict,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference time per data is 32.004833ms.\n",
      "conv1.weight 1024\n",
      "layer1.0.conv1.weight 1024\n",
      "layer1.0.conv2.weight 1024\n",
      "layer1.1.conv1.weight 1024\n",
      "layer1.1.conv2.weight 1024\n",
      "layer2.0.conv1.weight 256\n",
      "layer2.0.conv2.weight 256\n",
      "layer2.0.shortcut.0.weight 256\n",
      "layer2.1.conv1.weight 256\n",
      "layer2.1.conv2.weight 256\n",
      "layer3.0.conv1.weight 64\n",
      "layer3.0.conv2.weight 64\n",
      "layer3.0.shortcut.0.weight 64\n",
      "layer3.1.conv1.weight 64\n",
      "layer3.1.conv2.weight 64\n",
      "layer4.0.conv1.weight 16\n",
      "layer4.0.conv2.weight 16\n",
      "layer4.0.shortcut.0.weight 16\n",
      "layer4.1.conv1.weight 16\n",
      "layer4.1.conv2.weight 16\n",
      "['x', 'conv1', 'bn1', 'relu', 'layer1.0.conv1', 'layer1.0.bn1', 'layer1.0.relu', 'layer1.0.conv2', 'layer1.0.bn2', 'layer1.0.add', 'layer1.0.relu_1', 'layer1.1.conv1', 'layer1.1.bn1', 'layer1.1.relu', 'layer1.1.conv2', 'layer1.1.bn2', 'layer1.1.add', 'layer1.1.relu_1', 'layer2.0.conv1', 'layer2.0.bn1', 'layer2.0.relu', 'layer2.0.conv2', 'layer2.0.bn2', 'layer2.0.shortcut.0', 'layer2.0.shortcut.1', 'layer2.0.add', 'layer2.0.relu_1', 'layer2.1.conv1', 'layer2.1.bn1', 'layer2.1.relu', 'layer2.1.conv2', 'layer2.1.bn2', 'layer2.1.add', 'layer2.1.relu_1', 'layer3.0.conv1', 'layer3.0.bn1', 'layer3.0.relu', 'layer3.0.conv2', 'layer3.0.bn2', 'layer3.0.shortcut.0', 'layer3.0.shortcut.1', 'layer3.0.add', 'layer3.0.relu_1', 'layer3.1.conv1', 'layer3.1.bn1', 'layer3.1.relu', 'layer3.1.conv2', 'layer3.1.bn2', 'layer3.1.add', 'layer3.1.relu_1', 'layer4.0.conv1', 'layer4.0.bn1', 'layer4.0.relu', 'layer4.0.conv2', 'layer4.0.bn2', 'layer4.0.shortcut.0', 'layer4.0.shortcut.1', 'layer4.0.add', 'layer4.0.relu_1', 'layer4.1.conv1', 'layer4.1.bn1', 'layer4.1.relu', 'layer4.1.conv2', 'layer4.1.bn2', 'layer4.1.add', 'layer4.1.relu_1', 'avg_pool2d', 'size', 'view', 'linear']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    add partitions and communications to configs\n",
    "'''\n",
    "\n",
    "# gets random test input (with correct size)\n",
    "input_var = engine.get_input_from_code(configs)\n",
    "#print(input_var)\n",
    "\n",
    "# Config partitions and prune_ratio\n",
    "configs = engine.partition_generator(configs, model)\n",
    "            \n",
    "# Compute output size of each layer\n",
    "configs['partition'] = engine.featuremap_summary(model, configs['partition'], input_var)\n",
    "\n",
    "# split model general parameters\n",
    "\n",
    "# make copies of model per machine\n",
    "num_machines = max(configs['partition']['bn_partition']) # TODO: double check this makes sense\n",
    "model_machines = [model]*num_machines\n",
    "\n",
    "layer_names_fx =  get_graph_node_names(model)[1]\n",
    "total_layers_fx = len(layer_names_fx)\n",
    "\n",
    "split_module_names = list(configs['partition'].keys())\n",
    "\n",
    "print(layer_names_fx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "    IO helper\n",
    "'''\n",
    "\n",
    "def prep_data(tensor, node, layer):\n",
    "    '''\n",
    "        Prepare input tensors to model \n",
    "    '''\n",
    "    return {\n",
    "        'node' : node,\n",
    "        'layer' : layer,\n",
    "        'tensor' : tensor, \n",
    "        'Cin' : list(get_nonzero_channels(tensor))\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SplitManager' object has no attribute 'device'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 24\u001b[0m\n\u001b[0;32m     22\u001b[0m split_managers \u001b[38;5;241m=\u001b[39m [SplitManager]\u001b[38;5;241m*\u001b[39mnum_machines\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_machines):\n\u001b[1;32m---> 24\u001b[0m     split_managers[i] \u001b[38;5;241m=\u001b[39m \u001b[43mSplitManager\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfigs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_machines\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# broadcast input_tensor to different machines\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# TODO: find a better datastructure for this\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m#input = np.empty((num_machines, num_machines), dtype=torch.Tensor)\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\natet\\Desktop\\graduate school\\thesis\\CaP\\source\\core\\split_manager.py:39\u001b[0m, in \u001b[0;36mSplitManager.__init__\u001b[1;34m(self, configs, machine, N_machines, input_tensor, debug)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWarning found no dtype field in config\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_channel_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# load model TODO: only load what is necessary for this thread\u001b[39;00m\n\u001b[0;32m     42\u001b[0m model \u001b[38;5;241m=\u001b[39m get_model_from_code(configs)\u001b[38;5;241m.\u001b[39mto(configs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;66;03m# grabs model architecture from ./source/models/escnet.py\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'SplitManager' object has no attribute 'device'"
     ]
    }
   ],
   "source": [
    "''' Prep Inout for Mock Run version 2 (fx)'''\n",
    "\n",
    "# TODO: reduce size of communicated tensors to only what is necessary \n",
    "# TODO: also check bias for nonzero\n",
    "# TODO: come up with more general scheme to handle residual layers\n",
    "\n",
    "# channel_id == INPUTS\n",
    "# filter_id  == OUTPUTS\n",
    "\n",
    "# try greater precision\n",
    "#torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "#torch.set_default_dtype(torch.float64)\n",
    "\n",
    "# setup input \n",
    "N_batch = 1\n",
    "torch.manual_seed(0)\n",
    "input_tensor = torch.rand(N_batch, 3, 32, 32, dtype=torch.float64, device=torch.device(configs['device'])) # 1k images, 3 channels, 32x32 image (cifar100) \n",
    "model = model.type(torch.float64)\n",
    "\n",
    "# make SplitManagers for split model execution \n",
    "configs['dtype'] = 'float64'\n",
    "split_managers = [SplitManager]*num_machines\n",
    "for i in range(num_machines):\n",
    "    split_managers[i] = SplitManager(configs, i, num_machines, input_tensor)\n",
    "\n",
    "# broadcast input_tensor to different machines\n",
    "# TODO: find a better datastructure for this\n",
    "#input = np.empty((num_machines, num_machines), dtype=torch.Tensor)\n",
    "input = []\n",
    "for imach in range(num_machines):\n",
    "    in_data = prep_data(input_tensor, -1, 0)\n",
    "    input.append([in_data])\n",
    "\n",
    "residual_block_start, residual_connection_start, residual_block_end = get_residual_block_indexes(model)\n",
    "\n",
    "# put models into eval mode and on device\n",
    "model.eval()\n",
    "model.to(configs['device'])\n",
    "\n",
    "# set index to execute vertical splitting up to \n",
    "exec_to_layer = 69\n",
    "\n",
    "# get true output at each layer\n",
    "# WARNING: bn layers are initialized differently between model in this notebook and model in split manager. The moving mean and variance fields do not match HOWEVER the weights and biases are the same\n",
    "#horz_output, size_LUT = get_output_at_each_layer(model, input_tensor) \n",
    "horz_output, size_LUT = get_output_at_each_layer(split_managers[0].model, input_tensor)\n",
    "\n",
    "BREAK_LOOP = 0 # break loop when output differs\n",
    "\n",
    "'''\n",
    "    mock run through inference using split models \n",
    "'''\n",
    "\n",
    "# timing\n",
    "split_execution_start_time = time.time()\n",
    "layer_completion_time_stamp = {}\n",
    "layer_execution_duration = {}\n",
    "\n",
    "# make inference \n",
    "with torch.no_grad():\n",
    "    residual_input = {} # use this to keep track of inputs stored in machine memory for residule layers\n",
    "\n",
    "    # iterate through layers 1 module at a time \n",
    "    for imodule in range(exec_to_layer+1):#range(num_total_modules): # 16 <=> layer_1 block \n",
    "\n",
    "        # initialize output for ilayer\n",
    "        #output = np.empty((num_machines, num_machines), dtype=torch.Tensor) # square list indexed as: output[destination/RX machine][origin/TX machine]\n",
    "        # TODO: find a better datastructure for this \n",
    "\n",
    "        output = [None]*num_machines\n",
    "        output = [output[:] for i in range(num_machines)]\n",
    "\n",
    "        # DEBUG\n",
    "        full_input = combine_all_inputs(input, num_machines)\n",
    "\n",
    "        print(f'Executing module {imodule}: {layer_names_fx[imodule]}')\n",
    "\n",
    "        # iterate through each machine (done in parallel later)\n",
    "        for imach in range(num_machines):\n",
    "            print(f'\\tExecuting on machine {imach}')\n",
    "\n",
    "            # collect communication inputs if necessary \n",
    "            if not imodule == 0 and ('conv' in layer_names_fx[imodule-1] or 'linear' in layer_names_fx[imodule-1] or 'shortcut.1' in layer_names_fx[imodule]): # TODO: this is very hacky, needs to be generalized. The issue is ID'ing conv layers in shortcut blocks\n",
    "                curr_input = combine_inputs(input, num_machines, imach)\n",
    "            else:\n",
    "                curr_input = input[imach][imach]\n",
    "\n",
    "            out_tensor, do_comms = split_managers[imach].execute_split_layer(curr_input, imodule)\n",
    "            if not do_comms:\n",
    "                # update output to current machine and continue\n",
    "                if torch.is_tensor(out_tensor):\n",
    "                    # sometimes out_tensor is None\n",
    "                    # input is sent to all machines for 1st layer execution even though not all machines need to compute \n",
    "                    # Output from machine is None in this case TODO: fix where inputs are sent \n",
    "                    output[imach][imach] = out_tensor\n",
    "                continue\n",
    "\n",
    "            # END SplitManager execute split_layer\n",
    "\n",
    "            print(f'\\t\\t Output tensor shape : {out_tensor.shape}')\n",
    "\n",
    "            # debug\n",
    "            nonzero_out_tensor = torch.unique(torch.nonzero(out_tensor, as_tuple=True)[1])\n",
    "\n",
    "            # look at which C_out need to be computed and sent\n",
    "            #nonzero_Cout = torch.unique(torch.nonzero(split_layer.weight, as_tuple=True)[0]) # find nonzero dimensions in output channels\n",
    "            nonzero_Cout = get_nonzero_channels(out_tensor)\n",
    "\n",
    "            # prep communications by populating output\n",
    "            out_channel_array = torch.arange(out_tensor.shape[1])\n",
    "            for rx_mach in range(num_machines):\n",
    "                # only add to output if communication is necessary \n",
    "\n",
    "                # Get output channels for current rx machine? TODO: consider removing, this just maps C_out's to machine\n",
    "                #output_channels = torch.tensor(configs['partition'][][rx_mach],\n",
    "                #        device=torch.device(configs['device']))\n",
    "                output_channels = torch.tensor(split_managers[imach].output_channel_map[rx_mach],\n",
    "                        device=torch.device(configs['device']))\n",
    "\n",
    "                # TODO: is there a faster way to do this? Consider putting larger array 1st... just not sure which one that'd be\n",
    "                nonzero_out_channels = nonzero_Cout[torch.isin(nonzero_Cout, output_channels)]\n",
    "                if nonzero_out_channels.nelement() > 0:\n",
    "                        communication_mask = torch.isin(out_channel_array, nonzero_out_channels)\n",
    "\n",
    "                        # TODO: this is inefficient, redo. Probbably need to send a tensor and some info what output channels are being sent\n",
    "                        tmp_out = torch.zeros(out_tensor.shape, dtype= split_managers[imach].dtype) \n",
    "                        if imodule == total_layers_fx-1:\n",
    "                                tmp_out[:,communication_mask] = out_tensor[:,communication_mask]\n",
    "                        else:\n",
    "                                tmp_out[:,communication_mask,:,:] = out_tensor[:,communication_mask,:,:]\n",
    "                        output[rx_mach][imach] = tmp_out\n",
    "\n",
    "                        # debug\n",
    "                        print(f'\\t\\t sending C_out {nonzero_out_channels} to machine {rx_mach}')\n",
    "\n",
    "        # send to next layer  \n",
    "        input = output\n",
    "        print(f'Finished execution of layer {imodule}')\n",
    "\n",
    "        # update timing\n",
    "        layer_completion_time_stamp[layer_names_fx[imodule]] = time.time()\n",
    "        if imodule > 0:\n",
    "            layer_execution_duration[layer_names_fx[imodule]] = layer_completion_time_stamp[layer_names_fx[imodule]] - layer_completion_time_stamp[layer_names_fx[imodule-1]] \n",
    "        else:\n",
    "            layer_execution_duration[layer_names_fx[imodule]] = layer_completion_time_stamp[layer_names_fx[imodule]] - split_execution_start_time\n",
    "\n",
    "        # check output at end of each layer to see if fit matches \n",
    "        tmp_output = input \n",
    "        need_to_init  = True\n",
    "        for rx_mach in range(num_machines):\n",
    "                for tx_mach in range(num_machines):\n",
    "                        if not tmp_output[rx_mach][tx_mach] == None:\n",
    "                                if need_to_init:\n",
    "                                        vert_output = tmp_output[rx_mach][tx_mach]\n",
    "                                        need_to_init = False\n",
    "                                else:\n",
    "                                        # TODO: += causes assignment issues, switched to x = x+y which might be more more inefficent memory wise ... \n",
    "                                        vert_output = vert_output + tmp_output[rx_mach][tx_mach] \n",
    "                                        #nz_channels = get_nonzero_channels(vert_output)\n",
    "                                        #print(f'({rx_mach},{tx_mach}) {nz_channels}')\n",
    "        \n",
    "        if imodule == total_layers_fx-1:\n",
    "            # apply bias\n",
    "            # TODO: assumes Linear layer is final layer and bias can be handled as final step \n",
    "            vert_output = vert_output + get_current_module(model, imodule).bias\n",
    "            \n",
    "            # final execution time\n",
    "            tot_split_execution_time = time.time() - split_execution_start_time\n",
    "            print(f'\\n\\n############################# FINAL EXECUTION TIME {tot_split_execution_time} [seconds] #############################\\n\\n')\n",
    "\n",
    "        truth_output = horz_output[layer_names_fx[imodule]]\n",
    "        if 'x' == layer_names_fx[imodule]:\n",
    "            print(f'Input layer. Skipping comparison')\n",
    "        elif torch.is_tensor(truth_output):\n",
    "            max_diff, max_by_Cout = compare_outputs(vert_output, truth_output)\n",
    "            if max_diff > 0.1:\n",
    "                BREAK_LOOP = 1 \n",
    "        else:\n",
    "            print(f'Horizontal output is {type(truth_output)}. Skipping comparison')\n",
    "        print()\n",
    "\n",
    "        if BREAK_LOOP:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1102e-16, dtype=torch.float64, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Conv1 layer test -- should we expect some difference, or exactly 0 in the output when we split the model?\n",
    "'''\n",
    "\n",
    "# DIFFERENCE SHOULD BE 0 NOT 1E-7\n",
    "\n",
    "N_in = 1\n",
    "split_1 = nn.Conv2d(N_in,\n",
    "            model.conv1.weight.shape[0], # TODO does this need to be an int? (currently tensor)\n",
    "            kernel_size= model.conv1.kernel_size,\n",
    "            stride=model.conv1.stride,\n",
    "            padding=model.conv1.padding, \n",
    "            bias=False) # TODO: add bias during input collecting step on next layer \n",
    "split_1.weight = torch.nn.Parameter(model.conv1.weight.index_select(1, torch.tensor([0])))  \n",
    "out_split1 = split_1(input_tensor.index_select(1, torch.tensor([0])))\n",
    "\n",
    "split_2 = split_1\n",
    "split_2.weight = torch.nn.Parameter(model.conv1.weight.index_select(1, torch.tensor([1])))  \n",
    "out_split2 = split_2(input_tensor.index_select(1, torch.tensor([1])))\n",
    "\n",
    "split_3 = split_1\n",
    "split_3.weight = torch.nn.Parameter(model.conv1.weight.index_select(1, torch.tensor([2])))  \n",
    "out_split3 = split_3(input_tensor.index_select(1, torch.tensor([2])))\n",
    "\n",
    "split_out = torch.add(torch.add(out_split1, out_split2), out_split3)\n",
    "full_out = model.conv1(input_tensor)\n",
    "\n",
    "diff_output = torch.abs(full_out - split_out)\n",
    "max_diff = torch.max(diff_output)\n",
    "max_diff.sci_mode = True\n",
    "print(max_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([], size=(0, 4), dtype=torch.int64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''  \n",
    "    Inspect I/O of single layer\n",
    "'''\n",
    "\n",
    "t = torch.ones((1,2,2,2), dtype=torch.float32) # (batch, in channel, H, W)\n",
    "w = torch.ones((1,2,2,2), dtype=torch.float32) # (out channels, in channels, H, W)\n",
    "w[0,0,0,0] = 1e-10\n",
    "w[0,0,0,1] = 1e-10\n",
    "w[0,1,0,0] = 1e-10\n",
    "\n",
    "full_conv = torch.nn.Conv2d(2,1,kernel_size=(2,2), bias=False, stride=(1),dtype=torch.float32)\n",
    "full_conv.weight = torch.nn.Parameter(w)\n",
    "conv1 = torch.nn.Conv2d(1,1,kernel_size=(2,2), bias=False, stride=1, dtype=torch.float32)\n",
    "conv1.weight =torch.nn.Parameter( w[:,0:1,:,:])\n",
    "conv2 = torch.nn.Conv2d(1,1,kernel_size=(2,2), bias=False, stride=1,dtype=torch.float32)\n",
    "conv2.weight = torch.nn.Parameter(w[:,1:2,:,:])\n",
    "\n",
    "full_conv.eval()\n",
    "conv1.eval()\n",
    "conv2.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    full_out = full_conv(t)\n",
    "    split_out =  conv2(t[0,1:2,:,:]) + conv1(t[:,0:1,:,:])\n",
    "\n",
    "diff = torch.abs(full_out - split_out)\n",
    "\n",
    "torch.nonzero(diff)\n",
    "#print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test bn1\n",
    "#estimate = -bn1.running_mean[1]/torch.sqrt(bn1.running_var[1] + bn1.eps)*bn1.weight[1] + bn1.bias[1]\n",
    "#estimate_split = -split_layer.running_mean[1]/torch.sqrt(split_layer.running_var[1] + split_layer.eps)*split_layer.weight[1] + split_layer.bias[1]\n",
    "\n",
    "# running estimates are different \n",
    "#bn1.running_mean[1] - split_layer.running_mean[1]\n",
    "#bn1.running_var[1] - split_layer.running_var[1] \n",
    "#bn1.weight[1] - split_layer.weight[1]\n",
    "#bn1.eps - split_layer.eps\n",
    "#bn1.bias[1] - split_layer.bias[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cap_nb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
