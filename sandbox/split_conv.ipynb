{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    Load model and split it\\n        1. layer by layer\\n        2. [TODO] vertically \\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    Load model and split it\n",
    "        1. layer by layer\n",
    "        2. [TODO] vertically \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from source.core.engine import MoP\n",
    "import source.core.run_partition as run_p\n",
    "from os import environ\n",
    "from source.utils.dataset import *\n",
    "from source.utils.misc import *\n",
    "from source.utils.split_network import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "from source.models import resnet\n",
    "from source.core.split_manager import SplitManager\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from source.utils import io\n",
    "from source.utils import testers\n",
    "from source.core import engine\n",
    "import json\n",
    "import itertools\n",
    "\n",
    "from torchvision.models.feature_extraction import create_feature_extractor, get_graph_node_names\n",
    "from torchsummary import summary\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model\n",
    "\n",
    "# model = 'resnet18'\n",
    "# dataset = 'cifar10'\n",
    "\n",
    "# model = 'wrn28_10'\n",
    "dataset = 'cifar100'\n",
    "\n",
    "# model = 'EscFusion'\n",
    "# dataset = 'esc'\n",
    "\n",
    "# model = 'InfoFusionThree'\n",
    "# dataset = 'flash'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# yaml_version = 'v0'\n",
    "# num_partitions = 4\n",
    "# prune_ratio = 0.75\n",
    "\n",
    "# yaml_version = 'v1'\n",
    "# yaml_version = 'v2'\n",
    "# yaml_version = 'v3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device :  cuda:2\n",
      "model :  wrn28_10\n",
      "data_code :  cifar100\n",
      "num_classes :  100\n",
      "epochs :  1\n",
      "batch_size :  128\n",
      "optimizer :  sgd\n",
      "lr_scheduler :  default\n",
      "learning_rate :  0.01\n",
      "seed :  1234\n",
      "sparsity_type :  kernel\n",
      "prune_ratio :  0.75\n",
      "admm :  True\n",
      "admm_epochs :  300\n",
      "rho :  0.0001\n",
      "multi_rho :  True\n",
      "retrain_bs :  128\n",
      "retrain_lr :  0.001\n",
      "retrain_ep :  1\n",
      "retrain_opt :  default\n",
      "xentropy_weight :  1.0\n",
      "warmup :  False\n",
      "warmup_lr :  0.01\n",
      "warmup_epochs :  10\n",
      "mix_up :  True\n",
      "alpha :  0.3\n",
      "smooth :  False\n",
      "smooth_eps :  0\n",
      "save_last_model_only :  False\n",
      "num_partition :  4\n",
      "layer_type :  regular\n",
      "bn_type :  masked\n",
      "par_first_layer :  True\n",
      "comm_outsize :  True\n",
      "lambda_comm :  0.001\n",
      "lambda_comp :  0\n",
      "create_partition :  False\n",
      "load_model :  False\n",
      "distill_loss :  kl\n",
      "distill_temp :  30\n",
      "distill_alpha :  1\n"
     ]
    }
   ],
   "source": [
    "# model config\n",
    "\n",
    "environ[\"config\"] = f\"config/{dataset}.yaml\"\n",
    "configs = run_p.main()\n",
    "\n",
    "load_model = f\"{configs['data_code']}-{configs['model']}-{configs['sparsity_type']}-np{configs['num_partition']}-pr{configs['prune_ratio']}-lcm{configs['lambda_comm']}.pt\"\n",
    "\n",
    "# configs = {}\n",
    "# configs[\"num_classes\"] = num_classes\n",
    "# configs[\"data_code\"] = dataset\n",
    "# configs[\"layer_type\"] = layer_type\n",
    "# configs[\"bn_type\"] = bn_type\n",
    "# configs[\"model\"] = model\n",
    "# configs[\"prune_ratio\"] = prune_ratio\n",
    "# configs[\"seed\"] = 1234\n",
    "# configs[\"model_file\"] = load_model\n",
    "\n",
    "# if model == 'resnet18':\n",
    "#     dataset='cifar10'\n",
    "#     load_model = f\"cifar10-resnet18-kernel-np{num_partitions}-pr{prune_ratio}-lcm0.001.pt\"\n",
    "#     num_classes = 10\n",
    "#     layer_type = 'regular'\n",
    "#     bn_type = 'masked'\n",
    "#     model = 'resnet18'\n",
    "# elif model == 'EscFusion':\n",
    "#     dataset='esc'\n",
    "#     load_model = f\"esc-escnet-kernel-np{num_partitions}-pr{prune_ratio}-lcm0.001.pt\"\n",
    "#     num_classes = 2\n",
    "#     layer_type = 'regular'\n",
    "#     bn_type = 'regular'\n",
    "#     model = 'EscFusion'\n",
    "# elif model == 'InfoFusionThree':\n",
    "#     dataset='flash'\n",
    "#     load_model = f\"flash-flashnet-kernel-np{num_partitions}-pr{prune_ratio}-lcm0.001.pt\"\n",
    "# elif model == 'wrn28_10':\n",
    "#     dataset='cifar100'\n",
    "#     load_model = f\"cifar100-wrn28-kernel-np{num_partitions}-pr{prune_ratio}-lcm0.001.pt\"\n",
    "#     num_classes = 100\n",
    "#     layer_type = 'regular'\n",
    "#     bn_type = 'masked'\n",
    "#     model = 'wrn28_10'\n",
    "    \n",
    "\n",
    "configs[\"device\"] = \"cpu\"\n",
    "configs['load_model'] = load_model\n",
    "\n",
    "# if yaml_version == 'v0':\n",
    "#     configs['num_partition'] = num_partitions\n",
    "# else:\n",
    "#     if model == 'resnet18':\n",
    "#         configs[\"num_partition\"] = f'config/resnet18-{yaml_version}.yaml'\n",
    "#     elif model == 'EscFusion':\n",
    "#         configs[\"num_partition\"] = f'config/escnet-{yaml_version}.yaml'\n",
    "#     elif model == 'InfoFusionThree':\n",
    "#         configs[\"num_partition\"] = f'config/flashnet.yaml'\n",
    "#     elif model == 'wrn28_10':\n",
    "#         configs[\"num_partition\"] = f'config/wrn28-{yaml_version}.yaml'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WideResNet(\n",
      "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): ReLU(inplace=True)\n",
      "      (conv1): Conv2d(16, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu2): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (shortcut): Conv2d(16, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (bn1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): ReLU(inplace=True)\n",
      "      (conv1): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu2): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (bn1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): ReLU(inplace=True)\n",
      "      (conv1): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu2): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      (bn1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): ReLU(inplace=True)\n",
      "      (conv1): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu2): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (bn1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): ReLU(inplace=True)\n",
      "      (conv1): Conv2d(160, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu2): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (shortcut): Conv2d(160, 320, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (bn1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): ReLU(inplace=True)\n",
      "      (conv1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu2): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (bn1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): ReLU(inplace=True)\n",
      "      (conv1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu2): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      (bn1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): ReLU(inplace=True)\n",
      "      (conv1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu2): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (bn1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): ReLU(inplace=True)\n",
      "      (conv1): Conv2d(320, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu2): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (shortcut): Conv2d(320, 640, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (bn1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): ReLU(inplace=True)\n",
      "      (conv1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu2): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (bn1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): ReLU(inplace=True)\n",
      "      (conv1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu2): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      (bn1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): ReLU(inplace=True)\n",
      "      (conv1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu2): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    )\n",
      "  )\n",
      "  (bn1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)\n",
      "  (out): Linear(in_features=640, out_features=100, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# load data and load or train model\n",
    "model = get_model_from_code(configs).to(configs['device']) # grabs model architecture from ./source/models/escnet.py\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight\n",
      "torch.Size([16, 3, 3, 3])\n",
      "layer1.0.bn1.weight\n",
      "torch.Size([16])\n",
      "layer1.0.bn1.bias\n",
      "torch.Size([16])\n",
      "layer1.0.conv1.weight\n",
      "torch.Size([160, 16, 3, 3])\n",
      "layer1.0.bn2.weight\n",
      "torch.Size([160])\n",
      "layer1.0.bn2.bias\n",
      "torch.Size([160])\n",
      "layer1.0.conv2.weight\n",
      "torch.Size([160, 160, 3, 3])\n",
      "layer1.0.shortcut.weight\n",
      "torch.Size([160, 16, 1, 1])\n",
      "layer1.1.bn1.weight\n",
      "torch.Size([160])\n",
      "layer1.1.bn1.bias\n",
      "torch.Size([160])\n",
      "layer1.1.conv1.weight\n",
      "torch.Size([160, 160, 3, 3])\n",
      "layer1.1.bn2.weight\n",
      "torch.Size([160])\n",
      "layer1.1.bn2.bias\n",
      "torch.Size([160])\n",
      "layer1.1.conv2.weight\n",
      "torch.Size([160, 160, 3, 3])\n",
      "layer1.2.bn1.weight\n",
      "torch.Size([160])\n",
      "layer1.2.bn1.bias\n",
      "torch.Size([160])\n",
      "layer1.2.conv1.weight\n",
      "torch.Size([160, 160, 3, 3])\n",
      "layer1.2.bn2.weight\n",
      "torch.Size([160])\n",
      "layer1.2.bn2.bias\n",
      "torch.Size([160])\n",
      "layer1.2.conv2.weight\n",
      "torch.Size([160, 160, 3, 3])\n",
      "layer1.3.bn1.weight\n",
      "torch.Size([160])\n",
      "layer1.3.bn1.bias\n",
      "torch.Size([160])\n",
      "layer1.3.conv1.weight\n",
      "torch.Size([160, 160, 3, 3])\n",
      "layer1.3.bn2.weight\n",
      "torch.Size([160])\n",
      "layer1.3.bn2.bias\n",
      "torch.Size([160])\n",
      "layer1.3.conv2.weight\n",
      "torch.Size([160, 160, 3, 3])\n",
      "layer2.0.bn1.weight\n",
      "torch.Size([160])\n",
      "layer2.0.bn1.bias\n",
      "torch.Size([160])\n",
      "layer2.0.conv1.weight\n",
      "torch.Size([320, 160, 3, 3])\n",
      "layer2.0.bn2.weight\n",
      "torch.Size([320])\n",
      "layer2.0.bn2.bias\n",
      "torch.Size([320])\n",
      "layer2.0.conv2.weight\n",
      "torch.Size([320, 320, 3, 3])\n",
      "layer2.0.shortcut.weight\n",
      "torch.Size([320, 160, 1, 1])\n",
      "layer2.1.bn1.weight\n",
      "torch.Size([320])\n",
      "layer2.1.bn1.bias\n",
      "torch.Size([320])\n",
      "layer2.1.conv1.weight\n",
      "torch.Size([320, 320, 3, 3])\n",
      "layer2.1.bn2.weight\n",
      "torch.Size([320])\n",
      "layer2.1.bn2.bias\n",
      "torch.Size([320])\n",
      "layer2.1.conv2.weight\n",
      "torch.Size([320, 320, 3, 3])\n",
      "layer2.2.bn1.weight\n",
      "torch.Size([320])\n",
      "layer2.2.bn1.bias\n",
      "torch.Size([320])\n",
      "layer2.2.conv1.weight\n",
      "torch.Size([320, 320, 3, 3])\n",
      "layer2.2.bn2.weight\n",
      "torch.Size([320])\n",
      "layer2.2.bn2.bias\n",
      "torch.Size([320])\n",
      "layer2.2.conv2.weight\n",
      "torch.Size([320, 320, 3, 3])\n",
      "layer2.3.bn1.weight\n",
      "torch.Size([320])\n",
      "layer2.3.bn1.bias\n",
      "torch.Size([320])\n",
      "layer2.3.conv1.weight\n",
      "torch.Size([320, 320, 3, 3])\n",
      "layer2.3.bn2.weight\n",
      "torch.Size([320])\n",
      "layer2.3.bn2.bias\n",
      "torch.Size([320])\n",
      "layer2.3.conv2.weight\n",
      "torch.Size([320, 320, 3, 3])\n",
      "layer3.0.bn1.weight\n",
      "torch.Size([320])\n",
      "layer3.0.bn1.bias\n",
      "torch.Size([320])\n",
      "layer3.0.conv1.weight\n",
      "torch.Size([640, 320, 3, 3])\n",
      "layer3.0.bn2.weight\n",
      "torch.Size([640])\n",
      "layer3.0.bn2.bias\n",
      "torch.Size([640])\n",
      "layer3.0.conv2.weight\n",
      "torch.Size([640, 640, 3, 3])\n",
      "layer3.0.shortcut.weight\n",
      "torch.Size([640, 320, 1, 1])\n",
      "layer3.1.bn1.weight\n",
      "torch.Size([640])\n",
      "layer3.1.bn1.bias\n",
      "torch.Size([640])\n",
      "layer3.1.conv1.weight\n",
      "torch.Size([640, 640, 3, 3])\n",
      "layer3.1.bn2.weight\n",
      "torch.Size([640])\n",
      "layer3.1.bn2.bias\n",
      "torch.Size([640])\n",
      "layer3.1.conv2.weight\n",
      "torch.Size([640, 640, 3, 3])\n",
      "layer3.2.bn1.weight\n",
      "torch.Size([640])\n",
      "layer3.2.bn1.bias\n",
      "torch.Size([640])\n",
      "layer3.2.conv1.weight\n",
      "torch.Size([640, 640, 3, 3])\n",
      "layer3.2.bn2.weight\n",
      "torch.Size([640])\n",
      "layer3.2.bn2.bias\n",
      "torch.Size([640])\n",
      "layer3.2.conv2.weight\n",
      "torch.Size([640, 640, 3, 3])\n",
      "layer3.3.bn1.weight\n",
      "torch.Size([640])\n",
      "layer3.3.bn1.bias\n",
      "torch.Size([640])\n",
      "layer3.3.conv1.weight\n",
      "torch.Size([640, 640, 3, 3])\n",
      "layer3.3.bn2.weight\n",
      "torch.Size([640])\n",
      "layer3.3.bn2.bias\n",
      "torch.Size([640])\n",
      "layer3.3.conv2.weight\n",
      "torch.Size([640, 640, 3, 3])\n",
      "bn1.weight\n",
      "torch.Size([640])\n",
      "bn1.bias\n",
      "torch.Size([640])\n",
      "out.weight\n",
      "torch.Size([100, 640])\n",
      "out.bias\n",
      "torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "# load weights into full model\n",
    "state_dict = torch.load(io.get_model_path_split(\"{}\".format(configs[\"load_model\"])), map_location=configs['device'])\n",
    "model = io.load_state_dict(model, \n",
    "                    state_dict['model_state_dict'] if 'model_state_dict' in state_dict \n",
    "                    else state_dict['state_dict'] if 'state_dict' in state_dict else state_dict,)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(name)\n",
    "    print(param.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_partition: {'conv1.weight': 4, 'inputs': 4, 'layer1.0.conv1.weight': 4, 'layer1.0.conv2.weight': 4, 'layer1.0.shortcut.weight': 4, 'layer1.1.conv1.weight': 4, 'layer1.1.conv2.weight': 4, 'layer1.2.conv1.weight': 4, 'layer1.2.conv2.weight': 4, 'layer1.3.conv1.weight': 4, 'layer1.3.conv2.weight': 4, 'layer2.0.conv1.weight': 4, 'layer2.0.conv2.weight': 4, 'layer2.0.shortcut.weight': 4, 'layer2.1.conv1.weight': 4, 'layer2.1.conv2.weight': 4, 'layer2.2.conv1.weight': 4, 'layer2.2.conv2.weight': 4, 'layer2.3.conv1.weight': 4, 'layer2.3.conv2.weight': 4, 'layer3.0.conv1.weight': 4, 'layer3.0.conv2.weight': 4, 'layer3.0.shortcut.weight': 4, 'layer3.1.conv1.weight': 4, 'layer3.1.conv2.weight': 4, 'layer3.2.conv1.weight': 4, 'layer3.2.conv2.weight': 4, 'layer3.3.conv1.weight': 4, 'layer3.3.conv2.weight': 4}\n",
      "ratio_partition: {'conv1.weight': [1, 1, 1, 1], 'inputs': [1, 1, 1, 1], 'layer1.0.conv1.weight': [1, 1, 1, 1], 'layer1.0.conv2.weight': [1, 1, 1, 1], 'layer1.0.shortcut.weight': [1, 1, 1, 1], 'layer1.1.conv1.weight': [1, 1, 1, 1], 'layer1.1.conv2.weight': [1, 1, 1, 1], 'layer1.2.conv1.weight': [1, 1, 1, 1], 'layer1.2.conv2.weight': [1, 1, 1, 1], 'layer1.3.conv1.weight': [1, 1, 1, 1], 'layer1.3.conv2.weight': [1, 1, 1, 1], 'layer2.0.conv1.weight': [1, 1, 1, 1], 'layer2.0.conv2.weight': [1, 1, 1, 1], 'layer2.0.shortcut.weight': [1, 1, 1, 1], 'layer2.1.conv1.weight': [1, 1, 1, 1], 'layer2.1.conv2.weight': [1, 1, 1, 1], 'layer2.2.conv1.weight': [1, 1, 1, 1], 'layer2.2.conv2.weight': [1, 1, 1, 1], 'layer2.3.conv1.weight': [1, 1, 1, 1], 'layer2.3.conv2.weight': [1, 1, 1, 1], 'layer3.0.conv1.weight': [1, 1, 1, 1], 'layer3.0.conv2.weight': [1, 1, 1, 1], 'layer3.0.shortcut.weight': [1, 1, 1, 1], 'layer3.1.conv1.weight': [1, 1, 1, 1], 'layer3.1.conv2.weight': [1, 1, 1, 1], 'layer3.2.conv1.weight': [1, 1, 1, 1], 'layer3.2.conv2.weight': [1, 1, 1, 1], 'layer3.3.conv1.weight': [1, 1, 1, 1], 'layer3.3.conv2.weight': [1, 1, 1, 1]}\n",
      "map_partition: {'conv1.weight': [[0, 1, 1, 1], [1, 0, 1, 1], [1, 1, 0, 1], [1, 1, 1, 0]], 'inputs': [[0, 1, 1, 1], [1, 0, 1, 1], [1, 1, 0, 1], [1, 1, 1, 0]], 'layer1.0.conv1.weight': [[0, 1, 1, 1], [1, 0, 1, 1], [1, 1, 0, 1], [1, 1, 1, 0]], 'layer1.0.conv2.weight': [[0, 1, 1, 1], [1, 0, 1, 1], [1, 1, 0, 1], [1, 1, 1, 0]], 'layer1.0.shortcut.weight': [[0, 1, 1, 1], [1, 0, 1, 1], [1, 1, 0, 1], [1, 1, 1, 0]], 'layer1.1.conv1.weight': [[0, 1, 1, 1], [1, 0, 1, 1], [1, 1, 0, 1], [1, 1, 1, 0]], 'layer1.1.conv2.weight': [[0, 1, 1, 1], [1, 0, 1, 1], [1, 1, 0, 1], [1, 1, 1, 0]], 'layer1.2.conv1.weight': [[0, 1, 1, 1], [1, 0, 1, 1], [1, 1, 0, 1], [1, 1, 1, 0]], 'layer1.2.conv2.weight': [[0, 1, 1, 1], [1, 0, 1, 1], [1, 1, 0, 1], [1, 1, 1, 0]], 'layer1.3.conv1.weight': [[0, 1, 1, 1], [1, 0, 1, 1], [1, 1, 0, 1], [1, 1, 1, 0]], 'layer1.3.conv2.weight': [[0, 1, 1, 1], [1, 0, 1, 1], [1, 1, 0, 1], [1, 1, 1, 0]], 'layer2.0.conv1.weight': [[0, 1, 1, 1], [1, 0, 1, 1], [1, 1, 0, 1], [1, 1, 1, 0]], 'layer2.0.conv2.weight': [[0, 1, 1, 1], [1, 0, 1, 1], [1, 1, 0, 1], [1, 1, 1, 0]], 'layer2.0.shortcut.weight': [[0, 1, 1, 1], [1, 0, 1, 1], [1, 1, 0, 1], [1, 1, 1, 0]], 'layer2.1.conv1.weight': [[0, 1, 1, 1], [1, 0, 1, 1], [1, 1, 0, 1], [1, 1, 1, 0]], 'layer2.1.conv2.weight': [[0, 1, 1, 1], [1, 0, 1, 1], [1, 1, 0, 1], [1, 1, 1, 0]], 'layer2.2.conv1.weight': [[0, 1, 1, 1], [1, 0, 1, 1], [1, 1, 0, 1], [1, 1, 1, 0]], 'layer2.2.conv2.weight': [[0, 1, 1, 1], [1, 0, 1, 1], [1, 1, 0, 1], [1, 1, 1, 0]], 'layer2.3.conv1.weight': [[0, 1, 1, 1], [1, 0, 1, 1], [1, 1, 0, 1], [1, 1, 1, 0]], 'layer2.3.conv2.weight': [[0, 1, 1, 1], [1, 0, 1, 1], [1, 1, 0, 1], [1, 1, 1, 0]], 'layer3.0.conv1.weight': [[0, 1, 1, 1], [1, 0, 1, 1], [1, 1, 0, 1], [1, 1, 1, 0]], 'layer3.0.conv2.weight': [[0, 1, 1, 1], [1, 0, 1, 1], [1, 1, 0, 1], [1, 1, 1, 0]], 'layer3.0.shortcut.weight': [[0, 1, 1, 1], [1, 0, 1, 1], [1, 1, 0, 1], [1, 1, 1, 0]], 'layer3.1.conv1.weight': [[0, 1, 1, 1], [1, 0, 1, 1], [1, 1, 0, 1], [1, 1, 1, 0]], 'layer3.1.conv2.weight': [[0, 1, 1, 1], [1, 0, 1, 1], [1, 1, 0, 1], [1, 1, 1, 0]], 'layer3.2.conv1.weight': [[0, 1, 1, 1], [1, 0, 1, 1], [1, 1, 0, 1], [1, 1, 1, 0]], 'layer3.2.conv2.weight': [[0, 1, 1, 1], [1, 0, 1, 1], [1, 1, 0, 1], [1, 1, 1, 0]], 'layer3.3.conv1.weight': [[0, 1, 1, 1], [1, 0, 1, 1], [1, 1, 0, 1], [1, 1, 1, 0]], 'layer3.3.conv2.weight': [[0, 1, 1, 1], [1, 0, 1, 1], [1, 1, 0, 1], [1, 1, 1, 0]]}\n",
      "bn_partition: [4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "Inference time per data is 53.766251ms.\n",
      "conv1.weight 1024\n",
      "layer1.0.conv1.weight 1024\n",
      "layer1.0.conv2.weight 1024\n",
      "layer1.0.shortcut.weight 1024\n",
      "layer1.1.conv1.weight 1024\n",
      "layer1.1.conv2.weight 1024\n",
      "layer1.2.conv1.weight 1024\n",
      "layer1.2.conv2.weight 1024\n",
      "layer1.3.conv1.weight 1024\n",
      "layer1.3.conv2.weight 1024\n",
      "layer2.0.conv1.weight 256\n",
      "layer2.0.conv2.weight 256\n",
      "layer2.0.shortcut.weight 256\n",
      "layer2.1.conv1.weight 256\n",
      "layer2.1.conv2.weight 256\n",
      "layer2.2.conv1.weight 256\n",
      "layer2.2.conv2.weight 256\n",
      "layer2.3.conv1.weight 256\n",
      "layer2.3.conv2.weight 256\n",
      "layer3.0.conv1.weight 64\n",
      "layer3.0.conv2.weight 64\n",
      "layer3.0.shortcut.weight 64\n",
      "layer3.1.conv1.weight 64\n",
      "layer3.1.conv2.weight 64\n",
      "layer3.2.conv1.weight 64\n",
      "layer3.2.conv2.weight 64\n",
      "layer3.3.conv1.weight 64\n",
      "layer3.3.conv2.weight 64\n",
      "Total layers: 106\n",
      "['x', 'conv1', 'layer1.0.bn1', 'layer1.0.relu1', 'layer1.0.conv1', 'layer1.0.bn2', 'layer1.0.relu2', 'layer1.0.dropout', 'layer1.0.conv2', 'layer1.0.shortcut', 'layer1.0.add', 'layer1.1.bn1', 'layer1.1.relu1', 'layer1.1.conv1', 'layer1.1.bn2', 'layer1.1.relu2', 'layer1.1.dropout', 'layer1.1.conv2', 'layer1.1.add', 'layer1.2.bn1', 'layer1.2.relu1', 'layer1.2.conv1', 'layer1.2.bn2', 'layer1.2.relu2', 'layer1.2.dropout', 'layer1.2.conv2', 'layer1.2.add', 'layer1.3.bn1', 'layer1.3.relu1', 'layer1.3.conv1', 'layer1.3.bn2', 'layer1.3.relu2', 'layer1.3.dropout', 'layer1.3.conv2', 'layer1.3.add', 'layer2.0.bn1', 'layer2.0.relu1', 'layer2.0.conv1', 'layer2.0.bn2', 'layer2.0.relu2', 'layer2.0.dropout', 'layer2.0.conv2', 'layer2.0.shortcut', 'layer2.0.add', 'layer2.1.bn1', 'layer2.1.relu1', 'layer2.1.conv1', 'layer2.1.bn2', 'layer2.1.relu2', 'layer2.1.dropout', 'layer2.1.conv2', 'layer2.1.add', 'layer2.2.bn1', 'layer2.2.relu1', 'layer2.2.conv1', 'layer2.2.bn2', 'layer2.2.relu2', 'layer2.2.dropout', 'layer2.2.conv2', 'layer2.2.add', 'layer2.3.bn1', 'layer2.3.relu1', 'layer2.3.conv1', 'layer2.3.bn2', 'layer2.3.relu2', 'layer2.3.dropout', 'layer2.3.conv2', 'layer2.3.add', 'layer3.0.bn1', 'layer3.0.relu1', 'layer3.0.conv1', 'layer3.0.bn2', 'layer3.0.relu2', 'layer3.0.dropout', 'layer3.0.conv2', 'layer3.0.shortcut', 'layer3.0.add', 'layer3.1.bn1', 'layer3.1.relu1', 'layer3.1.conv1', 'layer3.1.bn2', 'layer3.1.relu2', 'layer3.1.dropout', 'layer3.1.conv2', 'layer3.1.add', 'layer3.2.bn1', 'layer3.2.relu1', 'layer3.2.conv1', 'layer3.2.bn2', 'layer3.2.relu2', 'layer3.2.dropout', 'layer3.2.conv2', 'layer3.2.add', 'layer3.3.bn1', 'layer3.3.relu1', 'layer3.3.conv1', 'layer3.3.bn2', 'layer3.3.relu2', 'layer3.3.dropout', 'layer3.3.conv2', 'layer3.3.add', 'bn1', 'relu', 'avg_pool', 'view', 'out']\n",
      "num_machines: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yildiz.ay/miniconda3/envs/cap/lib/python3.9/site-packages/torch/overrides.py:110: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  torch.has_cuda,\n",
      "/home/yildiz.ay/miniconda3/envs/cap/lib/python3.9/site-packages/torch/overrides.py:111: UserWarning: 'has_cudnn' is deprecated, please use 'torch.backends.cudnn.is_available()'\n",
      "  torch.has_cudnn,\n",
      "/home/yildiz.ay/miniconda3/envs/cap/lib/python3.9/site-packages/torch/overrides.py:117: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  torch.has_mps,\n",
      "/home/yildiz.ay/miniconda3/envs/cap/lib/python3.9/site-packages/torch/overrides.py:118: UserWarning: 'has_mkldnn' is deprecated, please use 'torch.backends.mkldnn.is_available()'\n",
      "  torch.has_mkldnn,\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    add partitions and communications to configs\n",
    "'''\n",
    "\n",
    "# gets random test input (with correct size)\n",
    "input_var = get_input_from_code(configs)\n",
    "\n",
    "# Config partitions and prune_ratio\n",
    "configs = engine.partition_generator(configs, model)\n",
    "            \n",
    "# Compute output size of each layer\n",
    "configs['partition'] = engine.featuremap_summary(model, configs['partition'], input_var)\n",
    "\n",
    "# print(configs['partition'])\n",
    "        \n",
    "# model communication costs\n",
    "configs['comm_costs'] = engine.set_communication_cost(model, configs['partition'],)\n",
    "\n",
    "# print(configs['comm_costs'])\n",
    "\n",
    "\n",
    "# split model general parameters\n",
    "\n",
    "# make copies of model per machine\n",
    "num_machines = max(configs['partition']['bn_partition']) # TODO: double check this makes sense\n",
    "model_machines = [model]*num_machines\n",
    "\n",
    "layer_names_fx =  get_graph_node_names(model)[1]\n",
    "total_layers_fx = len(layer_names_fx)\n",
    "print(f\"Total layers: {total_layers_fx}\")\n",
    "\n",
    "split_module_names = list(configs['partition'].keys())\n",
    "\n",
    "print(layer_names_fx)\n",
    "print('num_machines:', num_machines)\n",
    "\n",
    "# print(model.layer1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing module 0: x\n",
      "\tExecuting on machine 0\n",
      "\t\t-model input layer.. skipping\n",
      "\tExecuting on machine 1\n",
      "\t\t-model input layer.. skipping\n",
      "\tExecuting on machine 2\n",
      "\t\t-model input layer.. skipping\n",
      "\tExecuting on machine 3\n",
      "\t\t-model input layer.. skipping\n",
      "Finished execution of layer 0\n",
      "Input layer. Skipping comparison\n",
      "\n",
      "Executing module 1: conv1\n",
      "\tExecuting on machine 0\n",
      "\t\t-WARNING: No input assigned to this machine (but it was sent input?). Skipping...\n",
      "\tExecuting on machine 1\n",
      "\t\t-Splitting conv layer 1\n",
      "\t\t Output tensor shape : torch.Size([1, 16, 32, 32])\n",
      "\t\t sending C_out tensor([4, 5, 6, 7]) to machine 1\n",
      "\tExecuting on machine 2\n",
      "\t\t-Splitting conv layer 1\n",
      "\t\t Output tensor shape : torch.Size([1, 16, 32, 32])\n",
      "\t\t sending C_out tensor([ 8,  9, 10, 11]) to machine 2\n",
      "\tExecuting on machine 3\n",
      "\t\t-Splitting conv layer 1\n",
      "\t\t Output tensor shape : torch.Size([1, 16, 32, 32])\n",
      "\t\t sending C_out tensor([12, 13, 14, 15]) to machine 3\n",
      "Finished execution of layer 1\n",
      "Max diff:\n",
      "tensor([0.], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([], dtype=torch.int64)  (len = 0)\n",
      "passing Cout = tensor([ 4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15])  (len = 12)\n",
      "\n",
      "Executing module 2: layer1.0.bn1\n",
      "\tExecuting on machine 0\n",
      "\t\t-No input received but bn still needs to produce output.\n",
      "\t\t-Saving input for later...\n",
      "\t\t-Splitting batch norm layer 2\n",
      "\t\t Output tensor shape : torch.Size([1, 16, 32, 32])\n",
      "\t\t sending C_out tensor([0, 1, 2, 3]) to machine 0\n",
      "\tExecuting on machine 1\n",
      "\t\t-Saving input for later...\n",
      "\t\t-Splitting batch norm layer 2\n",
      "\t\t Output tensor shape : torch.Size([1, 16, 32, 32])\n",
      "\t\t sending C_out tensor([4, 5, 6, 7]) to machine 1\n",
      "\tExecuting on machine 2\n",
      "\t\t-Saving input for later...\n",
      "\t\t-Splitting batch norm layer 2\n",
      "\t\t Output tensor shape : torch.Size([1, 16, 32, 32])\n",
      "\t\t sending C_out tensor([ 8,  9, 10, 11]) to machine 2\n",
      "\tExecuting on machine 3\n",
      "\t\t-Saving input for later...\n",
      "\t\t-Splitting batch norm layer 2\n",
      "\t\t Output tensor shape : torch.Size([1, 16, 32, 32])\n",
      "\t\t sending C_out tensor([12, 13, 14, 15]) to machine 3\n",
      "Finished execution of layer 2\n",
      "Max diff:\n",
      "tensor([1.1102], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([ 4,  5,  6,  7,  8, 10, 11, 12, 13, 14, 15])  (len = 11)\n",
      "passing Cout = tensor([1, 2])  (len = 2)\n",
      "\n",
      "Executing module 3: layer1.0.relu1\n",
      "\tExecuting on machine 0\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 1\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 2\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 3\n",
      "\t\t-Applying ReLU\n",
      "Finished execution of layer 3\n",
      "Max diff:\n",
      "tensor([0.], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([], dtype=torch.int64)  (len = 0)\n",
      "passing Cout = tensor([ 1,  2,  4,  5,  6,  7,  8, 10, 11, 12, 13, 14, 15])  (len = 13)\n",
      "\n",
      "Executing module 4: layer1.0.conv1\n",
      "\tExecuting on machine 0\n",
      "\t\t-Splitting conv layer 4\n",
      "\t\t Output tensor shape : torch.Size([1, 160, 32, 32])\n",
      "\t\t sending C_out tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "        36, 37, 38, 39]) to machine 0\n",
      "\t\t sending C_out tensor([40, 45]) to machine 1\n",
      "\tExecuting on machine 1\n",
      "\t\t-Splitting conv layer 4\n",
      "\t\t Output tensor shape : torch.Size([1, 160, 32, 32])\n",
      "\t\t sending C_out tensor([26]) to machine 0\n",
      "\t\t sending C_out tensor([40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57,\n",
      "        58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75,\n",
      "        76, 77, 78, 79]) to machine 1\n",
      "\t\t sending C_out tensor([91]) to machine 2\n",
      "\t\t sending C_out tensor([128, 144]) to machine 3\n",
      "\tExecuting on machine 2\n",
      "\t\t-Splitting conv layer 4\n",
      "\t\t Output tensor shape : torch.Size([1, 160, 32, 32])\n",
      "\t\t sending C_out tensor([16]) to machine 0\n",
      "\t\t sending C_out tensor([45]) to machine 1\n",
      "\t\t sending C_out tensor([ 80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,  92,  93,\n",
      "         94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104, 105, 106, 107,\n",
      "        108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119]) to machine 2\n",
      "\tExecuting on machine 3\n",
      "\t\t-Splitting conv layer 4\n",
      "\t\t Output tensor shape : torch.Size([1, 160, 32, 32])\n",
      "\t\t sending C_out tensor([46, 50, 69]) to machine 1\n",
      "\t\t sending C_out tensor([120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133,\n",
      "        134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147,\n",
      "        148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159]) to machine 3\n",
      "Finished execution of layer 4\n",
      "Max diff:\n",
      "tensor([2.7756e-17], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([ 16,  26,  40,  45,  46,  50,  69,  91, 128, 144])  (len = 10)\n",
      "passing Cout = tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  17,  18,  19,  20,  21,  22,  23,  24,  25,  27,  28,  29,\n",
      "         30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  41,  42,  43,  44,\n",
      "         47,  48,  49,  51,  52,  53,  54,  55,  56,  57,  58,  59,  60,  61,\n",
      "         62,  63,  64,  65,  66,  67,  68,  70,  71,  72,  73,  74,  75,  76,\n",
      "         77,  78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
      "         92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104, 105,\n",
      "        106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119,\n",
      "        120, 121, 122, 123, 124, 125, 126, 127, 129, 130, 131, 132, 133, 134,\n",
      "        135, 136, 137, 138, 139, 140, 141, 142, 143, 145, 146, 147, 148, 149,\n",
      "        150, 151, 152, 153, 154, 155, 156, 157, 158, 159])  (len = 150)\n",
      "\n",
      "Executing module 5: layer1.0.bn2\n",
      "\tExecuting on machine 0\n",
      "\t\t-Splitting batch norm layer 5\n",
      "\t\t Output tensor shape : torch.Size([1, 160, 32, 32])\n",
      "\t\t sending C_out tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "        36, 37, 38, 39]) to machine 0\n",
      "\tExecuting on machine 1\n",
      "\t\t-Splitting batch norm layer 5\n",
      "\t\t Output tensor shape : torch.Size([1, 160, 32, 32])\n",
      "\t\t sending C_out tensor([40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57,\n",
      "        58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75,\n",
      "        76, 77, 78, 79]) to machine 1\n",
      "\tExecuting on machine 2\n",
      "\t\t-Splitting batch norm layer 5\n",
      "\t\t Output tensor shape : torch.Size([1, 160, 32, 32])\n",
      "\t\t sending C_out tensor([ 80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,  92,  93,\n",
      "         94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104, 105, 106, 107,\n",
      "        108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119]) to machine 2\n",
      "\tExecuting on machine 3\n",
      "\t\t-Splitting batch norm layer 5\n",
      "\t\t Output tensor shape : torch.Size([1, 160, 32, 32])\n",
      "\t\t sending C_out tensor([120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133,\n",
      "        134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147,\n",
      "        148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159]) to machine 3\n",
      "Finished execution of layer 5\n",
      "Max diff:\n",
      "tensor([0.1411], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([ 16,  40,  41,  42,  43,  44,  45,  46,  47,  48,  50,  51,  53,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  66,  68,  70,  71,  73,\n",
      "         74,  75,  76,  77,  78,  82,  83,  84,  85,  87,  88,  89,  90,  91,\n",
      "         92,  93,  94,  95,  96,  98,  99, 101, 102, 103, 104, 105, 106, 107,\n",
      "        108, 110, 111, 112, 113, 114, 115, 116, 120, 121, 122, 123, 124, 126,\n",
      "        128, 129, 130, 131, 132, 133, 134, 135, 137, 138, 139, 141, 142, 143,\n",
      "        144, 145, 147, 148, 151, 152, 153, 154, 155, 156, 158, 159])  (len = 96)\n",
      "passing Cout = tensor([ 11,  33,  37, 150])  (len = 4)\n",
      "\n",
      "Executing module 6: layer1.0.relu2\n",
      "\tExecuting on machine 0\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 1\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 2\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 3\n",
      "\t\t-Applying ReLU\n",
      "Finished execution of layer 6\n",
      "Max diff:\n",
      "tensor([1.0408e-17], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([ 16,  40,  45,  46,  91, 128, 144])  (len = 7)\n",
      "passing Cout = tensor([ 11,  33,  37,  41,  42,  43,  44,  47,  48,  50,  51,  53,  55,  56,\n",
      "         57,  58,  59,  60,  61,  62,  63,  64,  66,  68,  70,  71,  73,  74,\n",
      "         75,  76,  77,  78,  82,  83,  84,  85,  87,  88,  89,  90,  92,  93,\n",
      "         94,  95,  96,  98,  99, 101, 102, 103, 104, 105, 106, 107, 108, 110,\n",
      "        111, 112, 113, 114, 115, 116, 120, 121, 122, 123, 124, 126, 129, 130,\n",
      "        131, 132, 133, 134, 135, 137, 138, 139, 141, 142, 143, 145, 147, 148,\n",
      "        150, 151, 152, 153, 154, 155, 156, 158, 159])  (len = 93)\n",
      "\n",
      "Executing module 7: layer1.0.dropout\n",
      "\tExecuting on machine 0\n",
      "\t\t-Applying Dropout\n",
      "\tExecuting on machine 1\n",
      "\t\t-Applying Dropout\n",
      "\tExecuting on machine 2\n",
      "\t\t-Applying Dropout\n",
      "\tExecuting on machine 3\n",
      "\t\t-Applying Dropout\n",
      "Finished execution of layer 7\n",
      "Max diff:\n",
      "tensor([0.0962], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([ 11,  16,  33,  37,  40,  41,  42,  43,  44,  45,  46,  47,  48,  50,\n",
      "         51,  53,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,  66,  68,\n",
      "         70,  71,  73,  74,  75,  76,  77,  78,  82,  83,  84,  85,  87,  88,\n",
      "         89,  90,  91,  92,  93,  94,  95,  96,  98,  99, 101, 102, 103, 104,\n",
      "        105, 106, 107, 108, 110, 111, 112, 113, 114, 115, 116, 120, 121, 122,\n",
      "        123, 124, 126, 128, 129, 130, 131, 132, 133, 134, 135, 137, 138, 139,\n",
      "        141, 142, 143, 144, 145, 147, 148, 150, 151, 152, 153, 154, 155, 156,\n",
      "        158, 159])  (len = 100)\n",
      "passing Cout = tensor([], dtype=torch.int64)  (len = 0)\n",
      "\n",
      "Executing module 8: layer1.0.conv2\n",
      "\tExecuting on machine 0\n",
      "\t\t-Splitting conv layer 8\n",
      "\t\t Output tensor shape : torch.Size([1, 160, 32, 32])\n",
      "\t\t sending C_out tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "        36, 37, 38, 39]) to machine 0\n",
      "\t\t sending C_out tensor([65]) to machine 1\n",
      "\t\t sending C_out tensor([ 85, 119]) to machine 2\n",
      "\tExecuting on machine 1\n",
      "\t\t-Splitting conv layer 8\n",
      "\t\t Output tensor shape : torch.Size([1, 160, 32, 32])\n",
      "\t\t sending C_out tensor([16, 23, 32]) to machine 0\n",
      "\t\t sending C_out tensor([40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57,\n",
      "        58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75,\n",
      "        76, 77, 78, 79]) to machine 1\n",
      "\t\t sending C_out tensor([ 81,  85,  88,  94, 102, 106, 107, 109, 111, 112, 113, 116]) to machine 2\n",
      "\t\t sending C_out tensor([121, 128, 130, 131, 132, 133, 141, 145, 154, 157, 158]) to machine 3\n",
      "\tExecuting on machine 2\n",
      "\t\t-Splitting conv layer 8\n",
      "\t\t Output tensor shape : torch.Size([1, 160, 32, 32])\n",
      "\t\t sending C_out tensor([ 5,  7,  9, 15, 16, 18, 19, 25, 31, 38, 39]) to machine 0\n",
      "\t\t sending C_out tensor([44, 47, 53, 58, 61, 62, 65, 75]) to machine 1\n",
      "\t\t sending C_out tensor([ 80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,  92,  93,\n",
      "         94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104, 105, 106, 107,\n",
      "        108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119]) to machine 2\n",
      "\t\t sending C_out tensor([127, 129, 141, 156, 157]) to machine 3\n",
      "\tExecuting on machine 3\n",
      "\t\t-Splitting conv layer 8\n",
      "\t\t Output tensor shape : torch.Size([1, 160, 32, 32])\n",
      "\t\t sending C_out tensor([16, 22, 32, 39]) to machine 0\n",
      "\t\t sending C_out tensor([44, 45, 59, 69, 79]) to machine 1\n",
      "\t\t sending C_out tensor([ 88,  91,  95,  96,  97,  98, 104, 105, 108, 109, 115]) to machine 2\n",
      "\t\t sending C_out tensor([120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133,\n",
      "        134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147,\n",
      "        148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159]) to machine 3\n",
      "Finished execution of layer 8\n",
      "Max diff:\n",
      "tensor([0.0320], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
      "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
      "        154, 155, 156, 157, 158, 159])  (len = 160)\n",
      "passing Cout = tensor([], dtype=torch.int64)  (len = 0)\n",
      "\n",
      "Executing module 9: layer1.0.shortcut\n",
      "\tExecuting on machine 0\n",
      "\t\t-Splitting conv layer 9\n",
      "\t\t Output tensor shape : torch.Size([1, 160, 32, 32])\n",
      "\t\t sending C_out tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "        36, 37, 38, 39]) to machine 0\n",
      "\t\t sending C_out tensor([43, 44, 49, 53, 68, 72, 79]) to machine 1\n",
      "\t\t sending C_out tensor([ 86,  90,  97, 102, 108]) to machine 2\n",
      "\t\t sending C_out tensor([129, 132, 135, 144, 150, 151, 152, 153, 156, 159]) to machine 3\n",
      "\tExecuting on machine 1\n",
      "\t\t-Splitting conv layer 9\n",
      "\t\t Output tensor shape : torch.Size([1, 160, 32, 32])\n",
      "\tExecuting on machine 2\n",
      "\t\t-Splitting conv layer 9\n",
      "\t\t Output tensor shape : torch.Size([1, 160, 32, 32])\n",
      "\tExecuting on machine 3\n",
      "\t\t-Splitting conv layer 9\n",
      "\t\t Output tensor shape : torch.Size([1, 160, 32, 32])\n",
      "Finished execution of layer 9\n",
      "Max diff:\n",
      "tensor([0.2050], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  11,  12,  13,  14,\n",
      "         15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,  28,\n",
      "         29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,  42,\n",
      "         43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,  56,\n",
      "         57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,  70,\n",
      "         71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,  84,\n",
      "         85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,  98,\n",
      "         99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112,\n",
      "        113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126,\n",
      "        127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140,\n",
      "        141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154,\n",
      "        155, 156, 157, 158, 159])  (len = 159)\n",
      "passing Cout = tensor([], dtype=torch.int64)  (len = 0)\n",
      "\n",
      "Executing module 10: layer1.0.add\n",
      "\tExecuting on machine 0\n",
      "\t\t-adding residual\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (160) must match the size of tensor b (16) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 134\u001b[0m\n\u001b[1;32m    129\u001b[0m     curr_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m[imach][imach]\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;66;03m# print(f'ikinci elif e girdi, curr_input: {curr_input}')\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m# print(f'len curr_input: {len(curr_input)}')\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m out_tensor, do_comms \u001b[38;5;241m=\u001b[39m \u001b[43msplit_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mimach\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_split_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurr_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimodule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m# print(f'out_tensor: {out_tensor}')\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m do_comms:\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;66;03m# update output to current machine and continue\u001b[39;00m\n",
      "File \u001b[0;32m~/PROJECTS/CaP/source/core/split_manager.py:112\u001b[0m, in \u001b[0;36mSplitManager.execute_split_layer\u001b[0;34m(self, curr_input, imodule)\u001b[0m\n\u001b[1;32m    110\u001b[0m         curr_input \u001b[38;5;241m=\u001b[39m curr_input \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual_input[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmachine][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblock_out\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblock_in\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual_input[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmachine]:\n\u001b[0;32m--> 112\u001b[0m         curr_input \u001b[38;5;241m=\u001b[39m \u001b[43mcurr_input\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresidual_input\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmachine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mblock_in\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    113\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m-assuming shortcut had no layers\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (160) must match the size of tensor b (16) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "''' Prep Inout for Mock Run version 2 (fx)'''\n",
    "\n",
    "# TODO: reduce size of communicated tensors to only what is necessary \n",
    "# TODO: also check bias for nonzero\n",
    "# TODO: come up with more general scheme to handle residual layers\n",
    "\n",
    "# channel_id == INPUTS\n",
    "# filter_id  == OUTPUTS\n",
    "\n",
    "# try greater precision\n",
    "#torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "#torch.set_default_dtype(torch.float64)\n",
    "\n",
    "# model input \n",
    "N_batch = 1\n",
    "torch.manual_seed(0)\n",
    "\n",
    "if configs['model'] == 'resnet18' or configs['model'] == 'wrn28_10':\n",
    "    input_tensor = torch.rand(N_batch, 3, 32, 32, dtype=torch.float64, device=torch.device(configs['device'])) # 1k images, 3 channels, 32x32 image (cifar10)\n",
    "    input_tensor = tuple([input_tensor])\n",
    "    input_tensor = torch.cat(input_tensor, dim=1)\n",
    "elif configs['model'] == 'EscFusion':\n",
    "    input_tensor = torch.rand(N_batch, 15, 266, 320, dtype=torch.float64, device=torch.device(configs['device']))\n",
    "    # input_tensor = torch.cat([input_tensor]*5, dim=1)\n",
    "    input_tensor = tuple([input_tensor])\n",
    "    input_tensor = torch.cat(input_tensor, dim=1)\n",
    "elif configs['model'] == 'InfoFusionThree':\n",
    "    input_tensor = torch.rand(N_batch, 2, 10, 10, dtype=torch.float64, device=torch.device(configs['device']))\n",
    "\n",
    "\n",
    "# if configs['data_code'] == 'flash':\n",
    "#     input_shape = [(2, 1),\n",
    "#                 #    (90, 160, 3),\n",
    "#                     (360, 640, 3),\n",
    "#                     (20, 20, 20),]\n",
    "# elif configs['data_code'] == 'esc':\n",
    "#     input_shape = [(3, 266, 320) for _ in range(5)]\n",
    "# else:\n",
    "#     input_shape = [(3, 32, 32)]\n",
    "\n",
    "# input_np = (np.random.uniform(0, 1, (1,)+x) for x in input_shape)\n",
    "# input_tensor = tuple(Variable(torch.FloatTensor(x), requires_grad=False).to(configs[\"device\"]) for x in input_np)\n",
    "# # input_tensor = torch.cat(input_tensor, dim=1)\n",
    "\n",
    "# print(input_tensor.shape)\n",
    "# print(input_tensor[0].shape)\n",
    "# print(len(input_tensor))\n",
    "\n",
    "# print(f'input_tensor: {input_tensor}')\n",
    "\n",
    "model = model.type(torch.float64)\n",
    "\n",
    "# make SplitManagers for split model execution \n",
    "configs['dtype'] = 'float64'\n",
    "split_managers = [SplitManager]*num_machines\n",
    "for i in range(num_machines):\n",
    "    split_managers[i] = SplitManager(configs, i, num_machines, input_tensor)\n",
    "\n",
    "# broadcast input_tensor to different machines\n",
    "# TODO: find a better datastructure for this\n",
    "#input = np.empty((num_machines, num_machines), dtype=torch.Tensor)\n",
    "input = [None]*num_machines\n",
    "input = [input[:] for i in range(num_machines)]\n",
    "for imach in range(num_machines):\n",
    "    input[imach][imach] = input_tensor\n",
    "\n",
    "# print(f'input: {input}')\n",
    "\n",
    "residual_block_start, residual_connection_start, residual_block_end = get_residual_block_indexes(model)\n",
    "\n",
    "# print(f'residual_block_start: {residual_block_start}')\n",
    "# print(f'residual_connection_start: {residual_connection_start}')\n",
    "# print(f'residual_block_end: {residual_block_end}')\n",
    "\n",
    "# put models into eval mode and on device\n",
    "model.eval()\n",
    "model.to(configs['device'])\n",
    "\n",
    "# set index to execute vertical splitting up to \n",
    "exec_to_layer = total_layers_fx-1\n",
    "\n",
    "# get true output at each layer\n",
    "# WARNING: bn layers are initialized differently between model in this notebook and model in split manager. The moving mean and variance fields do not match HOWEVER the weights and biases are the same\n",
    "#horz_output, size_LUT = get_output_at_each_layer(model, input_tensor) \n",
    "horz_output, size_LUT = get_output_at_each_layer(split_managers[0].model, input_tensor)\n",
    "\n",
    "BREAK_LOOP = 0 # break loop when output differs\n",
    "\n",
    "'''\n",
    "    mock run through inference using split models \n",
    "'''\n",
    "\n",
    "# timing\n",
    "split_execution_start_time = time.time()\n",
    "layer_completion_time_stamp = {}\n",
    "layer_execution_duration = {}\n",
    "\n",
    "# make inference \n",
    "with torch.no_grad():\n",
    "    residual_input = {} # use this to keep track of inputs stored in machine memory for residule layers\n",
    "\n",
    "    # iterate through layers 1 module at a time \n",
    "    for imodule in range(exec_to_layer+1):#range(num_total_modules): # 16 <=> layer_1 block \n",
    "\n",
    "        # initialize output for ilayer\n",
    "        #output = np.empty((num_machines, num_machines), dtype=torch.Tensor) # square list indexed as: output[destination/RX machine][origin/TX machine]\n",
    "        # TODO: find a better datastructure for this \n",
    "\n",
    "        output = [None]*num_machines\n",
    "        output = [output[:] for i in range(num_machines)]\n",
    "\n",
    "        # print(f'output: {output}')\n",
    "\n",
    "        # DEBUG\n",
    "        full_input = combine_all_inputs(input, num_machines)\n",
    "\n",
    "        print(f'Executing module {imodule}: {layer_names_fx[imodule]}')\n",
    "\n",
    "        # iterate through each machine (done in parallel later)\n",
    "        for imach in range(num_machines):\n",
    "            print(f'\\tExecuting on machine {imach}')\n",
    "            # print(f'curr_input: {curr_input}')\n",
    "            # print(f'input: {input}')\n",
    "            # collect communication inputs if necessary \n",
    "            if not imodule == 0 and ('conv' in layer_names_fx[imodule-1] or 'linear' in layer_names_fx[imodule-1] or 'shortcut.1' in layer_names_fx[imodule] or 'convShortcut' in layer_names_fx[imodule]): # TODO: this is very hacky, needs to be generalized. The issue is ID'ing conv layers in shortcut blocks\n",
    "                curr_input = combine_inputs(input, num_machines, imach)\n",
    "                # print(f'ilk if e girdi, curr_input: {curr_input}')\n",
    "            else:\n",
    "                curr_input = input[imach][imach]\n",
    "                # print(f'ikinci elif e girdi, curr_input: {curr_input}')\n",
    "\n",
    "            # print(f'len curr_input: {len(curr_input)}')\n",
    "            \n",
    "            out_tensor, do_comms = split_managers[imach].execute_split_layer(curr_input, imodule)\n",
    "            # print(f'out_tensor: {out_tensor}')\n",
    "            if not do_comms:\n",
    "                # update output to current machine and continue\n",
    "                if torch.is_tensor(out_tensor):\n",
    "                    # sometimes out_tensor is None\n",
    "                    # input is sent to all machines for 1st layer execution even though not all machines need to compute \n",
    "                    # Output from machine is None in this case TODO: fix where inputs are sent \n",
    "                    output[imach][imach] = out_tensor\n",
    "                    # print(f'output: {output}')\n",
    "                continue\n",
    "            # print(f'out_tensor: {out_tensor}')\n",
    "            # END SplitManager execute split_layer\n",
    "\n",
    "            print(f'\\t\\t Output tensor shape : {out_tensor.shape}')\n",
    "\n",
    "            # debug\n",
    "            nonzero_out_tensor = torch.unique(torch.nonzero(out_tensor, as_tuple=True)[1])\n",
    "\n",
    "            # look at which C_out need to be computed and sent\n",
    "            #nonzero_Cout = torch.unique(torch.nonzero(split_layer.weight, as_tuple=True)[0]) # find nonzero dimensions in output channels\n",
    "            nonzero_Cout = get_nonzero_channels(out_tensor)\n",
    "            # print(f'\\t\\t\\t nonzero_Cout: {nonzero_Cout}')\n",
    "\n",
    "            # prep communications by populating output\n",
    "            out_channel_array = torch.arange(out_tensor.shape[1])\n",
    "            for rx_mach in range(num_machines):\n",
    "                # only add to output if communication is necessary \n",
    "\n",
    "                # Get output channels for current rx machine? TODO: consider removing, this just maps C_out's to machine\n",
    "                #output_channels = torch.tensor(configs['partition'][][rx_mach],\n",
    "                #        device=torch.device(configs['device']))\n",
    "                output_channels = torch.tensor(split_managers[imach].output_channel_map[rx_mach],\n",
    "                        device=torch.device(configs['device']))\n",
    "                \n",
    "                # print(f'\\t\\t\\t output_channels: {output_channels}')\n",
    "\n",
    "                # TODO: is there a faster way to do this? Consider putting larger array 1st... just not sure which one that'd be\n",
    "                nonzero_out_channels = nonzero_Cout[torch.isin(nonzero_Cout, output_channels)]\n",
    "                if nonzero_out_channels.nelement() > 0:\n",
    "                        communication_mask = torch.isin(out_channel_array, nonzero_out_channels)\n",
    "\n",
    "                        # TODO: this is inefficient, redo. Probbably need to send a tensor and some info what output channels are being sent\n",
    "                        tmp_out = torch.zeros(out_tensor.shape, dtype= split_managers[imach].dtype) \n",
    "                        if imodule == total_layers_fx-1 or 'linear' in layer_names_fx[imodule]:\n",
    "                                tmp_out[:,communication_mask] = out_tensor[:,communication_mask]\n",
    "                        else:\n",
    "                                tmp_out[:,communication_mask,:,:] = out_tensor[:,communication_mask,:,:]\n",
    "                        output[rx_mach][imach] = tmp_out\n",
    "\n",
    "                        # debug\n",
    "                        print(f'\\t\\t sending C_out {nonzero_out_channels} to machine {rx_mach}')\n",
    "\n",
    "        # send to next layer  \n",
    "        input = output\n",
    "        print(f'Finished execution of layer {imodule}')\n",
    "\n",
    "        # update timing\n",
    "        layer_completion_time_stamp[layer_names_fx[imodule]] = time.time()\n",
    "        if imodule > 0:\n",
    "            layer_execution_duration[layer_names_fx[imodule]] = layer_completion_time_stamp[layer_names_fx[imodule]] - layer_completion_time_stamp[layer_names_fx[imodule-1]] \n",
    "        else:\n",
    "            layer_execution_duration[layer_names_fx[imodule]] = layer_completion_time_stamp[layer_names_fx[imodule]] - split_execution_start_time\n",
    "\n",
    "        # check output at end of each layer to see if fit matches \n",
    "        tmp_output = input \n",
    "        # print(f'tmp_output = {tmp_output}')\n",
    "        need_to_init  = True\n",
    "        for rx_mach in range(num_machines):\n",
    "                for tx_mach in range(num_machines):\n",
    "                        if not tmp_output[rx_mach][tx_mach] == None:\n",
    "                                if need_to_init:\n",
    "                                        vert_output = tmp_output[rx_mach][tx_mach]\n",
    "                                        need_to_init = False\n",
    "                                else:\n",
    "                                        # TODO: += causes assignment issues, switched to x = x+y which might be more more inefficent memory wise ... \n",
    "                                        vert_output = vert_output + tmp_output[rx_mach][tx_mach] \n",
    "                                        #nz_channels = get_nonzero_channels(vert_output)\n",
    "                                        #print(f'({rx_mach},{tx_mach}) {nz_channels}')\n",
    "        \n",
    "        if imodule == total_layers_fx-1:\n",
    "            # apply bias\n",
    "            # TODO: assumes Linear layer is final layer and bias can be handled as final step \n",
    "            vert_output = vert_output + get_current_module(model, imodule).bias\n",
    "            \n",
    "            # final execution time\n",
    "            tot_split_execution_time = time.time() - split_execution_start_time\n",
    "            print(f'\\n\\n############################# FINAL EXECUTION TIME {tot_split_execution_time} [seconds] #############################\\n\\n')\n",
    "\n",
    "        truth_output = horz_output[layer_names_fx[imodule]]\n",
    "        # if 'x' == layer_names_fx[imodule] or '_x' == layer_names_fx[imodule] or 'getitem' == layer_names_fx[imodule] or 'getitem_1' == layer_names_fx[imodule] or 'getitem_2' == layer_names_fx[imodule] or 'getitem_3' == layer_names_fx[imodule] or 'getitem_4' == layer_names_fx[imodule] or 'cat' == layer_names_fx[imodule]:\n",
    "        if 'x' == layer_names_fx[imodule]:    \n",
    "            print(f'Input layer. Skipping comparison')\n",
    "        elif torch.is_tensor(truth_output):\n",
    "            max_diff, max_by_Cout = compare_outputs(vert_output, truth_output)\n",
    "            if max_diff > 0.1:\n",
    "                pass\n",
    "                # BREAK_LOOP = 1 \n",
    "        else:\n",
    "            print(f'Horizontal output is {type(truth_output)}. Skipping comparison')\n",
    "        print()\n",
    "\n",
    "        if BREAK_LOOP:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "#     Conv1 layer test -- should we expect some difference, or exactly 0 in the output when we split the model?\n",
    "# '''\n",
    "\n",
    "# # DIFFERENCE SHOULD BE 0 NOT 1E-7\n",
    "\n",
    "# N_in = 1\n",
    "# split_1 = nn.Conv2d(N_in,\n",
    "#             model.conv1.weight.shape[0], # TODO does this need to be an int? (currently tensor)\n",
    "#             kernel_size= model.conv1.kernel_size,\n",
    "#             stride=model.conv1.stride,\n",
    "#             padding=model.conv1.padding, \n",
    "#             bias=False) # TODO: add bias during input collecting step on next layer \n",
    "# split_1.weight = torch.nn.Parameter(model.conv1.weight.index_select(1, torch.tensor([0])))  \n",
    "# out_split1 = split_1(input_tensor.index_select(1, torch.tensor([0])))\n",
    "\n",
    "# split_2 = split_1\n",
    "# split_2.weight = torch.nn.Parameter(model.conv1.weight.index_select(1, torch.tensor([1])))  \n",
    "# out_split2 = split_2(input_tensor.index_select(1, torch.tensor([1])))\n",
    "\n",
    "# split_3 = split_1\n",
    "# split_3.weight = torch.nn.Parameter(model.conv1.weight.index_select(1, torch.tensor([2])))  \n",
    "# out_split3 = split_3(input_tensor.index_select(1, torch.tensor([2])))\n",
    "\n",
    "# split_out = torch.add(torch.add(out_split1, out_split2), out_split3)\n",
    "# full_out = model.conv1(input_tensor)\n",
    "\n",
    "# diff_output = torch.abs(full_out - split_out)\n",
    "# max_diff = torch.max(diff_output)\n",
    "# max_diff.sci_mode = True\n",
    "# print(max_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''  \n",
    "#     Inspect I/O of single layer\n",
    "# '''\n",
    "\n",
    "# t = torch.ones((1,2,2,2), dtype=torch.float32) # (batch, in channel, H, W)\n",
    "# w = torch.ones((1,2,2,2), dtype=torch.float32) # (out channels, in channels, H, W)\n",
    "# w[0,0,0,0] = 1e-10\n",
    "# w[0,0,0,1] = 1e-10\n",
    "# w[0,1,0,0] = 1e-10\n",
    "\n",
    "# full_conv = torch.nn.Conv2d(2,1,kernel_size=(2,2), bias=False, stride=(1),dtype=torch.float32)\n",
    "# full_conv.weight = torch.nn.Parameter(w)\n",
    "# conv1 = torch.nn.Conv2d(1,1,kernel_size=(2,2), bias=False, stride=1, dtype=torch.float32)\n",
    "# conv1.weight =torch.nn.Parameter( w[:,0:1,:,:])\n",
    "# conv2 = torch.nn.Conv2d(1,1,kernel_size=(2,2), bias=False, stride=1,dtype=torch.float32)\n",
    "# conv2.weight = torch.nn.Parameter(w[:,1:2,:,:])\n",
    "\n",
    "# full_conv.eval()\n",
    "# conv1.eval()\n",
    "# conv2.eval()\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     full_out = full_conv(t)\n",
    "#     split_out =  conv2(t[0,1:2,:,:]) + conv1(t[:,0:1,:,:])\n",
    "\n",
    "# diff = torch.abs(full_out - split_out)\n",
    "\n",
    "# torch.nonzero(diff)\n",
    "# #print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test bn1\n",
    "#estimate = -bn1.running_mean[1]/torch.sqrt(bn1.running_var[1] + bn1.eps)*bn1.weight[1] + bn1.bias[1]\n",
    "#estimate_split = -split_layer.running_mean[1]/torch.sqrt(split_layer.running_var[1] + split_layer.eps)*split_layer.weight[1] + split_layer.bias[1]\n",
    "\n",
    "# running estimates are different \n",
    "#bn1.running_mean[1] - split_layer.running_mean[1]\n",
    "#bn1.running_var[1] - split_layer.running_var[1] \n",
    "#bn1.weight[1] - split_layer.weight[1]\n",
    "#bn1.eps - split_layer.eps\n",
    "#bn1.bias[1] - split_layer.bias[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cap_nb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
