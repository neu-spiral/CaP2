{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    Load model and split it\\n        1. layer by layer\\n        2. [TODO] vertically \\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    Load model and split it\n",
    "        1. layer by layer\n",
    "        2. [TODO] vertically \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from source.core.engine import MoP\n",
    "import source.core.run_partition as run_p\n",
    "from os import environ\n",
    "from source.utils.dataset import *\n",
    "from source.utils.misc import *\n",
    "from source.utils.split_network import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "from source.models import resnet\n",
    "from source.core.split_manager import SplitManager\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from source.utils import io\n",
    "from source.utils import testers\n",
    "from source.core import engine\n",
    "import json\n",
    "import itertools\n",
    "\n",
    "from torchvision.models.feature_extraction import create_feature_extractor, get_graph_node_names\n",
    "from torchsummary import summary\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model\n",
    "\n",
    "# model = 'resnet18'\n",
    "# dataset = 'cifar10'\n",
    "\n",
    "# model = 'wrn28_10'\n",
    "# dataset = 'cifar100'\n",
    "\n",
    "# model = 'EscFusion'\n",
    "dataset = 'esc'\n",
    "\n",
    "# model = 'InfoFusionThree'\n",
    "# dataset = 'flash'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# yaml_version = 'v0'\n",
    "# num_partitions = 4\n",
    "# prune_ratio = 0.75\n",
    "\n",
    "# yaml_version = 'v1'\n",
    "# yaml_version = 'v2'\n",
    "# yaml_version = 'v3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device :  cuda:2\n",
      "model :  EscFusion\n",
      "data_code :  esc\n",
      "num_classes :  2\n",
      "epochs :  10\n",
      "batch_size :  32\n",
      "optimizer :  adam\n",
      "lr_scheduler :  cosine\n",
      "learning_rate :  0.01\n",
      "seed :  1234\n",
      "sparsity_type :  kernel\n",
      "prune_ratio :  0.75\n",
      "admm :  True\n",
      "admm_epochs :  300\n",
      "rho :  0.01\n",
      "multi_rho :  True\n",
      "retrain_bs :  128\n",
      "retrain_lr :  0.001\n",
      "retrain_ep :  100\n",
      "retrain_opt :  adam\n",
      "xentropy_weight :  1.0\n",
      "warmup :  False\n",
      "warmup_lr :  0.01\n",
      "warmup_epochs :  10\n",
      "mix_up :  True\n",
      "alpha :  0.3\n",
      "smooth :  False\n",
      "smooth_eps :  0\n",
      "save_last_model_only :  False\n",
      "num_partition :  4\n",
      "layer_type :  regular\n",
      "bn_type :  regular\n",
      "par_first_layer :  True\n",
      "comm_outsize :  True\n",
      "lambda_comm :  10\n",
      "lambda_comp :  0\n",
      "create_partition :  False\n",
      "load_model :  True\n",
      "distill_loss :  kl\n",
      "distill_temp :  30\n",
      "distill_alpha :  1\n"
     ]
    }
   ],
   "source": [
    "# model config\n",
    "\n",
    "environ[\"config\"] = f\"config/{dataset}.yaml\"\n",
    "configs = run_p.main()\n",
    "\n",
    "load_model = f\"{configs['data_code']}-{configs['model']}-{configs['sparsity_type']}-np{configs['num_partition']}-pr{configs['prune_ratio']}-lcm{configs['lambda_comm']}.pt\"\n",
    "\n",
    "# configs = {}\n",
    "# configs[\"num_classes\"] = num_classes\n",
    "# configs[\"data_code\"] = dataset\n",
    "# configs[\"layer_type\"] = layer_type\n",
    "# configs[\"bn_type\"] = bn_type\n",
    "# configs[\"model\"] = model\n",
    "# configs[\"prune_ratio\"] = prune_ratio\n",
    "# configs[\"seed\"] = 1234\n",
    "# configs[\"model_file\"] = load_model\n",
    "\n",
    "# if model == 'resnet18':\n",
    "#     dataset='cifar10'\n",
    "#     load_model = f\"cifar10-resnet18-kernel-np{num_partitions}-pr{prune_ratio}-lcm0.001.pt\"\n",
    "#     num_classes = 10\n",
    "#     layer_type = 'regular'\n",
    "#     bn_type = 'masked'\n",
    "#     model = 'resnet18'\n",
    "# elif model == 'EscFusion':\n",
    "#     dataset='esc'\n",
    "#     load_model = f\"esc-escnet-kernel-np{num_partitions}-pr{prune_ratio}-lcm0.001.pt\"\n",
    "#     num_classes = 2\n",
    "#     layer_type = 'regular'\n",
    "#     bn_type = 'regular'\n",
    "#     model = 'EscFusion'\n",
    "# elif model == 'InfoFusionThree':\n",
    "#     dataset='flash'\n",
    "#     load_model = f\"flash-flashnet-kernel-np{num_partitions}-pr{prune_ratio}-lcm0.001.pt\"\n",
    "# elif model == 'wrn28_10':\n",
    "#     dataset='cifar100'\n",
    "#     load_model = f\"cifar100-wrn28-kernel-np{num_partitions}-pr{prune_ratio}-lcm0.001.pt\"\n",
    "#     num_classes = 100\n",
    "#     layer_type = 'regular'\n",
    "#     bn_type = 'masked'\n",
    "#     model = 'wrn28_10'\n",
    "    \n",
    "\n",
    "configs[\"device\"] = \"cpu\"\n",
    "configs['load_model'] = load_model\n",
    "\n",
    "# if yaml_version == 'v0':\n",
    "#     configs['num_partition'] = num_partitions\n",
    "# else:\n",
    "#     if model == 'resnet18':\n",
    "#         configs[\"num_partition\"] = f'config/resnet18-{yaml_version}.yaml'\n",
    "#     elif model == 'EscFusion':\n",
    "#         configs[\"num_partition\"] = f'config/escnet-{yaml_version}.yaml'\n",
    "#     elif model == 'InfoFusionThree':\n",
    "#         configs[\"num_partition\"] = f'config/flashnet.yaml'\n",
    "#     elif model == 'wrn28_10':\n",
    "#         configs[\"num_partition\"] = f'config/wrn28-{yaml_version}.yaml'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResLike(\n",
      "  (conv1): Conv2d(15, 32, kernel_size=(7, 7), stride=(1, 1), padding=same, bias=False)\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=same, bias=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=same, bias=False)\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=same, bias=False)\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (max_pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (shortcut): Sequential()\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=same, bias=False)\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=same, bias=False)\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (max_pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (shortcut): Sequential()\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (linear1): Linear(in_features=10240, out_features=512, bias=False)\n",
      "  (linear2): Linear(in_features=512, out_features=256, bias=False)\n",
      "  (out): Linear(in_features=256, out_features=2, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (max_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (avg_pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      ")\n",
      "conv1.weight\n",
      "conv2.weight\n",
      "layer1.0.conv1.weight\n",
      "layer1.0.conv2.weight\n",
      "layer1.0.bn1.weight\n",
      "layer1.0.bn1.bias\n",
      "layer1.0.bn2.weight\n",
      "layer1.0.bn2.bias\n",
      "layer2.0.conv1.weight\n",
      "layer2.0.conv2.weight\n",
      "layer2.0.bn1.weight\n",
      "layer2.0.bn1.bias\n",
      "layer2.0.bn2.weight\n",
      "layer2.0.bn2.bias\n",
      "linear1.weight\n",
      "linear2.weight\n",
      "out.weight\n",
      "out.bias\n",
      "bn.weight\n",
      "bn.bias\n"
     ]
    }
   ],
   "source": [
    "# load data and load or train model\n",
    "model = get_model_from_code(configs).to(configs['device']) # grabs model architecture from ./source/models/escnet.py\n",
    "print(model)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load weights into full model\n",
    "state_dict = torch.load(io.get_model_path_split(\"{}\".format(configs[\"load_model\"])), map_location=configs['device'])\n",
    "model = io.load_state_dict(model, \n",
    "                    state_dict['model_state_dict'] if 'model_state_dict' in state_dict \n",
    "                    else state_dict['state_dict'] if 'state_dict' in state_dict else state_dict,)\n",
    "\n",
    "# for name, param in model.named_parameters():\n",
    "#     print(name)\n",
    "#     print(param.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_partition: {'conv1.weight': 4, 'conv2.weight': 4, 'inputs': 4, 'layer1.0.conv1.weight': 4, 'layer1.0.conv2.weight': 4, 'layer2.0.conv1.weight': 4, 'layer2.0.conv2.weight': 4, 'linear1.weight': 4, 'linear2.weight': 4}\n",
      "ratio_partition: {'conv1.weight': [1, 1, 1, 1], 'conv2.weight': [1, 1, 1, 1], 'inputs': [1, 1, 1, 1], 'layer1.0.conv1.weight': [1, 1, 1, 1], 'layer1.0.conv2.weight': [1, 1, 1, 1], 'layer2.0.conv1.weight': [1, 1, 1, 1], 'layer2.0.conv2.weight': [1, 1, 1, 1], 'linear1.weight': [1, 1, 1, 1], 'linear2.weight': [1, 1, 1, 1]}\n",
      "map_partition: {'conv1.weight': [[0, 1, 1, 1], [1, 0, 1, 1], [1, 1, 0, 1], [1, 1, 1, 0]], 'conv2.weight': [[0, 1, 1, 1], [1, 0, 1, 1], [1, 1, 0, 1], [1, 1, 1, 0]], 'inputs': [[0, 1, 1, 1], [1, 0, 1, 1], [1, 1, 0, 1], [1, 1, 1, 0]], 'layer1.0.conv1.weight': [[0, 1, 1, 1], [1, 0, 1, 1], [1, 1, 0, 1], [1, 1, 1, 0]], 'layer1.0.conv2.weight': [[0, 1, 1, 1], [1, 0, 1, 1], [1, 1, 0, 1], [1, 1, 1, 0]], 'layer2.0.conv1.weight': [[0, 1, 1, 1], [1, 0, 1, 1], [1, 1, 0, 1], [1, 1, 1, 0]], 'layer2.0.conv2.weight': [[0, 1, 1, 1], [1, 0, 1, 1], [1, 1, 0, 1], [1, 1, 1, 0]], 'linear1.weight': [[0, 1, 1, 1], [1, 0, 1, 1], [1, 1, 0, 1], [1, 1, 1, 0]], 'linear2.weight': [[0, 1, 1, 1], [1, 0, 1, 1], [1, 1, 0, 1], [1, 1, 1, 0]]}\n",
      "bn_partition: [4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "Inference time per data is 21.237135ms.\n",
      "conv1.weight 85120\n",
      "conv2.weight 1320\n",
      "layer1.0.conv1.weight 21280\n",
      "layer1.0.conv2.weight 21280\n",
      "layer2.0.conv1.weight 5280\n",
      "layer2.0.conv2.weight 5280\n",
      "linear1.weight 1\n",
      "linear2.weight 1\n",
      "Total layers: 32\n",
      "['x', 'conv1', 'bn', 'relu', 'max_pool', 'layer1.0.conv1', 'layer1.0.bn1', 'layer1.0.relu', 'layer1.0.conv2', 'layer1.0.bn2', 'layer1.0.add', 'layer1.0.relu_1', 'layer1.0.max_pool1', 'layer2.0.conv1', 'layer2.0.bn1', 'layer2.0.relu', 'layer2.0.conv2', 'layer2.0.bn2', 'layer2.0.add', 'layer2.0.relu_1', 'layer2.0.max_pool1', 'conv2', 'bn_1', 'relu_1', 'avg_pool', 'size', 'view', 'linear1', 'relu_2', 'linear2', 'relu_3', 'out']\n",
      "num_machines: 4\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    add partitions and communications to configs\n",
    "'''\n",
    "\n",
    "# gets random test input (with correct size)\n",
    "input_var = get_input_from_code(configs)\n",
    "\n",
    "# Config partitions and prune_ratio\n",
    "configs = engine.partition_generator(configs, model)\n",
    "            \n",
    "# Compute output size of each layer\n",
    "configs['partition'] = engine.featuremap_summary(model, configs['partition'], input_var)\n",
    "\n",
    "# print(configs['partition'])\n",
    "        \n",
    "# model communication costs\n",
    "configs['comm_costs'] = engine.set_communication_cost(model, configs['partition'],)\n",
    "\n",
    "# print(configs['comm_costs'])\n",
    "\n",
    "\n",
    "# split model general parameters\n",
    "\n",
    "# make copies of model per machine\n",
    "num_machines = max(configs['partition']['bn_partition']) # TODO: double check this makes sense\n",
    "model_machines = [model]*num_machines\n",
    "\n",
    "layer_names_fx =  get_graph_node_names(model)[1]\n",
    "total_layers_fx = len(layer_names_fx)\n",
    "print(f\"Total layers: {total_layers_fx}\")\n",
    "\n",
    "split_module_names = list(configs['partition'].keys())\n",
    "\n",
    "print(layer_names_fx)\n",
    "print('num_machines:', num_machines)\n",
    "\n",
    "# print(model.layer1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing module 0: x\n",
      "\tExecuting on machine 0\n",
      "\t\t-model input layer.. skipping\n",
      "\tExecuting on machine 1\n",
      "\t\t-model input layer.. skipping\n",
      "\tExecuting on machine 2\n",
      "\t\t-model input layer.. skipping\n",
      "\tExecuting on machine 3\n",
      "\t\t-model input layer.. skipping\n",
      "Finished execution of layer 0\n",
      "Input layer. Skipping comparison\n",
      "\n",
      "Executing module 1: conv1\n",
      "\tExecuting on machine 0\n",
      "\t\t-Splitting conv layer 1\n",
      "\t\t Output tensor shape : torch.Size([1, 32, 266, 320])\n",
      "\t\t sending C_out tensor([0, 1, 2, 3, 4, 5, 6, 7]) to machine 0\n",
      "\tExecuting on machine 1\n",
      "\t\t-Splitting conv layer 1\n",
      "\t\t Output tensor shape : torch.Size([1, 32, 266, 320])\n",
      "\t\t sending C_out tensor([ 8,  9, 10, 11, 12, 13, 14, 15]) to machine 1\n",
      "\tExecuting on machine 2\n",
      "\t\t-Splitting conv layer 1\n",
      "\t\t Output tensor shape : torch.Size([1, 32, 266, 320])\n",
      "\t\t sending C_out tensor([16, 17, 18, 19, 20, 21, 22, 23]) to machine 2\n",
      "\tExecuting on machine 3\n",
      "\t\t-Splitting conv layer 1\n",
      "\t\t Output tensor shape : torch.Size([1, 32, 266, 320])\n",
      "\t\t sending C_out tensor([24, 25, 26, 27, 28, 29, 30, 31]) to machine 3\n",
      "Finished execution of layer 1\n",
      "Max diff:\n",
      "tensor([0.], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([], dtype=torch.int64)  (len = 0)\n",
      "passing Cout = tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31])  (len = 32)\n",
      "\n",
      "Executing module 2: bn\n",
      "\tExecuting on machine 0\n",
      "\t\t-Splitting batch norm layer 2\n",
      "\t\t Output tensor shape : torch.Size([1, 32, 266, 320])\n",
      "\t\t sending C_out tensor([0, 1, 2, 3, 4, 5, 6, 7]) to machine 0\n",
      "\tExecuting on machine 1\n",
      "\t\t-Splitting batch norm layer 2\n",
      "\t\t Output tensor shape : torch.Size([1, 32, 266, 320])\n",
      "\t\t sending C_out tensor([ 8,  9, 10, 11, 12, 13, 14, 15]) to machine 1\n",
      "\tExecuting on machine 2\n",
      "\t\t-Splitting batch norm layer 2\n",
      "\t\t Output tensor shape : torch.Size([1, 32, 266, 320])\n",
      "\t\t sending C_out tensor([16, 17, 18, 19, 20, 21, 22, 23]) to machine 2\n",
      "\tExecuting on machine 3\n",
      "\t\t-Splitting batch norm layer 2\n",
      "\t\t Output tensor shape : torch.Size([1, 32, 266, 320])\n",
      "\t\t sending C_out tensor([24, 25, 26, 27, 28, 29, 30, 31]) to machine 3\n",
      "Finished execution of layer 2\n",
      "Max diff:\n",
      "tensor([0.], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([], dtype=torch.int64)  (len = 0)\n",
      "passing Cout = tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31])  (len = 32)\n",
      "\n",
      "Executing module 3: relu\n",
      "\tExecuting on machine 0\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 1\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 2\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 3\n",
      "\t\t-Applying ReLU\n",
      "Finished execution of layer 3\n",
      "Max diff:\n",
      "tensor([0.], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([], dtype=torch.int64)  (len = 0)\n",
      "passing Cout = tensor([ 1, 10])  (len = 2)\n",
      "\n",
      "Executing module 4: max_pool\n",
      "\tExecuting on machine 0\n",
      "\t\t-max pooling\n",
      "\tExecuting on machine 1\n",
      "\t\t-max pooling\n",
      "\tExecuting on machine 2\n",
      "\t\t-max pooling\n",
      "\tExecuting on machine 3\n",
      "\t\t-max pooling\n",
      "Finished execution of layer 4\n",
      "Max diff:\n",
      "tensor([0.], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([], dtype=torch.int64)  (len = 0)\n",
      "passing Cout = tensor([ 1, 10])  (len = 2)\n",
      "\n",
      "Executing module 5: layer1.0.conv1\n",
      "\tExecuting on machine 0\n",
      "\t\t-Saving input for later...\n",
      "\t\t-Splitting conv layer 5\n",
      "\t\t Output tensor shape : torch.Size([1, 32, 133, 160])\n",
      "\t\t sending C_out tensor([0, 1, 2, 3, 4, 5, 6, 7]) to machine 0\n",
      "\tExecuting on machine 1\n",
      "\t\t-Saving input for later...\n",
      "\t\t-Splitting conv layer 5\n",
      "\t\t Output tensor shape : torch.Size([1, 32, 133, 160])\n",
      "\t\t sending C_out tensor([ 8,  9, 10, 11, 12, 13, 14, 15]) to machine 1\n",
      "\tExecuting on machine 2\n",
      "\t\t-Saving input for later...\n",
      "\t\t-Splitting conv layer 5\n",
      "\t\t Output tensor shape : torch.Size([1, 32, 133, 160])\n",
      "\tExecuting on machine 3\n",
      "\t\t-Saving input for later...\n",
      "\t\t-Splitting conv layer 5\n",
      "\t\t Output tensor shape : torch.Size([1, 32, 133, 160])\n",
      "Finished execution of layer 5\n",
      "Max diff:\n",
      "tensor([0.], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([], dtype=torch.int64)  (len = 0)\n",
      "passing Cout = tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15])  (len = 16)\n",
      "\n",
      "Executing module 6: layer1.0.bn1\n",
      "\tExecuting on machine 0\n",
      "\t\t-Splitting batch norm layer 6\n",
      "\t\t Output tensor shape : torch.Size([1, 32, 133, 160])\n",
      "\t\t sending C_out tensor([0, 1, 2, 3, 4, 5, 6, 7]) to machine 0\n",
      "\tExecuting on machine 1\n",
      "\t\t-Splitting batch norm layer 6\n",
      "\t\t Output tensor shape : torch.Size([1, 32, 133, 160])\n",
      "\t\t sending C_out tensor([ 8,  9, 10, 11, 12, 13, 14, 15]) to machine 1\n",
      "\tExecuting on machine 2\n",
      "\t\t-No input received but bn still needs to produce output.\n",
      "\t\t-Splitting batch norm layer 6\n",
      "\t\t Output tensor shape : torch.Size([1, 32, 133, 160])\n",
      "\t\t sending C_out tensor([16, 17, 18, 19, 20, 21, 22, 23]) to machine 2\n",
      "\tExecuting on machine 3\n",
      "\t\t-No input received but bn still needs to produce output.\n",
      "\t\t-Splitting batch norm layer 6\n",
      "\t\t Output tensor shape : torch.Size([1, 32, 133, 160])\n",
      "\t\t sending C_out tensor([24, 25, 26, 27, 28, 29, 30, 31]) to machine 3\n",
      "Finished execution of layer 6\n",
      "Max diff:\n",
      "tensor([0.], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([], dtype=torch.int64)  (len = 0)\n",
      "passing Cout = tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31])  (len = 32)\n",
      "\n",
      "Executing module 7: layer1.0.relu\n",
      "\tExecuting on machine 0\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 1\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 2\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 3\n",
      "\t\t-Applying ReLU\n",
      "Finished execution of layer 7\n",
      "Max diff:\n",
      "tensor([0.], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([], dtype=torch.int64)  (len = 0)\n",
      "passing Cout = tensor([ 0,  2,  7, 16, 18, 21, 22, 23, 25, 26, 28, 29])  (len = 12)\n",
      "\n",
      "Executing module 8: layer1.0.conv2\n",
      "\tExecuting on machine 0\n",
      "\t\t-Splitting conv layer 8\n",
      "\t\t Output tensor shape : torch.Size([1, 32, 133, 160])\n",
      "\t\t sending C_out tensor([0, 1, 2, 3, 4, 5, 6, 7]) to machine 0\n",
      "\tExecuting on machine 1\n",
      "\t\t-Splitting conv layer 8\n",
      "\t\t Output tensor shape : torch.Size([1, 32, 133, 160])\n",
      "\tExecuting on machine 2\n",
      "\t\t-Splitting conv layer 8\n",
      "\t\t Output tensor shape : torch.Size([1, 32, 133, 160])\n",
      "\t\t sending C_out tensor([16, 17, 18, 19, 20, 21, 22, 23]) to machine 2\n",
      "\tExecuting on machine 3\n",
      "\t\t-Splitting conv layer 8\n",
      "\t\t Output tensor shape : torch.Size([1, 32, 133, 160])\n",
      "\t\t sending C_out tensor([24, 25, 26, 27, 28, 29, 30, 31]) to machine 3\n",
      "Finished execution of layer 8\n",
      "Max diff:\n",
      "tensor([0.], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([], dtype=torch.int64)  (len = 0)\n",
      "passing Cout = tensor([ 0,  1,  2,  3,  4,  5,  6,  7, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25,\n",
      "        26, 27, 28, 29, 30, 31])  (len = 24)\n",
      "\n",
      "Executing module 9: layer1.0.bn2\n",
      "\tExecuting on machine 0\n",
      "\t\t-Splitting batch norm layer 9\n",
      "\t\t Output tensor shape : torch.Size([1, 32, 133, 160])\n",
      "\t\t sending C_out tensor([0, 1, 2, 3, 4, 5, 6, 7]) to machine 0\n",
      "\tExecuting on machine 1\n",
      "\t\t-No input received but bn still needs to produce output.\n",
      "\t\t-Splitting batch norm layer 9\n",
      "\t\t Output tensor shape : torch.Size([1, 32, 133, 160])\n",
      "\t\t sending C_out tensor([ 8,  9, 10, 11, 12, 13, 14, 15]) to machine 1\n",
      "\tExecuting on machine 2\n",
      "\t\t-Splitting batch norm layer 9\n",
      "\t\t Output tensor shape : torch.Size([1, 32, 133, 160])\n",
      "\t\t sending C_out tensor([16, 17, 18, 19, 20, 21, 22, 23]) to machine 2\n",
      "\tExecuting on machine 3\n",
      "\t\t-Splitting batch norm layer 9\n",
      "\t\t Output tensor shape : torch.Size([1, 32, 133, 160])\n",
      "\t\t sending C_out tensor([24, 25, 26, 27, 28, 29, 30, 31]) to machine 3\n",
      "Finished execution of layer 9\n",
      "Max diff:\n",
      "tensor([0.], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([], dtype=torch.int64)  (len = 0)\n",
      "passing Cout = tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31])  (len = 32)\n",
      "\n",
      "Executing module 10: layer1.0.add\n",
      "\tExecuting on machine 0\n",
      "\t\t-adding residual\n",
      "\t\t-assuming shortcut had no layers\n",
      "\tExecuting on machine 1\n",
      "\t\t-adding residual\n",
      "\t\t-assuming shortcut had no layers\n",
      "\tExecuting on machine 2\n",
      "\t\t-adding residual\n",
      "\t\t-assuming shortcut had no layers\n",
      "\tExecuting on machine 3\n",
      "\t\t-adding residual\n",
      "\t\t-assuming shortcut had no layers\n",
      "Finished execution of layer 10\n",
      "Max diff:\n",
      "tensor([0.], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([], dtype=torch.int64)  (len = 0)\n",
      "passing Cout = tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31])  (len = 32)\n",
      "\n",
      "Executing module 11: layer1.0.relu_1\n",
      "\tExecuting on machine 0\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 1\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 2\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 3\n",
      "\t\t-Applying ReLU\n",
      "Finished execution of layer 11\n",
      "Max diff:\n",
      "tensor([0.], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([], dtype=torch.int64)  (len = 0)\n",
      "passing Cout = tensor([10, 11, 18, 20, 22, 23])  (len = 6)\n",
      "\n",
      "Executing module 12: layer1.0.max_pool1\n",
      "\tExecuting on machine 0\n",
      "\t\t-max pooling\n",
      "\tExecuting on machine 1\n",
      "\t\t-max pooling\n",
      "\tExecuting on machine 2\n",
      "\t\t-max pooling\n",
      "\tExecuting on machine 3\n",
      "\t\t-max pooling\n",
      "Finished execution of layer 12\n",
      "Max diff:\n",
      "tensor([0.], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([], dtype=torch.int64)  (len = 0)\n",
      "passing Cout = tensor([10, 11, 18, 20, 22, 23])  (len = 6)\n",
      "\n",
      "Executing module 13: layer2.0.conv1\n",
      "\tExecuting on machine 0\n",
      "\t\t-Saving input for later...\n",
      "\t\t-Splitting conv layer 13\n",
      "\t\t Output tensor shape : torch.Size([1, 32, 66, 80])\n",
      "\tExecuting on machine 1\n",
      "\t\t-Saving input for later...\n",
      "\t\t-Splitting conv layer 13\n",
      "\t\t Output tensor shape : torch.Size([1, 32, 66, 80])\n",
      "\t\t sending C_out tensor([ 8,  9, 10, 11, 12, 13, 14, 15]) to machine 1\n",
      "\tExecuting on machine 2\n",
      "\t\t-Saving input for later...\n",
      "\t\t-Splitting conv layer 13\n",
      "\t\t Output tensor shape : torch.Size([1, 32, 66, 80])\n",
      "\t\t sending C_out tensor([16, 17, 18, 19, 20, 21, 22, 23]) to machine 2\n",
      "\tExecuting on machine 3\n",
      "\t\t-Saving input for later...\n",
      "\t\t-Splitting conv layer 13\n",
      "\t\t Output tensor shape : torch.Size([1, 32, 66, 80])\n",
      "Finished execution of layer 13\n",
      "Max diff:\n",
      "tensor([0.], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([], dtype=torch.int64)  (len = 0)\n",
      "passing Cout = tensor([ 8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23])  (len = 16)\n",
      "\n",
      "Executing module 14: layer2.0.bn1\n",
      "\tExecuting on machine 0\n",
      "\t\t-No input received but bn still needs to produce output.\n",
      "\t\t-Splitting batch norm layer 14\n",
      "\t\t Output tensor shape : torch.Size([1, 32, 66, 80])\n",
      "\t\t sending C_out tensor([0, 1, 2, 3, 4, 5, 6, 7]) to machine 0\n",
      "\tExecuting on machine 1\n",
      "\t\t-Splitting batch norm layer 14\n",
      "\t\t Output tensor shape : torch.Size([1, 32, 66, 80])\n",
      "\t\t sending C_out tensor([ 8,  9, 10, 11, 12, 13, 14, 15]) to machine 1\n",
      "\tExecuting on machine 2\n",
      "\t\t-Splitting batch norm layer 14\n",
      "\t\t Output tensor shape : torch.Size([1, 32, 66, 80])\n",
      "\t\t sending C_out tensor([16, 17, 18, 19, 20, 21, 22, 23]) to machine 2\n",
      "\tExecuting on machine 3\n",
      "\t\t-No input received but bn still needs to produce output.\n",
      "\t\t-Splitting batch norm layer 14\n",
      "\t\t Output tensor shape : torch.Size([1, 32, 66, 80])\n",
      "\t\t sending C_out tensor([24, 25, 26, 27, 28, 29, 30, 31]) to machine 3\n",
      "Finished execution of layer 14\n",
      "Max diff:\n",
      "tensor([0.], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([], dtype=torch.int64)  (len = 0)\n",
      "passing Cout = tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31])  (len = 32)\n",
      "\n",
      "Executing module 15: layer2.0.relu\n",
      "\tExecuting on machine 0\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 1\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 2\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 3\n",
      "\t\t-Applying ReLU\n",
      "Finished execution of layer 15\n",
      "Max diff:\n",
      "tensor([0.], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([], dtype=torch.int64)  (len = 0)\n",
      "passing Cout = tensor([ 1,  3,  4,  5,  7,  9, 11, 14, 19, 23, 27, 28])  (len = 12)\n",
      "\n",
      "Executing module 16: layer2.0.conv2\n",
      "\tExecuting on machine 0\n",
      "\t\t-Splitting conv layer 16\n",
      "\t\t Output tensor shape : torch.Size([1, 32, 66, 80])\n",
      "\t\t sending C_out tensor([0, 1, 2, 3, 4, 5, 6, 7]) to machine 0\n",
      "\tExecuting on machine 1\n",
      "\t\t-Splitting conv layer 16\n",
      "\t\t Output tensor shape : torch.Size([1, 32, 66, 80])\n",
      "\t\t sending C_out tensor([ 8,  9, 10, 11, 12, 13, 14, 15]) to machine 1\n",
      "\tExecuting on machine 2\n",
      "\t\t-Splitting conv layer 16\n",
      "\t\t Output tensor shape : torch.Size([1, 32, 66, 80])\n",
      "\t\t sending C_out tensor([16, 17, 18, 19, 20, 21, 22, 23]) to machine 2\n",
      "\tExecuting on machine 3\n",
      "\t\t-Splitting conv layer 16\n",
      "\t\t Output tensor shape : torch.Size([1, 32, 66, 80])\n",
      "\t\t sending C_out tensor([24, 25, 26, 27, 28, 29, 30, 31]) to machine 3\n",
      "Finished execution of layer 16\n",
      "Max diff:\n",
      "tensor([0.], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([], dtype=torch.int64)  (len = 0)\n",
      "passing Cout = tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31])  (len = 32)\n",
      "\n",
      "Executing module 17: layer2.0.bn2\n",
      "\tExecuting on machine 0\n",
      "\t\t-Splitting batch norm layer 17\n",
      "\t\t Output tensor shape : torch.Size([1, 32, 66, 80])\n",
      "\t\t sending C_out tensor([0, 1, 2, 3, 4, 5, 6, 7]) to machine 0\n",
      "\tExecuting on machine 1\n",
      "\t\t-Splitting batch norm layer 17\n",
      "\t\t Output tensor shape : torch.Size([1, 32, 66, 80])\n",
      "\t\t sending C_out tensor([ 8,  9, 10, 11, 12, 13, 14, 15]) to machine 1\n",
      "\tExecuting on machine 2\n",
      "\t\t-Splitting batch norm layer 17\n",
      "\t\t Output tensor shape : torch.Size([1, 32, 66, 80])\n",
      "\t\t sending C_out tensor([16, 17, 18, 19, 20, 21, 22, 23]) to machine 2\n",
      "\tExecuting on machine 3\n",
      "\t\t-Splitting batch norm layer 17\n",
      "\t\t Output tensor shape : torch.Size([1, 32, 66, 80])\n",
      "\t\t sending C_out tensor([24, 25, 26, 27, 28, 29, 30, 31]) to machine 3\n",
      "Finished execution of layer 17\n",
      "Max diff:\n",
      "tensor([0.], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([], dtype=torch.int64)  (len = 0)\n",
      "passing Cout = tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31])  (len = 32)\n",
      "\n",
      "Executing module 18: layer2.0.add\n",
      "\tExecuting on machine 0\n",
      "\t\t-adding residual\n",
      "\t\t-assuming shortcut had no layers\n",
      "\tExecuting on machine 1\n",
      "\t\t-adding residual\n",
      "\t\t-assuming shortcut had no layers\n",
      "\tExecuting on machine 2\n",
      "\t\t-adding residual\n",
      "\t\t-assuming shortcut had no layers\n",
      "\tExecuting on machine 3\n",
      "\t\t-adding residual\n",
      "\t\t-assuming shortcut had no layers\n",
      "Finished execution of layer 18\n",
      "Max diff:\n",
      "tensor([0.], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([], dtype=torch.int64)  (len = 0)\n",
      "passing Cout = tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31])  (len = 32)\n",
      "\n",
      "Executing module 19: layer2.0.relu_1\n",
      "\tExecuting on machine 0\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 1\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 2\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 3\n",
      "\t\t-Applying ReLU\n",
      "Finished execution of layer 19\n",
      "Max diff:\n",
      "tensor([0.], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([], dtype=torch.int64)  (len = 0)\n",
      "passing Cout = tensor([ 1,  2,  6, 10, 11, 12, 14, 15, 18, 19, 20, 23, 24, 25, 26, 29])  (len = 16)\n",
      "\n",
      "Executing module 20: layer2.0.max_pool1\n",
      "\tExecuting on machine 0\n",
      "\t\t-max pooling\n",
      "\tExecuting on machine 1\n",
      "\t\t-max pooling\n",
      "\tExecuting on machine 2\n",
      "\t\t-max pooling\n",
      "\tExecuting on machine 3\n",
      "\t\t-max pooling\n",
      "Finished execution of layer 20\n",
      "Max diff:\n",
      "tensor([0.], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([], dtype=torch.int64)  (len = 0)\n",
      "passing Cout = tensor([ 1,  2,  6, 10, 11, 12, 14, 15, 18, 19, 20, 23, 24, 25, 26, 29])  (len = 16)\n",
      "\n",
      "Executing module 21: conv2\n",
      "\tExecuting on machine 0\n",
      "\t\t-Saving input for later...\n",
      "\t\t-Splitting conv layer 21\n",
      "\t\t Output tensor shape : torch.Size([1, 32, 33, 40])\n",
      "\t\t sending C_out tensor([0, 1, 2, 3, 4, 5, 6, 7]) to machine 0\n",
      "\tExecuting on machine 1\n",
      "\t\t-Saving input for later...\n",
      "\t\t-Splitting conv layer 21\n",
      "\t\t Output tensor shape : torch.Size([1, 32, 33, 40])\n",
      "\t\t sending C_out tensor([ 8,  9, 10, 11, 12, 13, 14, 15]) to machine 1\n",
      "\tExecuting on machine 2\n",
      "\t\t-Saving input for later...\n",
      "\t\t-Splitting conv layer 21\n",
      "\t\t Output tensor shape : torch.Size([1, 32, 33, 40])\n",
      "\t\t sending C_out tensor([16, 17, 18, 19, 20, 21, 22, 23]) to machine 2\n",
      "\tExecuting on machine 3\n",
      "\t\t-Saving input for later...\n",
      "\t\t-Splitting conv layer 21\n",
      "\t\t Output tensor shape : torch.Size([1, 32, 33, 40])\n",
      "\t\t sending C_out tensor([24, 25, 26, 27, 28, 29, 30, 31]) to machine 3\n",
      "Finished execution of layer 21\n",
      "Max diff:\n",
      "tensor([0.], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([], dtype=torch.int64)  (len = 0)\n",
      "passing Cout = tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31])  (len = 32)\n",
      "\n",
      "Executing module 22: bn_1\n",
      "\tExecuting on machine 0\n",
      "\t\t-Splitting batch norm layer 22\n",
      "\t\t Output tensor shape : torch.Size([1, 32, 33, 40])\n",
      "\t\t sending C_out tensor([0, 1, 2, 3, 4, 5, 6, 7]) to machine 0\n",
      "\tExecuting on machine 1\n",
      "\t\t-Splitting batch norm layer 22\n",
      "\t\t Output tensor shape : torch.Size([1, 32, 33, 40])\n",
      "\t\t sending C_out tensor([ 8,  9, 10, 11, 12, 13, 14, 15]) to machine 1\n",
      "\tExecuting on machine 2\n",
      "\t\t-Splitting batch norm layer 22\n",
      "\t\t Output tensor shape : torch.Size([1, 32, 33, 40])\n",
      "\t\t sending C_out tensor([16, 17, 18, 19, 20, 21, 22, 23]) to machine 2\n",
      "\tExecuting on machine 3\n",
      "\t\t-Splitting batch norm layer 22\n",
      "\t\t Output tensor shape : torch.Size([1, 32, 33, 40])\n",
      "\t\t sending C_out tensor([24, 25, 26, 27, 28, 29, 30, 31]) to machine 3\n",
      "Finished execution of layer 22\n",
      "Max diff:\n",
      "tensor([0.], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([], dtype=torch.int64)  (len = 0)\n",
      "passing Cout = tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31])  (len = 32)\n",
      "\n",
      "Executing module 23: relu_1\n",
      "\tExecuting on machine 0\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 1\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 2\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 3\n",
      "\t\t-Applying ReLU\n",
      "Finished execution of layer 23\n",
      "Max diff:\n",
      "tensor([0.], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([], dtype=torch.int64)  (len = 0)\n",
      "passing Cout = tensor([10, 29, 30])  (len = 3)\n",
      "\n",
      "Executing module 24: avg_pool\n",
      "\tExecuting on machine 0\n",
      "\t\t-average pooling\n",
      "\tExecuting on machine 1\n",
      "\t\t-average pooling\n",
      "\tExecuting on machine 2\n",
      "\t\t-average pooling\n",
      "\tExecuting on machine 3\n",
      "\t\t-average pooling\n",
      "Finished execution of layer 24\n",
      "Max diff:\n",
      "tensor([0.], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([], dtype=torch.int64)  (len = 0)\n",
      "passing Cout = tensor([10, 29, 30])  (len = 3)\n",
      "\n",
      "Executing module 25: size\n",
      "\tExecuting on machine 0\n",
      "\t\t-skipping\n",
      "\tExecuting on machine 1\n",
      "\t\t-skipping\n",
      "\tExecuting on machine 2\n",
      "\t\t-skipping\n",
      "\tExecuting on machine 3\n",
      "\t\t-skipping\n",
      "Finished execution of layer 25\n",
      "Horizontal output is <class 'int'>. Skipping comparison\n",
      "\n",
      "Executing module 26: view\n",
      "\tExecuting on machine 0\n",
      "\t\t-reshaping (view)\n",
      "\tExecuting on machine 1\n",
      "\t\t-reshaping (view)\n",
      "\tExecuting on machine 2\n",
      "\t\t-reshaping (view)\n",
      "\tExecuting on machine 3\n",
      "\t\t-reshaping (view)\n",
      "Finished execution of layer 26\n",
      "Max diff:\n",
      "tensor([0.], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([], dtype=torch.int64)  (len = 0)\n",
      "passing Cout = tensor([3200, 3201, 3202, 3203, 3204, 3205, 3206, 3207, 3208, 3209, 3210, 3211,\n",
      "        3212, 3213, 3214, 3215, 3216, 3217, 3218, 3219, 3220, 3221, 3222, 3223,\n",
      "        3224, 3225, 3226, 3227, 3228, 3229, 3230, 3231, 3232, 3233, 3234, 3235,\n",
      "        3236, 3237, 3238, 3239, 3240, 3241, 3242, 3243, 3244, 3245, 3246, 3247,\n",
      "        3248, 3249, 3250, 3251, 3252, 3253, 3254, 3255, 3256, 3257, 3258, 3259,\n",
      "        3260, 3261, 3262, 3263, 3264, 3265, 3266, 3267, 3268, 3269, 3270, 3271,\n",
      "        3272, 3273, 3274, 3275, 3276, 3277, 3278, 3279, 3280, 3281, 3282, 3283,\n",
      "        3284, 3285, 3286, 3287, 3288, 3289, 3290, 3291, 3292, 3293, 3294, 3295,\n",
      "        3296, 3297, 3298, 3299, 3300, 3301, 3302, 3303, 3304, 3305, 3306, 3307,\n",
      "        3308, 3309, 3310, 3311, 3312, 3313, 3314, 3315, 3316, 3317, 3318, 3319,\n",
      "        3320, 3321, 3322, 3323, 3324, 3325, 3326, 3327, 3328, 3329, 3330, 3331,\n",
      "        3332, 3333, 3334, 3335, 3336, 3337, 3338, 3339, 3340, 3341, 3342, 3343,\n",
      "        3344, 3345, 3346, 3347, 3348, 3349, 3350, 3351, 3352, 3353, 3354, 3355,\n",
      "        3356, 3357, 3358, 3359, 3360, 3361, 3362, 3363, 3364, 3365, 3366, 3367,\n",
      "        3368, 3369, 3370, 3371, 3372, 3373, 3374, 3375, 3376, 3377, 3378, 3379,\n",
      "        3380, 3381, 3382, 3383, 3384, 3385, 3386, 3387, 3388, 3389, 3390, 3391,\n",
      "        3392, 3393, 3394, 3395, 3396, 3397, 3398, 3399, 3400, 3401, 3402, 3403,\n",
      "        3404, 3405, 3406, 3407, 3408, 3409, 3410, 3411, 3412, 3413, 3414, 3415,\n",
      "        3416, 3417, 3418, 3419, 3420, 3421, 3422, 3423, 3424, 3425, 3426, 3427,\n",
      "        3428, 3429, 3430, 3431, 3432, 3433, 3434, 3435, 3436, 3437, 3438, 3439,\n",
      "        3440, 3441, 3442, 3443, 3444, 3445, 3446, 3447, 3448, 3449, 3450, 3451,\n",
      "        3452, 3453, 3454, 3455, 3456, 3457, 3458, 3459, 3460, 3461, 3462, 3463,\n",
      "        3464, 3465, 3466, 3467, 3468, 3469, 3470, 3471, 3472, 3473, 3474, 3475,\n",
      "        3476, 3477, 3478, 3479, 3480, 3481, 3482, 3483, 3484, 3485, 3486, 3487,\n",
      "        3488, 3489, 3490, 3491, 3492, 3493, 3494, 3495, 3496, 3497, 3498, 3499,\n",
      "        3500, 3501, 3502, 3503, 3504, 3505, 3506, 3507, 3508, 3509, 3510, 3511,\n",
      "        3512, 3513, 3514, 3515, 3516, 3517, 3518, 3519, 9280, 9281, 9282, 9283,\n",
      "        9284, 9285, 9286, 9287, 9288, 9289, 9290, 9291, 9292, 9293, 9294, 9295,\n",
      "        9296, 9297, 9298, 9299, 9300, 9301, 9302, 9303, 9304, 9305, 9306, 9307,\n",
      "        9308, 9309, 9310, 9311, 9312, 9313, 9314, 9315, 9316, 9317, 9318, 9319,\n",
      "        9320, 9321, 9322, 9323, 9324, 9325, 9326, 9327, 9328, 9329, 9330, 9331,\n",
      "        9332, 9333, 9334, 9335, 9336, 9337, 9338, 9339, 9340, 9341, 9342, 9343,\n",
      "        9344, 9345, 9346, 9347, 9348, 9349, 9350, 9351, 9352, 9353, 9354, 9355,\n",
      "        9356, 9357, 9358, 9359, 9360, 9361, 9362, 9363, 9364, 9365, 9366, 9367,\n",
      "        9368, 9369, 9370, 9371, 9372, 9373, 9374, 9375, 9376, 9377, 9378, 9379,\n",
      "        9380, 9381, 9382, 9383, 9384, 9385, 9386, 9387, 9388, 9389, 9390, 9391,\n",
      "        9392, 9393, 9394, 9395, 9396, 9397, 9398, 9399, 9400, 9401, 9402, 9403,\n",
      "        9404, 9405, 9406, 9407, 9408, 9409, 9410, 9411, 9412, 9413, 9414, 9415,\n",
      "        9416, 9417, 9418, 9419, 9420, 9421, 9422, 9423, 9424, 9425, 9426, 9427,\n",
      "        9428, 9429, 9430, 9431, 9432, 9433, 9434, 9435, 9436, 9437, 9438, 9439,\n",
      "        9440, 9441, 9442, 9443, 9444, 9445, 9446, 9447, 9448, 9449, 9450, 9451,\n",
      "        9452, 9453, 9454, 9455, 9456, 9457, 9458, 9459, 9460, 9461, 9462, 9463,\n",
      "        9464, 9465, 9466, 9467, 9468, 9469, 9470, 9471, 9472, 9473, 9474, 9475,\n",
      "        9476, 9477, 9478, 9479, 9480, 9481, 9482, 9483, 9484, 9485, 9486, 9487,\n",
      "        9488, 9489, 9490, 9491, 9492, 9493, 9494, 9495, 9496, 9497, 9498, 9499,\n",
      "        9500, 9501, 9502, 9503, 9504, 9505, 9506, 9507, 9508, 9509, 9510, 9511,\n",
      "        9512, 9513, 9514, 9515, 9516, 9517, 9518, 9519, 9520, 9521, 9522, 9523,\n",
      "        9524, 9525, 9526, 9527, 9528, 9529, 9530, 9531, 9532, 9533, 9534, 9535,\n",
      "        9536, 9537, 9538, 9539, 9540, 9541, 9542, 9543, 9544, 9545, 9546, 9547,\n",
      "        9548, 9549, 9550, 9551, 9552, 9553, 9554, 9555, 9556, 9557, 9558, 9559,\n",
      "        9560, 9561, 9562, 9563, 9564, 9565, 9566, 9567, 9568, 9569, 9570, 9571,\n",
      "        9572, 9573, 9574, 9575, 9576, 9577, 9578, 9579, 9580, 9581, 9582, 9583,\n",
      "        9584, 9585, 9586, 9587, 9588, 9589, 9590, 9591, 9592, 9593, 9594, 9595,\n",
      "        9596, 9597, 9598, 9599, 9600, 9601, 9602, 9603, 9604, 9605, 9606, 9607,\n",
      "        9608, 9609, 9610, 9611, 9612, 9613, 9614, 9615, 9616, 9617, 9618, 9619,\n",
      "        9620, 9639, 9640, 9659, 9660, 9679, 9680, 9699, 9700, 9719, 9720, 9739,\n",
      "        9740, 9759, 9760, 9779, 9780, 9799, 9800, 9819, 9820, 9839, 9840, 9859,\n",
      "        9860, 9879, 9880, 9899, 9900, 9919])  (len = 690)\n",
      "\n",
      "Executing module 27: linear1\n",
      "\tExecuting on machine 0\n",
      "\t\t-Splitting linear layer 27\n",
      "\t\t Output tensor shape : torch.Size([1, 512])\n",
      "\tExecuting on machine 1\n",
      "\t\t-Splitting linear layer 27\n",
      "\t\t Output tensor shape : torch.Size([1, 512])\n",
      "\t\t sending C_out tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127]) to machine 0\n",
      "\t\t sending C_out tensor([128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141,\n",
      "        142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
      "        156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169,\n",
      "        170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183,\n",
      "        184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197,\n",
      "        198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211,\n",
      "        212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225,\n",
      "        226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239,\n",
      "        240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253,\n",
      "        254, 255]) to machine 1\n",
      "\t\t sending C_out tensor([256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269,\n",
      "        270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283,\n",
      "        284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297,\n",
      "        298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311,\n",
      "        312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325,\n",
      "        326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339,\n",
      "        340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353,\n",
      "        354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367,\n",
      "        368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381,\n",
      "        382, 383]) to machine 2\n",
      "\t\t sending C_out tensor([384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397,\n",
      "        398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411,\n",
      "        412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425,\n",
      "        426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439,\n",
      "        440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453,\n",
      "        454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467,\n",
      "        468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481,\n",
      "        482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495,\n",
      "        496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509,\n",
      "        510, 511]) to machine 3\n",
      "\tExecuting on machine 2\n",
      "\t\t-Splitting linear layer 27\n",
      "\t\t Output tensor shape : torch.Size([1, 512])\n",
      "\tExecuting on machine 3\n",
      "\t\t-Splitting linear layer 27\n",
      "\t\t Output tensor shape : torch.Size([1, 512])\n",
      "\t\t sending C_out tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127]) to machine 0\n",
      "\t\t sending C_out tensor([128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141,\n",
      "        142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
      "        156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169,\n",
      "        170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183,\n",
      "        184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197,\n",
      "        198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211,\n",
      "        212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225,\n",
      "        226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239,\n",
      "        240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253,\n",
      "        254, 255]) to machine 1\n",
      "\t\t sending C_out tensor([256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269,\n",
      "        270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283,\n",
      "        284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297,\n",
      "        298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311,\n",
      "        312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325,\n",
      "        326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339,\n",
      "        340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353,\n",
      "        354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367,\n",
      "        368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381,\n",
      "        382, 383]) to machine 2\n",
      "\t\t sending C_out tensor([384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397,\n",
      "        398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411,\n",
      "        412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425,\n",
      "        426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439,\n",
      "        440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453,\n",
      "        454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467,\n",
      "        468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481,\n",
      "        482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495,\n",
      "        496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509,\n",
      "        510, 511]) to machine 3\n",
      "Finished execution of layer 27\n",
      "Max diff:\n",
      "tensor([1.7764e-15], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([  0,   2,   5,   8,   9,  10,  12,  13,  14,  19,  20,  21,  23,  25,\n",
      "         27,  28,  29,  30,  32,  33,  38,  40,  42,  43,  44,  45,  47,  49,\n",
      "         50,  51,  52,  53,  57,  59,  61,  64,  65,  66,  67,  71,  75,  76,\n",
      "         81,  82,  83,  87,  95,  98, 100, 101, 104, 108, 110, 111, 114, 115,\n",
      "        118, 120, 122, 126, 127, 128, 130, 132, 136, 138, 139, 144, 145, 146,\n",
      "        147, 148, 150, 151, 154, 155, 157, 158, 160, 161, 162, 163, 165, 166,\n",
      "        168, 169, 170, 173, 177, 180, 181, 182, 183, 184, 185, 186, 189, 193,\n",
      "        194, 201, 202, 203, 204, 206, 207, 209, 212, 214, 215, 216, 217, 219,\n",
      "        220, 227, 228, 230, 233, 234, 238, 241, 245, 247, 249, 251, 252, 255,\n",
      "        256, 257, 258, 260, 262, 263, 265, 268, 272, 273, 275, 277, 278, 280,\n",
      "        281, 284, 287, 291, 292, 293, 294, 298, 300, 304, 307, 309, 310, 311,\n",
      "        313, 314, 315, 321, 324, 326, 327, 330, 331, 333, 334, 335, 336, 340,\n",
      "        341, 343, 345, 348, 349, 351, 357, 358, 360, 362, 363, 364, 365, 368,\n",
      "        370, 371, 372, 373, 375, 376, 378, 380, 381, 382, 383, 384, 385, 386,\n",
      "        388, 389, 392, 394, 403, 404, 405, 406, 407, 408, 409, 410, 411, 414,\n",
      "        415, 416, 418, 419, 424, 427, 432, 433, 435, 437, 438, 443, 445, 448,\n",
      "        451, 452, 453, 454, 455, 456, 458, 459, 460, 461, 462, 463, 464, 465,\n",
      "        469, 470, 471, 474, 475, 476, 478, 479, 487, 488, 490, 491, 492, 493,\n",
      "        494, 495, 497, 498, 500, 502, 503, 504, 510])  (len = 261)\n",
      "passing Cout = tensor([  1,   3,   4,   6,   7,  11,  15,  16,  17,  18,  22,  24,  26,  31,\n",
      "         34,  35,  36,  37,  39,  41,  46,  48,  54,  55,  56,  58,  60,  62,\n",
      "         63,  68,  69,  70,  72,  73,  74,  77,  78,  79,  80,  84,  85,  86,\n",
      "         88,  89,  90,  91,  92,  93,  94,  96,  97,  99, 102, 103, 105, 106,\n",
      "        107, 109, 112, 113, 116, 117, 119, 121, 123, 124, 125, 129, 131, 133,\n",
      "        134, 135, 137, 140, 141, 142, 143, 149, 152, 153, 156, 159, 164, 167,\n",
      "        171, 172, 174, 175, 176, 178, 179, 187, 188, 190, 191, 192, 195, 196,\n",
      "        197, 198, 199, 200, 205, 208, 210, 211, 213, 218, 221, 222, 223, 224,\n",
      "        225, 226, 229, 231, 232, 235, 236, 237, 239, 240, 242, 243, 244, 246,\n",
      "        248, 250, 253, 254, 259, 261, 264, 266, 267, 269, 270, 271, 274, 276,\n",
      "        279, 282, 283, 285, 286, 288, 289, 290, 295, 296, 297, 299, 301, 302,\n",
      "        303, 305, 306, 308, 312, 316, 317, 318, 319, 320, 322, 323, 325, 328,\n",
      "        329, 332, 337, 338, 339, 342, 344, 346, 347, 350, 352, 353, 354, 355,\n",
      "        356, 359, 361, 366, 367, 369, 374, 377, 379, 387, 390, 391, 393, 395,\n",
      "        396, 397, 398, 399, 400, 401, 402, 412, 413, 417, 420, 421, 422, 423,\n",
      "        425, 426, 428, 429, 430, 431, 434, 436, 439, 440, 441, 442, 444, 446,\n",
      "        447, 449, 450, 457, 466, 467, 468, 472, 473, 477, 480, 481, 482, 483,\n",
      "        484, 485, 486, 489, 496, 499, 501, 505, 506, 507, 508, 509, 511])  (len = 251)\n",
      "\n",
      "Executing module 28: relu_2\n",
      "\tExecuting on machine 0\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 1\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 2\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 3\n",
      "\t\t-Applying ReLU\n",
      "Finished execution of layer 28\n",
      "Max diff:\n",
      "tensor([1.7764e-15], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([  0,  20,  27,  29,  38,  43,  53,  61,  64,  83, 118, 136, 138, 150,\n",
      "        154, 157, 158, 165, 166, 168, 170, 173, 184, 193, 194, 201, 202, 203,\n",
      "        209, 214, 216, 217, 220, 227, 233, 238, 247, 249, 252, 255, 257, 263,\n",
      "        265, 272, 273, 280, 281, 284, 300, 307, 311, 326, 343, 351, 378, 380,\n",
      "        382, 462, 490])  (len = 59)\n",
      "passing Cout = tensor([  1,  26,  35,  46,  54,  55,  68,  73,  74,  89,  94,  96, 129, 134,\n",
      "        135, 141, 171, 174, 175, 211, 218, 240, 254, 267, 286, 290, 303, 328,\n",
      "        344, 356, 361, 377, 399])  (len = 33)\n",
      "\n",
      "Executing module 29: linear2\n",
      "\tExecuting on machine 0\n",
      "\t\t-Splitting linear layer 29\n",
      "\t\t Output tensor shape : torch.Size([1, 256])\n",
      "\t\t sending C_out tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
      "        54, 55, 56, 57, 58, 59, 60, 61, 62, 63]) to machine 0\n",
      "\t\t sending C_out tensor([ 64,  65,  68,  72,  74,  79,  80,  81,  82,  83,  85,  88,  96, 100,\n",
      "        101, 102, 103, 107, 109, 112, 113, 115, 116, 117, 123, 124, 125, 126]) to machine 1\n",
      "\t\t sending C_out tensor([128, 129, 133, 134, 143, 147, 148, 151, 152, 157, 158, 159, 161, 162,\n",
      "        163, 165, 166, 168, 170, 171, 173, 175, 176, 179, 184, 186, 187, 188,\n",
      "        189, 190, 191]) to machine 2\n",
      "\t\t sending C_out tensor([196, 199, 200, 201, 202, 203, 208, 215, 216, 224, 226, 227, 229, 230,\n",
      "        237, 238, 241, 244, 245, 246, 247, 249, 250, 251, 253, 254, 255]) to machine 3\n",
      "\tExecuting on machine 1\n",
      "\t\t-Splitting linear layer 29\n",
      "\t\t Output tensor shape : torch.Size([1, 256])\n",
      "\t\t sending C_out tensor([ 0,  1,  3,  4,  5,  7,  8,  9, 11, 13, 14, 15, 17, 19, 21, 22, 23, 24,\n",
      "        25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 40, 41, 42, 45, 46,\n",
      "        48, 49, 50, 51, 52, 53, 54, 56, 58, 59, 61, 62, 63]) to machine 0\n",
      "\t\t sending C_out tensor([ 64,  65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
      "         78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,\n",
      "         92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104, 105,\n",
      "        106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119,\n",
      "        120, 121, 122, 123, 124, 125, 126, 127]) to machine 1\n",
      "\t\t sending C_out tensor([129, 132, 133, 134, 137, 139, 141, 142, 144, 146, 147, 148, 152, 154,\n",
      "        157, 158, 161, 162, 163, 164, 165, 167, 169, 171, 172, 173, 175, 176,\n",
      "        177, 178, 179, 182, 183, 184, 185, 186, 188, 190]) to machine 2\n",
      "\t\t sending C_out tensor([192, 193, 195, 197, 198, 199, 200, 201, 203, 204, 205, 206, 209, 210,\n",
      "        212, 213, 215, 218, 219, 220, 221, 222, 223, 224, 226, 227, 230, 231,\n",
      "        233, 236, 238, 239, 240, 242, 243, 244, 246, 247, 248, 249, 250, 252,\n",
      "        253, 255]) to machine 3\n",
      "\tExecuting on machine 2\n",
      "\t\t-Splitting linear layer 29\n",
      "\t\t Output tensor shape : torch.Size([1, 256])\n",
      "\t\t sending C_out tensor([ 0,  1,  2,  3,  5,  9, 10, 12, 13, 14, 15, 19, 20, 21, 22, 24, 25, 26,\n",
      "        27, 28, 29, 30, 31, 32, 33, 34, 35, 38, 40, 41, 45, 47, 48, 49, 50, 51,\n",
      "        53, 54, 55, 56, 57, 61, 62]) to machine 0\n",
      "\t\t sending C_out tensor([ 67,  70,  71,  72,  73,  74,  75,  76,  77,  80,  83,  84,  85,  88,\n",
      "         89,  90,  92,  93,  94,  95,  97, 101, 102, 103, 104, 106, 108, 109,\n",
      "        110, 113, 116, 118, 121, 122, 123, 124, 125, 127]) to machine 1\n",
      "\t\t sending C_out tensor([128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141,\n",
      "        142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
      "        156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169,\n",
      "        170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183,\n",
      "        184, 185, 186, 187, 188, 189, 190, 191]) to machine 2\n",
      "\t\t sending C_out tensor([193, 194, 197, 198, 199, 200, 203, 204, 205, 206, 208, 209, 211, 213,\n",
      "        215, 216, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229,\n",
      "        230, 231, 234, 236, 237, 238, 239, 240, 241, 242, 244, 245, 246, 247,\n",
      "        248, 249, 250, 252, 253, 254, 255]) to machine 3\n",
      "\tExecuting on machine 3\n",
      "\t\t-Splitting linear layer 29\n",
      "\t\t Output tensor shape : torch.Size([1, 256])\n",
      "\t\t sending C_out tensor([25, 30, 45]) to machine 0\n",
      "\t\t sending C_out tensor([ 97, 100]) to machine 1\n",
      "\t\t sending C_out tensor([143, 187, 190]) to machine 2\n",
      "\t\t sending C_out tensor([192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205,\n",
      "        206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219,\n",
      "        220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
      "        234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247,\n",
      "        248, 249, 250, 251, 252, 253, 254, 255]) to machine 3\n",
      "Finished execution of layer 29\n",
      "Max diff:\n",
      "tensor([8.8818e-16], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([  0,   1,   3,   4,   6,   7,   8,  12,  13,  14,  16,  17,  19,  20,\n",
      "         21,  22,  23,  24,  26,  27,  28,  30,  33,  37,  40,  44,  45,  46,\n",
      "         47,  50,  53,  54,  55,  56,  57,  59,  61,  62,  63,  65,  66,  67,\n",
      "         68,  69,  71,  72,  73,  74,  75,  81,  82,  83,  88,  89,  90,  92,\n",
      "         93,  96,  97, 101, 102, 105, 106, 107, 110, 112, 113, 114, 115, 116,\n",
      "        117, 118, 121, 123, 125, 126, 127, 129, 130, 131, 132, 133, 134, 135,\n",
      "        136, 137, 138, 141, 142, 143, 144, 145, 146, 148, 154, 155, 158, 162,\n",
      "        166, 168, 171, 172, 173, 174, 175, 178, 179, 182, 183, 185, 186, 187,\n",
      "        188, 190, 192, 193, 194, 195, 196, 197, 198, 199, 200, 202, 203, 204,\n",
      "        205, 206, 208, 210, 211, 212, 213, 214, 218, 219, 220, 221, 222, 223,\n",
      "        224, 225, 226, 228, 229, 230, 231, 233, 236, 240, 241, 242, 244, 246,\n",
      "        249, 250, 254, 255])  (len = 158)\n",
      "passing Cout = tensor([  2,   5,   9,  10,  11,  15,  18,  25,  29,  31,  32,  34,  35,  36,\n",
      "         38,  39,  41,  42,  43,  48,  49,  51,  52,  58,  60,  64,  70,  76,\n",
      "         77,  78,  79,  80,  84,  85,  86,  87,  91,  94,  95,  98,  99, 100,\n",
      "        103, 104, 108, 109, 111, 119, 120, 122, 124, 128, 139, 140, 147, 149,\n",
      "        150, 151, 152, 153, 156, 157, 159, 160, 161, 163, 164, 165, 167, 169,\n",
      "        170, 176, 177, 180, 181, 184, 189, 191, 201, 207, 209, 215, 216, 217,\n",
      "        227, 232, 234, 235, 237, 238, 239, 243, 245, 247, 248, 251, 252, 253])  (len = 98)\n",
      "\n",
      "Executing module 30: relu_3\n",
      "\tExecuting on machine 0\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 1\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 2\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 3\n",
      "\t\t-Applying ReLU\n",
      "Finished execution of layer 30\n",
      "Max diff:\n",
      "tensor([8.8818e-16], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([  6,   8,  17,  19,  24,  27,  30,  50,  54,  59,  65,  72,  89, 101,\n",
      "        105, 129, 130, 131, 133, 134, 137, 141, 142, 144, 146, 155, 162, 168,\n",
      "        171, 173, 174, 175, 178, 179, 183, 185, 187, 188, 195, 210, 244])  (len = 41)\n",
      "passing Cout = tensor([ 36,  43,  48,  85, 120, 140, 149, 150, 151, 152, 156, 160, 161, 163,\n",
      "        164, 176, 177, 180, 181, 189])  (len = 20)\n",
      "\n",
      "Executing module 31: out\n",
      "\tExecuting on machine 0\n",
      "\t\t-Splitting linear layer 31\n",
      "\t\t Output tensor shape : torch.Size([1, 2])\n",
      "\t\t sending C_out tensor([0, 1]) to machine 0\n",
      "\tExecuting on machine 1\n",
      "\t\t-Splitting linear layer 31\n",
      "\t\t Output tensor shape : torch.Size([1, 2])\n",
      "\t\t sending C_out tensor([0, 1]) to machine 0\n",
      "\tExecuting on machine 2\n",
      "\t\t-Splitting linear layer 31\n",
      "\t\t Output tensor shape : torch.Size([1, 2])\n",
      "\t\t sending C_out tensor([0, 1]) to machine 0\n",
      "\tExecuting on machine 3\n",
      "\t\t-Splitting linear layer 31\n",
      "\t\t Output tensor shape : torch.Size([1, 2])\n",
      "\t\t sending C_out tensor([0, 1]) to machine 0\n",
      "Finished execution of layer 31\n",
      "\n",
      "\n",
      "############################# FINAL EXECUTION TIME 1.1680595874786377 [seconds] #############################\n",
      "\n",
      "\n",
      "Max diff:\n",
      "tensor([5.5511e-16], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([0, 1])  (len = 2)\n",
      "passing Cout = tensor([], dtype=torch.int64)  (len = 0)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "''' Prep Inout for Mock Run version 2 (fx)'''\n",
    "\n",
    "# TODO: reduce size of communicated tensors to only what is necessary \n",
    "# TODO: also check bias for nonzero\n",
    "# TODO: come up with more general scheme to handle residual layers\n",
    "\n",
    "# channel_id == INPUTS\n",
    "# filter_id  == OUTPUTS\n",
    "\n",
    "# try greater precision\n",
    "#torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "#torch.set_default_dtype(torch.float64)\n",
    "\n",
    "# model input \n",
    "N_batch = 1\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# if configs['model'] == 'resnet18' or configs['model'] == 'wrn28_10':\n",
    "if dataset == 'cifar10' or dataset == 'cifar100':\n",
    "    input_tensor = torch.rand(N_batch, 3, 32, 32, dtype=torch.float64, device=torch.device(configs['device'])) # 1k images, 3 channels, 32x32 image (cifar10)\n",
    "    input_tensor = tuple([input_tensor])\n",
    "    input_tensor = torch.cat(input_tensor, dim=1)\n",
    "# elif configs['model'] == 'EscFusion':\n",
    "elif dataset == 'esc':\n",
    "    input_tensor = torch.rand(N_batch, 3, 266, 320, dtype=torch.float64, device=torch.device(configs['device']))\n",
    "    input_tensor = tuple([input_tensor]*5)\n",
    "    input_tensor = torch.cat(input_tensor, dim=1)\n",
    "    \n",
    "    # input_tensor = tuple([input_tensor])\n",
    "    # input_tensor = torch.cat(input_tensor, dim=1)\n",
    "# elif configs['model'] == 'InfoFusionThree':\n",
    "elif dataset == 'flash':\n",
    "    input_tensor = torch.rand(N_batch, 2, 10, 10, dtype=torch.float64, device=torch.device(configs['device']))\n",
    "\n",
    "\n",
    "# if configs['data_code'] == 'flash':\n",
    "#     input_shape = [(2, 1),\n",
    "#                 #    (90, 160, 3),\n",
    "#                     (360, 640, 3),\n",
    "#                     (20, 20, 20),]\n",
    "# elif configs['data_code'] == 'esc':\n",
    "#     input_shape = [(3, 266, 320) for _ in range(5)]\n",
    "# else:\n",
    "#     input_shape = [(3, 32, 32)]\n",
    "\n",
    "# input_np = (np.random.uniform(0, 1, (1,)+x) for x in input_shape)\n",
    "# input_tensor = tuple(Variable(torch.FloatTensor(x), requires_grad=False).to(configs[\"device\"]) for x in input_np)\n",
    "# # input_tensor = torch.cat(input_tensor, dim=1)\n",
    "\n",
    "# print(input_tensor.shape)\n",
    "# print(input_tensor[0].shape)\n",
    "# print(len(input_tensor))\n",
    "\n",
    "# print(f'input_tensor: {input_tensor}')\n",
    "\n",
    "model = model.type(torch.float64)\n",
    "\n",
    "# make SplitManagers for split model execution \n",
    "configs['dtype'] = 'float64'\n",
    "split_managers = [SplitManager]*num_machines\n",
    "for i in range(num_machines):\n",
    "    split_managers[i] = SplitManager(configs, i, num_machines, input_tensor)\n",
    "\n",
    "# broadcast input_tensor to different machines\n",
    "# TODO: find a better datastructure for this\n",
    "#input = np.empty((num_machines, num_machines), dtype=torch.Tensor)\n",
    "input = [None]*num_machines\n",
    "input = [input[:] for i in range(num_machines)]\n",
    "for imach in range(num_machines):\n",
    "    input[imach][imach] = input_tensor\n",
    "\n",
    "# print(f'input: {input}')\n",
    "\n",
    "residual_block_start, residual_connection_start, residual_block_end = get_residual_block_indexes(model)\n",
    "\n",
    "# print(f'residual_block_start: {residual_block_start}')\n",
    "# print(f'residual_connection_start: {residual_connection_start}')\n",
    "# print(f'residual_block_end: {residual_block_end}')\n",
    "\n",
    "# put models into eval mode and on device\n",
    "model.eval()\n",
    "model.to(configs['device'])\n",
    "\n",
    "# set index to execute vertical splitting up to \n",
    "exec_to_layer = total_layers_fx-1\n",
    "\n",
    "# get true output at each layer\n",
    "# WARNING: bn layers are initialized differently between model in this notebook and model in split manager. The moving mean and variance fields do not match HOWEVER the weights and biases are the same\n",
    "#horz_output, size_LUT = get_output_at_each_layer(model, input_tensor) \n",
    "horz_output, size_LUT = get_output_at_each_layer(split_managers[0].model, input_tensor)\n",
    "\n",
    "BREAK_LOOP = 0 # break loop when output differs\n",
    "\n",
    "'''\n",
    "    mock run through inference using split models \n",
    "'''\n",
    "\n",
    "# timing\n",
    "split_execution_start_time = time.time()\n",
    "layer_completion_time_stamp = {}\n",
    "layer_execution_duration = {}\n",
    "\n",
    "# make inference \n",
    "with torch.no_grad():\n",
    "    residual_input = {} # use this to keep track of inputs stored in machine memory for residule layers\n",
    "\n",
    "    # iterate through layers 1 module at a time \n",
    "    for imodule in range(exec_to_layer+1):#range(num_total_modules): # 16 <=> layer_1 block \n",
    "\n",
    "        # initialize output for ilayer\n",
    "        #output = np.empty((num_machines, num_machines), dtype=torch.Tensor) # square list indexed as: output[destination/RX machine][origin/TX machine]\n",
    "        # TODO: find a better datastructure for this \n",
    "\n",
    "        output = [None]*num_machines\n",
    "        output = [output[:] for i in range(num_machines)]\n",
    "\n",
    "        # print(f'output: {output}')\n",
    "\n",
    "        # DEBUG\n",
    "        full_input = combine_all_inputs(input, num_machines)\n",
    "\n",
    "        print(f'Executing module {imodule}: {layer_names_fx[imodule]}')\n",
    "\n",
    "        # iterate through each machine (done in parallel later)\n",
    "        for imach in range(num_machines):\n",
    "            print(f'\\tExecuting on machine {imach}')\n",
    "            # print(f'curr_input: {curr_input}')\n",
    "            # print(f'input: {input}')\n",
    "            # collect communication inputs if necessary \n",
    "            if not imodule == 0 and ('conv' in layer_names_fx[imodule-1] or 'linear' in layer_names_fx[imodule-1] or 'shortcut.1' in layer_names_fx[imodule]): # TODO: this is very hacky, needs to be generalized. The issue is ID'ing conv layers in shortcut blocks\n",
    "                curr_input = combine_inputs(input, num_machines, imach)\n",
    "                # print(f'ilk if e girdi, curr_input: {curr_input}')\n",
    "            else:\n",
    "                curr_input = input[imach][imach]\n",
    "                # print(f'ikinci elif e girdi, curr_input: {curr_input}')\n",
    "\n",
    "            # print(f'len curr_input: {len(curr_input)}')\n",
    "            \n",
    "            out_tensor, do_comms = split_managers[imach].execute_split_layer(curr_input, imodule)\n",
    "            # print(f'out_tensor: {out_tensor}')\n",
    "            if not do_comms:\n",
    "                # update output to current machine and continue\n",
    "                if torch.is_tensor(out_tensor):\n",
    "                    # sometimes out_tensor is None\n",
    "                    # input is sent to all machines for 1st layer execution even though not all machines need to compute \n",
    "                    # Output from machine is None in this case TODO: fix where inputs are sent \n",
    "                    output[imach][imach] = out_tensor\n",
    "                    # print(f'output: {output}')\n",
    "                continue\n",
    "            # print(f'out_tensor: {out_tensor}')\n",
    "            # END SplitManager execute split_layer\n",
    "\n",
    "            print(f'\\t\\t Output tensor shape : {out_tensor.shape}')\n",
    "\n",
    "            # debug\n",
    "            nonzero_out_tensor = torch.unique(torch.nonzero(out_tensor, as_tuple=True)[1])\n",
    "\n",
    "            # look at which C_out need to be computed and sent\n",
    "            #nonzero_Cout = torch.unique(torch.nonzero(split_layer.weight, as_tuple=True)[0]) # find nonzero dimensions in output channels\n",
    "            nonzero_Cout = get_nonzero_channels(out_tensor)\n",
    "            # print(f'\\t\\t\\t nonzero_Cout: {nonzero_Cout}')\n",
    "\n",
    "            # prep communications by populating output\n",
    "            out_channel_array = torch.arange(out_tensor.shape[1])\n",
    "            for rx_mach in range(num_machines):\n",
    "                # only add to output if communication is necessary \n",
    "\n",
    "                # Get output channels for current rx machine? TODO: consider removing, this just maps C_out's to machine\n",
    "                #output_channels = torch.tensor(configs['partition'][][rx_mach],\n",
    "                #        device=torch.device(configs['device']))\n",
    "                output_channels = torch.tensor(split_managers[imach].output_channel_map[rx_mach],\n",
    "                        device=torch.device(configs['device']))\n",
    "                \n",
    "                # print(f'\\t\\t\\t output_channels: {output_channels}')\n",
    "\n",
    "                # TODO: is there a faster way to do this? Consider putting larger array 1st... just not sure which one that'd be\n",
    "                nonzero_out_channels = nonzero_Cout[torch.isin(nonzero_Cout, output_channels)]\n",
    "                if nonzero_out_channels.nelement() > 0:\n",
    "                        communication_mask = torch.isin(out_channel_array, nonzero_out_channels)\n",
    "\n",
    "                        # TODO: this is inefficient, redo. Probbably need to send a tensor and some info what output channels are being sent\n",
    "                        tmp_out = torch.zeros(out_tensor.shape, dtype= split_managers[imach].dtype) \n",
    "                        if imodule == total_layers_fx-1 or 'linear' in layer_names_fx[imodule]:\n",
    "                                tmp_out[:,communication_mask] = out_tensor[:,communication_mask]\n",
    "                        else:\n",
    "                                tmp_out[:,communication_mask,:,:] = out_tensor[:,communication_mask,:,:]\n",
    "                        output[rx_mach][imach] = tmp_out\n",
    "\n",
    "                        # debug\n",
    "                        print(f'\\t\\t sending C_out {nonzero_out_channels} to machine {rx_mach}')\n",
    "\n",
    "        # send to next layer  \n",
    "        input = output\n",
    "        print(f'Finished execution of layer {imodule}')\n",
    "\n",
    "        # update timing\n",
    "        layer_completion_time_stamp[layer_names_fx[imodule]] = time.time()\n",
    "        if imodule > 0:\n",
    "            layer_execution_duration[layer_names_fx[imodule]] = layer_completion_time_stamp[layer_names_fx[imodule]] - layer_completion_time_stamp[layer_names_fx[imodule-1]] \n",
    "        else:\n",
    "            layer_execution_duration[layer_names_fx[imodule]] = layer_completion_time_stamp[layer_names_fx[imodule]] - split_execution_start_time\n",
    "\n",
    "        # check output at end of each layer to see if fit matches \n",
    "        tmp_output = input \n",
    "        # print(f'tmp_output = {tmp_output}')\n",
    "        need_to_init  = True\n",
    "        for rx_mach in range(num_machines):\n",
    "                for tx_mach in range(num_machines):\n",
    "                        if not tmp_output[rx_mach][tx_mach] == None:\n",
    "                                if need_to_init:\n",
    "                                        vert_output = tmp_output[rx_mach][tx_mach]\n",
    "                                        need_to_init = False\n",
    "                                else:\n",
    "                                        # TODO: += causes assignment issues, switched to x = x+y which might be more more inefficent memory wise ... \n",
    "                                        vert_output = vert_output + tmp_output[rx_mach][tx_mach] \n",
    "                                        #nz_channels = get_nonzero_channels(vert_output)\n",
    "                                        #print(f'({rx_mach},{tx_mach}) {nz_channels}')\n",
    "        \n",
    "        if imodule == total_layers_fx-1:\n",
    "            # apply bias\n",
    "            # TODO: assumes Linear layer is final layer and bias can be handled as final step \n",
    "            vert_output = vert_output + get_current_module(model, imodule).bias\n",
    "            \n",
    "            # final execution time\n",
    "            tot_split_execution_time = time.time() - split_execution_start_time\n",
    "            print(f'\\n\\n############################# FINAL EXECUTION TIME {tot_split_execution_time} [seconds] #############################\\n\\n')\n",
    "\n",
    "        truth_output = horz_output[layer_names_fx[imodule]]\n",
    "        # if 'x' == layer_names_fx[imodule] or '_x' == layer_names_fx[imodule] or 'getitem' == layer_names_fx[imodule] or 'getitem_1' == layer_names_fx[imodule] or 'getitem_2' == layer_names_fx[imodule] or 'getitem_3' == layer_names_fx[imodule] or 'getitem_4' == layer_names_fx[imodule] or 'cat' == layer_names_fx[imodule]:\n",
    "        if 'x' == layer_names_fx[imodule]:    \n",
    "            print(f'Input layer. Skipping comparison')\n",
    "        elif torch.is_tensor(truth_output):\n",
    "            max_diff, max_by_Cout = compare_outputs(vert_output, truth_output)\n",
    "            if max_diff > 0.1:\n",
    "                # pass\n",
    "                BREAK_LOOP = 1 \n",
    "        else:\n",
    "            print(f'Horizontal output is {type(truth_output)}. Skipping comparison')\n",
    "        print()\n",
    "\n",
    "        if BREAK_LOOP:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "#     Conv1 layer test -- should we expect some difference, or exactly 0 in the output when we split the model?\n",
    "# '''\n",
    "\n",
    "# # DIFFERENCE SHOULD BE 0 NOT 1E-7\n",
    "\n",
    "# N_in = 1\n",
    "# split_1 = nn.Conv2d(N_in,\n",
    "#             model.conv1.weight.shape[0], # TODO does this need to be an int? (currently tensor)\n",
    "#             kernel_size= model.conv1.kernel_size,\n",
    "#             stride=model.conv1.stride,\n",
    "#             padding=model.conv1.padding, \n",
    "#             bias=False) # TODO: add bias during input collecting step on next layer \n",
    "# split_1.weight = torch.nn.Parameter(model.conv1.weight.index_select(1, torch.tensor([0])))  \n",
    "# out_split1 = split_1(input_tensor.index_select(1, torch.tensor([0])))\n",
    "\n",
    "# split_2 = split_1\n",
    "# split_2.weight = torch.nn.Parameter(model.conv1.weight.index_select(1, torch.tensor([1])))  \n",
    "# out_split2 = split_2(input_tensor.index_select(1, torch.tensor([1])))\n",
    "\n",
    "# split_3 = split_1\n",
    "# split_3.weight = torch.nn.Parameter(model.conv1.weight.index_select(1, torch.tensor([2])))  \n",
    "# out_split3 = split_3(input_tensor.index_select(1, torch.tensor([2])))\n",
    "\n",
    "# split_out = torch.add(torch.add(out_split1, out_split2), out_split3)\n",
    "# full_out = model.conv1(input_tensor)\n",
    "\n",
    "# diff_output = torch.abs(full_out - split_out)\n",
    "# max_diff = torch.max(diff_output)\n",
    "# max_diff.sci_mode = True\n",
    "# print(max_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''  \n",
    "#     Inspect I/O of single layer\n",
    "# '''\n",
    "\n",
    "# t = torch.ones((1,2,2,2), dtype=torch.float32) # (batch, in channel, H, W)\n",
    "# w = torch.ones((1,2,2,2), dtype=torch.float32) # (out channels, in channels, H, W)\n",
    "# w[0,0,0,0] = 1e-10\n",
    "# w[0,0,0,1] = 1e-10\n",
    "# w[0,1,0,0] = 1e-10\n",
    "\n",
    "# full_conv = torch.nn.Conv2d(2,1,kernel_size=(2,2), bias=False, stride=(1),dtype=torch.float32)\n",
    "# full_conv.weight = torch.nn.Parameter(w)\n",
    "# conv1 = torch.nn.Conv2d(1,1,kernel_size=(2,2), bias=False, stride=1, dtype=torch.float32)\n",
    "# conv1.weight =torch.nn.Parameter( w[:,0:1,:,:])\n",
    "# conv2 = torch.nn.Conv2d(1,1,kernel_size=(2,2), bias=False, stride=1,dtype=torch.float32)\n",
    "# conv2.weight = torch.nn.Parameter(w[:,1:2,:,:])\n",
    "\n",
    "# full_conv.eval()\n",
    "# conv1.eval()\n",
    "# conv2.eval()\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     full_out = full_conv(t)\n",
    "#     split_out =  conv2(t[0,1:2,:,:]) + conv1(t[:,0:1,:,:])\n",
    "\n",
    "# diff = torch.abs(full_out - split_out)\n",
    "\n",
    "# torch.nonzero(diff)\n",
    "# #print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test bn1\n",
    "#estimate = -bn1.running_mean[1]/torch.sqrt(bn1.running_var[1] + bn1.eps)*bn1.weight[1] + bn1.bias[1]\n",
    "#estimate_split = -split_layer.running_mean[1]/torch.sqrt(split_layer.running_var[1] + split_layer.eps)*split_layer.weight[1] + split_layer.bias[1]\n",
    "\n",
    "# running estimates are different \n",
    "#bn1.running_mean[1] - split_layer.running_mean[1]\n",
    "#bn1.running_var[1] - split_layer.running_var[1] \n",
    "#bn1.weight[1] - split_layer.weight[1]\n",
    "#bn1.eps - split_layer.eps\n",
    "#bn1.bias[1] - split_layer.bias[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cap_nb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
