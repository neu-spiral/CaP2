{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    Load model and split it\\n        1. layer by layer\\n        2. [TODO] vertically \\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    Load model and split it\n",
    "        1. layer by layer\n",
    "        2. [TODO] vertically \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from source.core.engine import MoP\n",
    "import source.core.run_partition as run_p\n",
    "from os import environ\n",
    "from source.utils.dataset import *\n",
    "from source.utils.misc import *\n",
    "from source.utils.split_network import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "from source.models import resnet\n",
    "from source.core.split_manager import SplitManager\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from source.utils import io\n",
    "from source.utils import testers\n",
    "from source.core import engine\n",
    "import json\n",
    "import itertools\n",
    "\n",
    "from torchvision.models.feature_extraction import create_feature_extractor, get_graph_node_names\n",
    "from torchsummary import summary\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model setup\n",
    "\n",
    "setup = 'resnet18'\n",
    "# setup = 'escnet'\n",
    "# setup = 'flashnet'\n",
    "# setup = 'wideresnet'\n",
    "\n",
    "\n",
    "# yaml_version = 'v1'\n",
    "yaml_version = 'v2'\n",
    "# yaml_version = 'v3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device :  \n",
      "model :  resnet18\n",
      "data_code :  cifar10\n",
      "num_classes :  10\n",
      "model_file :  test.pt\n",
      "epochs :  0\n",
      "batch_size :  128\n",
      "optimizer :  sgd\n",
      "lr_scheduler :  default\n",
      "learning_rate :  0.01\n",
      "seed :  1234\n",
      "sparsity_type :  kernel\n",
      "prune_ratio :  1\n",
      "admm :  True\n",
      "admm_epochs :  300\n",
      "rho :  0.0001\n",
      "multi_rho :  True\n",
      "retrain_bs :  128\n",
      "retrain_lr :  0.005\n",
      "retrain_ep :  50\n",
      "retrain_opt :  default\n",
      "xentropy_weight :  1.0\n",
      "warmup :  False\n",
      "warmup_lr :  0.001\n",
      "warmup_epochs :  10\n",
      "mix_up :  True\n",
      "alpha :  0.3\n",
      "smooth :  False\n",
      "smooth_eps :  0\n",
      "save_last_model_only :  False\n",
      "num_partition :  1\n",
      "layer_type :  regular\n",
      "bn_type :  masked\n",
      "par_first_layer :  False\n",
      "comm_outsize :  False\n",
      "lambda_comm :  0\n",
      "lambda_comp :  0\n",
      "distill_model :  \n",
      "distill_loss :  kl\n",
      "distill_temp :  30\n",
      "distill_alpha :  1\n"
     ]
    }
   ],
   "source": [
    "# setup config\n",
    "\n",
    "if setup == 'resnet18':\n",
    "    dataset='cifar10'\n",
    "    load_model = f\"cifar10-resnet18-kernel-np{yaml_version}-pr0.75-lcm0.001.pt\"\n",
    "elif setup == 'escnet':\n",
    "    dataset='esc'\n",
    "    load_model = f\"esc-escnet-kernel-np{yaml_version}-pr0.75-lcm0.001.pt\"\n",
    "    # load_model = f\"esc-escnet-kernel-npv0-pr0.75-lcm0.001.pt\"\n",
    "elif setup == 'flashnet':\n",
    "    dataset='flash'\n",
    "    load_model = f\"flash-flashnet-kernel-np{yaml_version}-pr0.75-lcm0.001.pt\"\n",
    "elif setup == 'wideresnet':\n",
    "    dataset='cifar100'\n",
    "    load_model = f\"cifar100-wrn28-kernel-np{yaml_version}-pr0.75-lcm0.001.pt\"\n",
    "    \n",
    "\n",
    "environ[\"config\"] = f\"config/{dataset}.yaml\"\n",
    "configs = run_p.main()\n",
    "configs[\"device\"] = \"cpu\"\n",
    "configs['load_model'] = load_model\n",
    "\n",
    "if setup == 'resnet18':\n",
    "    configs[\"num_partition\"] = f'config/resnet18-{yaml_version}.yaml'\n",
    "elif setup == 'escnet':\n",
    "    configs[\"num_partition\"] = f'config/escnet-{yaml_version}.yaml'\n",
    "elif setup == 'flashnet':\n",
    "    configs[\"num_partition\"] = f'config/flashnet.yaml'\n",
    "elif setup == 'wideresnet':\n",
    "    configs[\"num_partition\"] = f'config/wrn28-{yaml_version}.yaml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (linear1): Linear(in_features=512, out_features=256, bias=False)\n",
      "  (linear2): Linear(in_features=256, out_features=128, bias=False)\n",
      "  (linear): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# load data and load or train model\n",
    "model = get_model_from_code(configs).to(configs['device']) # grabs model architecture from ./source/models/escnet.py\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load weights into full model\n",
    "state_dict = torch.load(io.get_model_path_split(\"{}\".format(configs[\"load_model\"])), map_location=configs['device'])\n",
    "model = io.load_state_dict(model, \n",
    "                    state_dict['model_state_dict'] if 'model_state_dict' in state_dict \n",
    "                    else state_dict['state_dict'] if 'state_dict' in state_dict else state_dict,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_partition: {'inputs': 4, 'conv1.weight': 4, 'layer1.0.conv1.weight': 4, 'layer1.0.conv2.weight': 4, 'layer1.1.conv1.weight': 4, 'layer1.1.conv2.weight': 4, 'layer2.0.conv1.weight': 4, 'layer2.0.conv2.weight': 4, 'layer2.0.shortcut.0.weight': 4, 'layer2.1.conv1.weight': 4, 'layer2.1.conv2.weight': 4, 'layer3.0.conv1.weight': 4, 'layer3.0.conv2.weight': 4, 'layer3.0.shortcut.0.weight': 4, 'layer3.1.conv1.weight': 4, 'layer3.1.conv2.weight': 4, 'layer4.0.conv1.weight': 4, 'layer4.0.conv2.weight': 4, 'layer4.0.shortcut.0.weight': 4, 'layer4.1.conv1.weight': 4, 'layer4.1.conv2.weight': 4, 'linear1.weight': 4, 'linear2.weight': 4}\n",
      "ratio_partition: {'inputs': [1, 1, 1, 1], 'conv1.weight': [1, 1, 1, 1], 'layer1.0.conv1.weight': [1, 1, 1, 1], 'layer1.0.conv2.weight': [1, 1, 1, 1], 'layer1.1.conv1.weight': [1, 1, 1, 1], 'layer1.1.conv2.weight': [1, 1, 1, 1], 'layer2.0.conv1.weight': [1, 1, 1, 1], 'layer2.0.conv2.weight': [1, 1, 1, 1], 'layer2.0.shortcut.0.weight': [1, 1, 1, 1], 'layer2.1.conv1.weight': [1, 1, 1, 1], 'layer2.1.conv2.weight': [1, 1, 1, 1], 'layer3.0.conv1.weight': [1, 1, 1, 1], 'layer3.0.conv2.weight': [1, 1, 1, 1], 'layer3.0.shortcut.0.weight': [1, 1, 1, 1], 'layer3.1.conv1.weight': [1, 1, 1, 1], 'layer3.1.conv2.weight': [1, 1, 1, 1], 'layer4.0.conv1.weight': [1, 1, 1, 1], 'layer4.0.conv2.weight': [1, 1, 1, 1], 'layer4.0.shortcut.0.weight': [1, 1, 1, 1], 'layer4.1.conv1.weight': [1, 1, 1, 1], 'layer4.1.conv2.weight': [1, 1, 1, 1], 'linear1.weight': [1, 1, 1, 1], 'linear2.weight': [1, 1, 1, 1]}\n",
      "map_partition: {'conv1.weight': [[0, 1, 1, 1], [1, 0, 1, 1], [1, 1, 0, 1], [1, 1, 1, 0]], 'layer1.0.conv1.weight': [[0, 1, 1, 1], [1, 0, 1, 1], [1, 1, 0, 1], [1, 1, 1, 0]], 'layer1.0.conv2.weight': [[0, 1, 1, 1], [1, 0, 1, 1], [1, 1, 0, 1], [1, 1, 1, 0]], 'layer1.1.conv1.weight': [[0, 1, 1, 1], [1, 0, 1, 1], [1, 1, 0, 1], [1, 1, 1, 0]], 'layer1.1.conv2.weight': [[0, 1, 1, 1], [1, 0, 1, 1], [1, 1, 0, 1], [1, 1, 1, 0]], 'layer2.0.conv1.weight': [[0, 1, 1, 1], [1, 0, 1, 1], [1, 1, 0, 1], [1, 1, 1, 0]], 'layer2.0.conv2.weight': [[0, 1, 1, 1], [1, 0, 1, 1], [1, 1, 0, 1], [1, 1, 1, 0]], 'layer2.0.shortcut.0.weight': [[0, 1, 1, 1], [1, 0, 1, 1], [1, 1, 0, 1], [1, 1, 1, 0]], 'layer2.1.conv1.weight': [[0, 1, 1, 1], [1, 0, 1, 1], [1, 1, 0, 1], [1, 1, 1, 0]], 'layer2.1.conv2.weight': [[0, 1, 1, 1], [1, 0, 1, 1], [1, 1, 0, 1], [1, 1, 1, 0]], 'layer3.0.conv1.weight': [[0, 1, 1, 1], [1, 0, 1, 1], [1, 1, 0, 1], [1, 1, 1, 0]], 'layer3.0.conv2.weight': [[0, 1, 1, 1], [1, 0, 1, 1], [1, 1, 0, 1], [1, 1, 1, 0]], 'layer3.0.shortcut.0.weight': [[0, 1, 1, 1], [1, 0, 1, 1], [1, 1, 0, 1], [1, 1, 1, 0]], 'layer3.1.conv1.weight': [[0, 1, 1, 1], [1, 0, 1, 1], [1, 1, 0, 1], [1, 1, 1, 0]], 'layer3.1.conv2.weight': [[0, 1, 1, 1], [1, 0, 1, 1], [1, 1, 0, 1], [1, 1, 1, 0]], 'layer4.0.conv1.weight': [[0, 1, 1, 1], [1, 0, 1, 1], [1, 1, 0, 1], [1, 1, 1, 0]], 'layer4.0.conv2.weight': [[0, 1, 1, 1], [1, 0, 1, 1], [1, 1, 0, 1], [1, 1, 1, 0]], 'layer4.0.shortcut.0.weight': [[0, 1, 1, 1], [1, 0, 1, 1], [1, 1, 0, 1], [1, 1, 1, 0]], 'layer4.1.conv1.weight': [[0, 1, 1, 1], [1, 0, 1, 1], [1, 1, 0, 1], [1, 1, 1, 0]], 'layer4.1.conv2.weight': [[0, 1, 1, 1], [1, 0, 1, 1], [1, 1, 0, 1], [1, 1, 1, 0]], 'linear1.weight': [[0, 1, 1, 1], [1, 0, 1, 1], [1, 1, 0, 1], [1, 1, 1, 0]], 'linear2.weight': [[0, 1, 1, 1], [1, 0, 1, 1], [1, 1, 0, 1], [1, 1, 1, 0]]}\n",
      "bn_partition: [4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "Inference time per data is 16.556025ms.\n",
      "conv1.weight 1024\n",
      "layer1.0.conv1.weight 1024\n",
      "layer1.0.conv2.weight 1024\n",
      "layer1.1.conv1.weight 1024\n",
      "layer1.1.conv2.weight 1024\n",
      "layer2.0.conv1.weight 256\n",
      "layer2.0.conv2.weight 256\n",
      "layer2.0.shortcut.0.weight 256\n",
      "layer2.1.conv1.weight 256\n",
      "layer2.1.conv2.weight 256\n",
      "layer3.0.conv1.weight 64\n",
      "layer3.0.conv2.weight 64\n",
      "layer3.0.shortcut.0.weight 64\n",
      "layer3.1.conv1.weight 64\n",
      "layer3.1.conv2.weight 64\n",
      "layer4.0.conv1.weight 16\n",
      "layer4.0.conv2.weight 16\n",
      "layer4.0.shortcut.0.weight 16\n",
      "layer4.1.conv1.weight 16\n",
      "layer4.1.conv2.weight 16\n",
      "linear1.weight 1\n",
      "linear2.weight 1\n",
      "Total layers: 74\n",
      "['x', 'conv1', 'bn1', 'relu', 'layer1.0.conv1', 'layer1.0.bn1', 'layer1.0.relu', 'layer1.0.conv2', 'layer1.0.bn2', 'layer1.0.add', 'layer1.0.relu_1', 'layer1.1.conv1', 'layer1.1.bn1', 'layer1.1.relu', 'layer1.1.conv2', 'layer1.1.bn2', 'layer1.1.add', 'layer1.1.relu_1', 'layer2.0.conv1', 'layer2.0.bn1', 'layer2.0.relu', 'layer2.0.conv2', 'layer2.0.bn2', 'layer2.0.shortcut.0', 'layer2.0.shortcut.1', 'layer2.0.add', 'layer2.0.relu_1', 'layer2.1.conv1', 'layer2.1.bn1', 'layer2.1.relu', 'layer2.1.conv2', 'layer2.1.bn2', 'layer2.1.add', 'layer2.1.relu_1', 'layer3.0.conv1', 'layer3.0.bn1', 'layer3.0.relu', 'layer3.0.conv2', 'layer3.0.bn2', 'layer3.0.shortcut.0', 'layer3.0.shortcut.1', 'layer3.0.add', 'layer3.0.relu_1', 'layer3.1.conv1', 'layer3.1.bn1', 'layer3.1.relu', 'layer3.1.conv2', 'layer3.1.bn2', 'layer3.1.add', 'layer3.1.relu_1', 'layer4.0.conv1', 'layer4.0.bn1', 'layer4.0.relu', 'layer4.0.conv2', 'layer4.0.bn2', 'layer4.0.shortcut.0', 'layer4.0.shortcut.1', 'layer4.0.add', 'layer4.0.relu_1', 'layer4.1.conv1', 'layer4.1.bn1', 'layer4.1.relu', 'layer4.1.conv2', 'layer4.1.bn2', 'layer4.1.add', 'layer4.1.relu_1', 'avg_pool2d', 'size', 'view', 'linear1', 'relu_1', 'linear2', 'relu_2', 'linear']\n",
      "num_machines: 4\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    add partitions and communications to configs\n",
    "'''\n",
    "\n",
    "# gets random test input (with correct size)\n",
    "input_var = get_input_from_code(configs)\n",
    "\n",
    "# Config partitions and prune_ratio\n",
    "configs = engine.partition_generator(configs, model)\n",
    "            \n",
    "# Compute output size of each layer\n",
    "configs['partition'] = engine.featuremap_summary(model, configs['partition'], input_var)\n",
    "        \n",
    "# Setup communication costs\n",
    "configs['comm_costs'] = engine.set_communication_cost(model, configs['partition'],)\n",
    "\n",
    "\n",
    "# split model general parameters\n",
    "\n",
    "# make copies of model per machine\n",
    "num_machines = max(configs['partition']['bn_partition']) # TODO: double check this makes sense\n",
    "model_machines = [model]*num_machines\n",
    "\n",
    "layer_names_fx =  get_graph_node_names(model)[1]\n",
    "total_layers_fx = len(layer_names_fx)\n",
    "print(f\"Total layers: {total_layers_fx}\")\n",
    "\n",
    "split_module_names = list(configs['partition'].keys())\n",
    "\n",
    "print(layer_names_fx)\n",
    "print('num_machines:', num_machines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "residual_block_start: [ 4. 11. 18. 27. 34. 43. 50. 59. 66.]\n",
      "residual_connection_start: [23. 39. 55.]\n",
      "residual_block_end: [ 9. 16. 25. 32. 41. 48. 57. 64.]\n",
      "False\n",
      "False\n",
      "Executing module 0: x\n",
      "\tExecuting on machine 0\n",
      "\t\t-model input layer.. skipping\n",
      "\tExecuting on machine 1\n",
      "\t\t-model input layer.. skipping\n",
      "\tExecuting on machine 2\n",
      "\t\t-model input layer.. skipping\n",
      "\tExecuting on machine 3\n",
      "\t\t-model input layer.. skipping\n",
      "Finished execution of layer 0\n",
      "Input layer. Skipping comparison\n",
      "\n",
      "Executing module 1: conv1\n",
      "\tExecuting on machine 0\n",
      "\t\t-WARNING: No input assigned to this machine (but it was sent input?). Skipping...\n",
      "\tExecuting on machine 1\n",
      "current layer type: <class 'torch.nn.modules.conv.Conv2d'>\n",
      "\t\t-Splitting conv layer 1\n",
      "\t\t Output tensor shape : torch.Size([1, 64, 32, 32])\n",
      "\t\t sending C_out tensor([16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]) to machine 1\n",
      "\tExecuting on machine 2\n",
      "current layer type: <class 'torch.nn.modules.conv.Conv2d'>\n",
      "\t\t-Splitting conv layer 1\n",
      "\t\t Output tensor shape : torch.Size([1, 64, 32, 32])\n",
      "\t\t sending C_out tensor([32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]) to machine 2\n",
      "\tExecuting on machine 3\n",
      "current layer type: <class 'torch.nn.modules.conv.Conv2d'>\n",
      "\t\t-Splitting conv layer 1\n",
      "\t\t Output tensor shape : torch.Size([1, 64, 32, 32])\n",
      "\t\t sending C_out tensor([48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]) to machine 3\n",
      "Finished execution of layer 1\n",
      "Max diff:\n",
      "tensor([0.], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([], dtype=torch.int64)  (len = 0)\n",
      "passing Cout = tensor([16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "        34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51,\n",
      "        52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63])  (len = 48)\n",
      "\n",
      "Executing module 2: bn1\n",
      "\tExecuting on machine 0\n",
      "\t\t-No input received but bn still needs to produce output.\n",
      "current layer type: <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "\t\t-Splitting batch norm layer 2\n",
      "\t\t Output tensor shape : torch.Size([1, 64, 32, 32])\n",
      "\t\t sending C_out tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15]) to machine 0\n",
      "\tExecuting on machine 1\n",
      "current layer type: <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "\t\t-Splitting batch norm layer 2\n",
      "\t\t Output tensor shape : torch.Size([1, 64, 32, 32])\n",
      "\t\t sending C_out tensor([16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]) to machine 1\n",
      "\tExecuting on machine 2\n",
      "current layer type: <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "\t\t-Splitting batch norm layer 2\n",
      "\t\t Output tensor shape : torch.Size([1, 64, 32, 32])\n",
      "\t\t sending C_out tensor([32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]) to machine 2\n",
      "\tExecuting on machine 3\n",
      "current layer type: <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "\t\t-Splitting batch norm layer 2\n",
      "\t\t Output tensor shape : torch.Size([1, 64, 32, 32])\n",
      "\t\t sending C_out tensor([48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]) to machine 3\n",
      "Finished execution of layer 2\n",
      "Max diff:\n",
      "tensor([0.], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([], dtype=torch.int64)  (len = 0)\n",
      "passing Cout = tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
      "        54, 55, 56, 57, 58, 59, 60, 61, 62, 63])  (len = 64)\n",
      "\n",
      "Executing module 3: relu\n",
      "\tExecuting on machine 0\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 1\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 2\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 3\n",
      "\t\t-Applying ReLU\n",
      "Finished execution of layer 3\n",
      "Max diff:\n",
      "tensor([0.], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([], dtype=torch.int64)  (len = 0)\n",
      "passing Cout = tensor([ 1,  6, 13, 14, 16, 17, 18, 19, 20, 21, 22, 24, 25, 26, 27, 28, 29, 30,\n",
      "        31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48,\n",
      "        49, 50, 51, 52, 53, 55, 56, 57, 58, 59, 60, 61, 62, 63])  (len = 50)\n",
      "\n",
      "Executing module 4: layer1.0.conv1\n",
      "\tExecuting on machine 0\n",
      "\t\t-Saving input for later...\n",
      "current layer type: <class 'torch.nn.modules.conv.Conv2d'>\n",
      "\t\t-Splitting conv layer 4\n",
      "\t\t Output tensor shape : torch.Size([1, 64, 32, 32])\n",
      "\t\t sending C_out tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15]) to machine 0\n",
      "\t\t sending C_out tensor([37]) to machine 2\n",
      "\t\t sending C_out tensor([49, 60]) to machine 3\n",
      "\tExecuting on machine 1\n",
      "\t\t-Saving input for later...\n",
      "current layer type: <class 'torch.nn.modules.conv.Conv2d'>\n",
      "\t\t-Splitting conv layer 4\n",
      "\t\t Output tensor shape : torch.Size([1, 64, 32, 32])\n",
      "\t\t sending C_out tensor([2, 3, 7, 8]) to machine 0\n",
      "\t\t sending C_out tensor([16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]) to machine 1\n",
      "\t\t sending C_out tensor([34]) to machine 2\n",
      "\t\t sending C_out tensor([54, 56, 57, 61, 63]) to machine 3\n",
      "\tExecuting on machine 2\n",
      "\t\t-Saving input for later...\n",
      "current layer type: <class 'torch.nn.modules.conv.Conv2d'>\n",
      "\t\t-Splitting conv layer 4\n",
      "\t\t Output tensor shape : torch.Size([1, 64, 32, 32])\n",
      "\t\t sending C_out tensor([ 7, 10, 12, 14, 15]) to machine 0\n",
      "\t\t sending C_out tensor([18, 19, 20, 30, 31]) to machine 1\n",
      "\t\t sending C_out tensor([32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]) to machine 2\n",
      "\t\t sending C_out tensor([50, 59, 61]) to machine 3\n",
      "\tExecuting on machine 3\n",
      "\t\t-Saving input for later...\n",
      "current layer type: <class 'torch.nn.modules.conv.Conv2d'>\n",
      "\t\t-Splitting conv layer 4\n",
      "\t\t Output tensor shape : torch.Size([1, 64, 32, 32])\n",
      "\t\t sending C_out tensor([1]) to machine 0\n",
      "\t\t sending C_out tensor([21, 26, 31]) to machine 1\n",
      "\t\t sending C_out tensor([36, 38]) to machine 2\n",
      "\t\t sending C_out tensor([48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]) to machine 3\n",
      "Finished execution of layer 4\n",
      "Max diff:\n",
      "tensor([3.3307e-16], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([ 1,  2,  3,  7,  8, 10, 12, 14, 15, 18, 19, 20, 21, 26, 30, 31, 34, 36,\n",
      "        37, 38, 49, 50, 54, 56, 57, 59, 60, 61, 63])  (len = 29)\n",
      "passing Cout = tensor([ 0,  4,  5,  6,  9, 11, 13, 16, 17, 22, 23, 24, 25, 27, 28, 29, 32, 33,\n",
      "        35, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 51, 52, 53, 55, 58, 62])  (len = 35)\n",
      "\n",
      "Executing module 5: layer1.0.bn1\n",
      "\tExecuting on machine 0\n",
      "current layer type: <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "\t\t-Splitting batch norm layer 5\n",
      "\t\t Output tensor shape : torch.Size([1, 64, 32, 32])\n",
      "\t\t sending C_out tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15]) to machine 0\n",
      "\tExecuting on machine 1\n",
      "current layer type: <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "\t\t-Splitting batch norm layer 5\n",
      "\t\t Output tensor shape : torch.Size([1, 64, 32, 32])\n",
      "\t\t sending C_out tensor([16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]) to machine 1\n",
      "\tExecuting on machine 2\n",
      "current layer type: <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "\t\t-Splitting batch norm layer 5\n",
      "\t\t Output tensor shape : torch.Size([1, 64, 32, 32])\n",
      "\t\t sending C_out tensor([32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]) to machine 2\n",
      "\tExecuting on machine 3\n",
      "current layer type: <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "\t\t-Splitting batch norm layer 5\n",
      "\t\t Output tensor shape : torch.Size([1, 64, 32, 32])\n",
      "\t\t sending C_out tensor([48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]) to machine 3\n",
      "Finished execution of layer 5\n",
      "Max diff:\n",
      "tensor([3.3307e-16], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([ 1,  2,  3,  7,  8, 10, 12, 14, 15, 18, 19, 20, 21, 26, 30, 31, 34, 36,\n",
      "        37, 38, 49, 50, 54, 56, 57, 59, 60, 61, 63])  (len = 29)\n",
      "passing Cout = tensor([ 0,  4,  5,  6,  9, 11, 13, 16, 17, 22, 23, 24, 25, 27, 28, 29, 32, 33,\n",
      "        35, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 51, 52, 53, 55, 58, 62])  (len = 35)\n",
      "\n",
      "Executing module 6: layer1.0.relu\n",
      "\tExecuting on machine 0\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 1\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 2\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 3\n",
      "\t\t-Applying ReLU\n",
      "Finished execution of layer 6\n",
      "Max diff:\n",
      "tensor([3.3307e-16], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([ 2,  3,  7,  8, 12, 14, 15, 18, 19, 20, 21, 26, 30, 31, 34, 36, 38, 49,\n",
      "        50, 54, 56, 59, 61, 63])  (len = 24)\n",
      "passing Cout = tensor([17, 23, 24, 27, 32, 33, 35, 39, 40, 42, 45, 47, 48, 51, 52, 55, 60])  (len = 17)\n",
      "\n",
      "Executing module 7: layer1.0.conv2\n",
      "\tExecuting on machine 0\n",
      "current layer type: <class 'torch.nn.modules.conv.Conv2d'>\n",
      "\t\t-Splitting conv layer 7\n",
      "\t\t Output tensor shape : torch.Size([1, 64, 32, 32])\n",
      "\t\t sending C_out tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15]) to machine 0\n",
      "\t\t sending C_out tensor([16, 22, 24]) to machine 1\n",
      "\t\t sending C_out tensor([36, 39, 43]) to machine 2\n",
      "\t\t sending C_out tensor([52, 55, 56, 63]) to machine 3\n",
      "\tExecuting on machine 1\n",
      "current layer type: <class 'torch.nn.modules.conv.Conv2d'>\n",
      "\t\t-Splitting conv layer 7\n",
      "\t\t Output tensor shape : torch.Size([1, 64, 32, 32])\n",
      "\t\t sending C_out tensor([ 7,  8, 14]) to machine 0\n",
      "\t\t sending C_out tensor([16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]) to machine 1\n",
      "\t\t sending C_out tensor([36, 37]) to machine 2\n",
      "\t\t sending C_out tensor([49, 53, 57]) to machine 3\n",
      "\tExecuting on machine 2\n",
      "current layer type: <class 'torch.nn.modules.conv.Conv2d'>\n",
      "\t\t-Splitting conv layer 7\n",
      "\t\t Output tensor shape : torch.Size([1, 64, 32, 32])\n",
      "\t\t sending C_out tensor([ 5,  8, 10, 13, 14]) to machine 0\n",
      "\t\t sending C_out tensor([18, 19, 21, 27, 30]) to machine 1\n",
      "\t\t sending C_out tensor([32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]) to machine 2\n",
      "\t\t sending C_out tensor([58, 63]) to machine 3\n",
      "\tExecuting on machine 3\n",
      "current layer type: <class 'torch.nn.modules.conv.Conv2d'>\n",
      "\t\t-Splitting conv layer 7\n",
      "\t\t Output tensor shape : torch.Size([1, 64, 32, 32])\n",
      "\t\t sending C_out tensor([ 1,  2,  4,  8, 13]) to machine 0\n",
      "\t\t sending C_out tensor([37, 46]) to machine 2\n",
      "\t\t sending C_out tensor([48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]) to machine 3\n",
      "Finished execution of layer 7\n",
      "Max diff:\n",
      "tensor([1.1102e-16], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
      "        54, 55, 56, 57, 58, 59, 60, 61, 62, 63])  (len = 64)\n",
      "passing Cout = tensor([], dtype=torch.int64)  (len = 0)\n",
      "\n",
      "Executing module 8: layer1.0.bn2\n",
      "\tExecuting on machine 0\n",
      "current layer type: <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "\t\t-Splitting batch norm layer 8\n",
      "\t\t Output tensor shape : torch.Size([1, 64, 32, 32])\n",
      "\t\t sending C_out tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15]) to machine 0\n",
      "\tExecuting on machine 1\n",
      "current layer type: <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "\t\t-Splitting batch norm layer 8\n",
      "\t\t Output tensor shape : torch.Size([1, 64, 32, 32])\n",
      "\t\t sending C_out tensor([16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]) to machine 1\n",
      "\tExecuting on machine 2\n",
      "current layer type: <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "\t\t-Splitting batch norm layer 8\n",
      "\t\t Output tensor shape : torch.Size([1, 64, 32, 32])\n",
      "\t\t sending C_out tensor([32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]) to machine 2\n",
      "\tExecuting on machine 3\n",
      "current layer type: <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "\t\t-Splitting batch norm layer 8\n",
      "\t\t Output tensor shape : torch.Size([1, 64, 32, 32])\n",
      "\t\t sending C_out tensor([48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]) to machine 3\n",
      "Finished execution of layer 8\n",
      "Max diff:\n",
      "tensor([1.1102e-16], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
      "        54, 55, 56, 57, 58, 59, 60, 61, 62, 63])  (len = 64)\n",
      "passing Cout = tensor([], dtype=torch.int64)  (len = 0)\n",
      "\n",
      "Executing module 9: layer1.0.add\n",
      "\tExecuting on machine 0\n",
      "\t\t-adding residual\n",
      "\t\t-assuming shortcut had no layers\n",
      "\tExecuting on machine 1\n",
      "\t\t-adding residual\n",
      "\t\t-assuming shortcut had no layers\n",
      "\tExecuting on machine 2\n",
      "\t\t-adding residual\n",
      "\t\t-assuming shortcut had no layers\n",
      "\tExecuting on machine 3\n",
      "\t\t-adding residual\n",
      "\t\t-assuming shortcut had no layers\n",
      "Finished execution of layer 9\n",
      "Max diff:\n",
      "tensor([1.1102e-16], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
      "        54, 55, 56, 57, 58, 59, 60, 61, 62, 63])  (len = 64)\n",
      "passing Cout = tensor([], dtype=torch.int64)  (len = 0)\n",
      "\n",
      "Executing module 10: layer1.0.relu_1\n",
      "\tExecuting on machine 0\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 1\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 2\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 3\n",
      "\t\t-Applying ReLU\n",
      "Finished execution of layer 10\n",
      "Max diff:\n",
      "tensor([1.1102e-16], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([ 0,  2,  6,  8, 14, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28,\n",
      "        29, 30, 31, 32, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47,\n",
      "        48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63])  (len = 52)\n",
      "passing Cout = tensor([13, 33])  (len = 2)\n",
      "\n",
      "Executing module 11: layer1.1.conv1\n",
      "\tExecuting on machine 0\n",
      "\t\t-Saving input for later...\n",
      "current layer type: <class 'torch.nn.modules.conv.Conv2d'>\n",
      "\t\t-Splitting conv layer 11\n",
      "\t\t Output tensor shape : torch.Size([1, 64, 32, 32])\n",
      "\t\t sending C_out tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15]) to machine 0\n",
      "\t\t sending C_out tensor([37]) to machine 2\n",
      "\t\t sending C_out tensor([55]) to machine 3\n",
      "\tExecuting on machine 1\n",
      "\t\t-Saving input for later...\n",
      "current layer type: <class 'torch.nn.modules.conv.Conv2d'>\n",
      "\t\t-Splitting conv layer 11\n",
      "\t\t Output tensor shape : torch.Size([1, 64, 32, 32])\n",
      "\t\t sending C_out tensor([ 1,  2,  4,  8, 13, 14, 15]) to machine 0\n",
      "\t\t sending C_out tensor([16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]) to machine 1\n",
      "\t\t sending C_out tensor([34, 35, 38, 43, 46]) to machine 2\n",
      "\t\t sending C_out tensor([51, 52, 54, 56, 63]) to machine 3\n",
      "\tExecuting on machine 2\n",
      "\t\t-Saving input for later...\n",
      "current layer type: <class 'torch.nn.modules.conv.Conv2d'>\n",
      "\t\t-Splitting conv layer 11\n",
      "\t\t Output tensor shape : torch.Size([1, 64, 32, 32])\n",
      "\t\t sending C_out tensor([1, 2, 4, 6]) to machine 0\n",
      "\t\t sending C_out tensor([27, 28, 30, 31]) to machine 1\n",
      "\t\t sending C_out tensor([32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]) to machine 2\n",
      "\t\t sending C_out tensor([49, 54, 55, 56, 57, 61]) to machine 3\n",
      "\tExecuting on machine 3\n",
      "\t\t-Saving input for later...\n",
      "current layer type: <class 'torch.nn.modules.conv.Conv2d'>\n",
      "\t\t-Splitting conv layer 11\n",
      "\t\t Output tensor shape : torch.Size([1, 64, 32, 32])\n",
      "\t\t sending C_out tensor([ 1,  5,  7,  8, 12]) to machine 0\n",
      "\t\t sending C_out tensor([18, 26, 28]) to machine 1\n",
      "\t\t sending C_out tensor([32, 40, 41, 45]) to machine 2\n",
      "\t\t sending C_out tensor([48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]) to machine 3\n",
      "Finished execution of layer 11\n",
      "Max diff:\n",
      "tensor([1.3878e-16], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
      "        54, 55, 56, 57, 58, 59, 60, 61, 62, 63])  (len = 64)\n",
      "passing Cout = tensor([], dtype=torch.int64)  (len = 0)\n",
      "\n",
      "Executing module 12: layer1.1.bn1\n",
      "\tExecuting on machine 0\n",
      "current layer type: <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "\t\t-Splitting batch norm layer 12\n",
      "\t\t Output tensor shape : torch.Size([1, 64, 32, 32])\n",
      "\t\t sending C_out tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15]) to machine 0\n",
      "\tExecuting on machine 1\n",
      "current layer type: <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "\t\t-Splitting batch norm layer 12\n",
      "\t\t Output tensor shape : torch.Size([1, 64, 32, 32])\n",
      "\t\t sending C_out tensor([16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]) to machine 1\n",
      "\tExecuting on machine 2\n",
      "current layer type: <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "\t\t-Splitting batch norm layer 12\n",
      "\t\t Output tensor shape : torch.Size([1, 64, 32, 32])\n",
      "\t\t sending C_out tensor([32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]) to machine 2\n",
      "\tExecuting on machine 3\n",
      "current layer type: <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "\t\t-Splitting batch norm layer 12\n",
      "\t\t Output tensor shape : torch.Size([1, 64, 32, 32])\n",
      "\t\t sending C_out tensor([48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]) to machine 3\n",
      "Finished execution of layer 12\n",
      "Max diff:\n",
      "tensor([1.3878e-16], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
      "        54, 55, 56, 57, 58, 59, 60, 61, 62, 63])  (len = 64)\n",
      "passing Cout = tensor([], dtype=torch.int64)  (len = 0)\n",
      "\n",
      "Executing module 13: layer1.1.relu\n",
      "\tExecuting on machine 0\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 1\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 2\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 3\n",
      "\t\t-Applying ReLU\n",
      "Finished execution of layer 13\n",
      "Max diff:\n",
      "tensor([1.1084e-16], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([ 0,  1,  3,  5,  6, 12, 14, 17, 18, 20, 21, 23, 25, 27, 28, 29, 30, 31,\n",
      "        33, 35, 36, 37, 38, 39, 40, 41, 42, 44, 45, 47, 48, 49, 51, 52, 54, 56,\n",
      "        57, 58, 59, 60, 61, 62, 63])  (len = 43)\n",
      "passing Cout = tensor([ 8, 19, 24])  (len = 3)\n",
      "\n",
      "Executing module 14: layer1.1.conv2\n",
      "\tExecuting on machine 0\n",
      "current layer type: <class 'torch.nn.modules.conv.Conv2d'>\n",
      "\t\t-Splitting conv layer 14\n",
      "\t\t Output tensor shape : torch.Size([1, 64, 32, 32])\n",
      "\t\t sending C_out tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15]) to machine 0\n",
      "\t\t sending C_out tensor([19, 22, 23, 31]) to machine 1\n",
      "\t\t sending C_out tensor([37, 38]) to machine 2\n",
      "\t\t sending C_out tensor([50]) to machine 3\n",
      "\tExecuting on machine 1\n",
      "current layer type: <class 'torch.nn.modules.conv.Conv2d'>\n",
      "\t\t-Splitting conv layer 14\n",
      "\t\t Output tensor shape : torch.Size([1, 64, 32, 32])\n",
      "\t\t sending C_out tensor([ 1,  8,  9, 12]) to machine 0\n",
      "\t\t sending C_out tensor([16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]) to machine 1\n",
      "\t\t sending C_out tensor([37, 38, 39, 41]) to machine 2\n",
      "\t\t sending C_out tensor([52, 57, 59, 61]) to machine 3\n",
      "\tExecuting on machine 2\n",
      "current layer type: <class 'torch.nn.modules.conv.Conv2d'>\n",
      "\t\t-Splitting conv layer 14\n",
      "\t\t Output tensor shape : torch.Size([1, 64, 32, 32])\n",
      "\t\t sending C_out tensor([6, 9]) to machine 0\n",
      "\t\t sending C_out tensor([27, 29, 31]) to machine 1\n",
      "\t\t sending C_out tensor([32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]) to machine 2\n",
      "\t\t sending C_out tensor([54, 61]) to machine 3\n",
      "\tExecuting on machine 3\n",
      "current layer type: <class 'torch.nn.modules.conv.Conv2d'>\n",
      "\t\t-Splitting conv layer 14\n",
      "\t\t Output tensor shape : torch.Size([1, 64, 32, 32])\n",
      "\t\t sending C_out tensor([5, 6, 9]) to machine 0\n",
      "\t\t sending C_out tensor([22, 23, 26, 27, 29, 30]) to machine 1\n",
      "\t\t sending C_out tensor([38, 45]) to machine 2\n",
      "\t\t sending C_out tensor([48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]) to machine 3\n",
      "Finished execution of layer 14\n",
      "Max diff:\n",
      "tensor([8.3267e-17], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
      "        54, 55, 56, 57, 58, 59, 60, 61, 62, 63])  (len = 64)\n",
      "passing Cout = tensor([], dtype=torch.int64)  (len = 0)\n",
      "\n",
      "Executing module 15: layer1.1.bn2\n",
      "\tExecuting on machine 0\n",
      "current layer type: <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "\t\t-Splitting batch norm layer 15\n",
      "\t\t Output tensor shape : torch.Size([1, 64, 32, 32])\n",
      "\t\t sending C_out tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15]) to machine 0\n",
      "\tExecuting on machine 1\n",
      "current layer type: <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "\t\t-Splitting batch norm layer 15\n",
      "\t\t Output tensor shape : torch.Size([1, 64, 32, 32])\n",
      "\t\t sending C_out tensor([16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]) to machine 1\n",
      "\tExecuting on machine 2\n",
      "current layer type: <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "\t\t-Splitting batch norm layer 15\n",
      "\t\t Output tensor shape : torch.Size([1, 64, 32, 32])\n",
      "\t\t sending C_out tensor([32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]) to machine 2\n",
      "\tExecuting on machine 3\n",
      "current layer type: <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "\t\t-Splitting batch norm layer 15\n",
      "\t\t Output tensor shape : torch.Size([1, 64, 32, 32])\n",
      "\t\t sending C_out tensor([48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]) to machine 3\n",
      "Finished execution of layer 15\n",
      "Max diff:\n",
      "tensor([8.3267e-17], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
      "        54, 55, 56, 57, 58, 59, 60, 61, 62, 63])  (len = 64)\n",
      "passing Cout = tensor([], dtype=torch.int64)  (len = 0)\n",
      "\n",
      "Executing module 16: layer1.1.add\n",
      "\tExecuting on machine 0\n",
      "\t\t-adding residual\n",
      "\t\t-assuming shortcut had no layers\n",
      "\tExecuting on machine 1\n",
      "\t\t-adding residual\n",
      "\t\t-assuming shortcut had no layers\n",
      "\tExecuting on machine 2\n",
      "\t\t-adding residual\n",
      "\t\t-assuming shortcut had no layers\n",
      "\tExecuting on machine 3\n",
      "\t\t-adding residual\n",
      "\t\t-assuming shortcut had no layers\n",
      "Finished execution of layer 16\n",
      "Max diff:\n",
      "tensor([1.3878e-16], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
      "        54, 55, 56, 57, 58, 59, 60, 61, 62, 63])  (len = 64)\n",
      "passing Cout = tensor([], dtype=torch.int64)  (len = 0)\n",
      "\n",
      "Executing module 17: layer1.1.relu_1\n",
      "\tExecuting on machine 0\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 1\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 2\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 3\n",
      "\t\t-Applying ReLU\n",
      "Finished execution of layer 17\n",
      "Max diff:\n",
      "tensor([1.3878e-16], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([ 1,  2,  6,  8,  9, 12, 14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 25, 26,\n",
      "        27, 29, 30, 31, 32, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46,\n",
      "        47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63])  (len = 53)\n",
      "passing Cout = tensor([11, 22, 28])  (len = 3)\n",
      "\n",
      "Executing module 18: layer2.0.conv1\n",
      "\tExecuting on machine 0\n",
      "\t\t-Saving input for later...\n",
      "current layer type: <class 'torch.nn.modules.conv.Conv2d'>\n",
      "\t\t-Splitting conv layer 18\n",
      "\t\t Output tensor shape : torch.Size([1, 128, 16, 16])\n",
      "\t\t sending C_out tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]) to machine 0\n",
      "\tExecuting on machine 1\n",
      "\t\t-Saving input for later...\n",
      "current layer type: <class 'torch.nn.modules.conv.Conv2d'>\n",
      "\t\t-Splitting conv layer 18\n",
      "\t\t Output tensor shape : torch.Size([1, 128, 16, 16])\n",
      "\t\t sending C_out tensor([32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49,\n",
      "        50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]) to machine 1\n",
      "\tExecuting on machine 2\n",
      "\t\t-Saving input for later...\n",
      "current layer type: <class 'torch.nn.modules.conv.Conv2d'>\n",
      "\t\t-Splitting conv layer 18\n",
      "\t\t Output tensor shape : torch.Size([1, 128, 16, 16])\n",
      "\t\t sending C_out tensor([64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81,\n",
      "        82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95]) to machine 2\n",
      "\tExecuting on machine 3\n",
      "\t\t-Saving input for later...\n",
      "current layer type: <class 'torch.nn.modules.conv.Conv2d'>\n",
      "\t\t-Splitting conv layer 18\n",
      "\t\t Output tensor shape : torch.Size([1, 128, 16, 16])\n",
      "\t\t sending C_out tensor([ 96,  97,  98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109,\n",
      "        110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123,\n",
      "        124, 125, 126, 127]) to machine 3\n",
      "Finished execution of layer 18\n",
      "Max diff:\n",
      "tensor([1.3878e-16], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127])  (len = 128)\n",
      "passing Cout = tensor([], dtype=torch.int64)  (len = 0)\n",
      "\n",
      "Executing module 19: layer2.0.bn1\n",
      "\tExecuting on machine 0\n",
      "current layer type: <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "\t\t-Splitting batch norm layer 19\n",
      "\t\t Output tensor shape : torch.Size([1, 128, 16, 16])\n",
      "\t\t sending C_out tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]) to machine 0\n",
      "\tExecuting on machine 1\n",
      "current layer type: <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "\t\t-Splitting batch norm layer 19\n",
      "\t\t Output tensor shape : torch.Size([1, 128, 16, 16])\n",
      "\t\t sending C_out tensor([32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49,\n",
      "        50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]) to machine 1\n",
      "\tExecuting on machine 2\n",
      "current layer type: <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "\t\t-Splitting batch norm layer 19\n",
      "\t\t Output tensor shape : torch.Size([1, 128, 16, 16])\n",
      "\t\t sending C_out tensor([64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81,\n",
      "        82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95]) to machine 2\n",
      "\tExecuting on machine 3\n",
      "current layer type: <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "\t\t-Splitting batch norm layer 19\n",
      "\t\t Output tensor shape : torch.Size([1, 128, 16, 16])\n",
      "\t\t sending C_out tensor([ 96,  97,  98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109,\n",
      "        110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123,\n",
      "        124, 125, 126, 127]) to machine 3\n",
      "Finished execution of layer 19\n",
      "Max diff:\n",
      "tensor([1.6653e-16], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127])  (len = 128)\n",
      "passing Cout = tensor([], dtype=torch.int64)  (len = 0)\n",
      "\n",
      "Executing module 20: layer2.0.relu\n",
      "\tExecuting on machine 0\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 1\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 2\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 3\n",
      "\t\t-Applying ReLU\n",
      "Finished execution of layer 20\n",
      "Max diff:\n",
      "tensor([1.1102e-16], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([  0,   1,   4,   6,   7,   9,  10,  11,  13,  16,  17,  19,  21,  25,\n",
      "         26,  27,  28,  31,  34,  35,  36,  37,  38,  39,  40,  41,  46,  47,\n",
      "         48,  49,  51,  52,  53,  55,  57,  58,  59,  61,  62,  63,  68,  69,\n",
      "         70,  72,  73,  74,  75,  76,  77,  78,  81,  82,  84,  90,  91,  94,\n",
      "         95,  96,  97,  98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108,\n",
      "        109, 111, 112, 113, 115, 117, 120, 121, 122, 124, 126, 127])  (len = 82)\n",
      "passing Cout = tensor([43, 60, 85])  (len = 3)\n",
      "\n",
      "Executing module 21: layer2.0.conv2\n",
      "\tExecuting on machine 0\n",
      "current layer type: <class 'torch.nn.modules.conv.Conv2d'>\n",
      "\t\t-Splitting conv layer 21\n",
      "\t\t Output tensor shape : torch.Size([1, 128, 16, 16])\n",
      "\t\t sending C_out tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]) to machine 0\n",
      "\tExecuting on machine 1\n",
      "current layer type: <class 'torch.nn.modules.conv.Conv2d'>\n",
      "\t\t-Splitting conv layer 21\n",
      "\t\t Output tensor shape : torch.Size([1, 128, 16, 16])\n",
      "\t\t sending C_out tensor([32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49,\n",
      "        50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]) to machine 1\n",
      "\tExecuting on machine 2\n",
      "current layer type: <class 'torch.nn.modules.conv.Conv2d'>\n",
      "\t\t-Splitting conv layer 21\n",
      "\t\t Output tensor shape : torch.Size([1, 128, 16, 16])\n",
      "\t\t sending C_out tensor([64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81,\n",
      "        82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95]) to machine 2\n",
      "\t\t sending C_out tensor([123]) to machine 3\n",
      "\tExecuting on machine 3\n",
      "current layer type: <class 'torch.nn.modules.conv.Conv2d'>\n",
      "\t\t-Splitting conv layer 21\n",
      "\t\t Output tensor shape : torch.Size([1, 128, 16, 16])\n",
      "\t\t sending C_out tensor([ 96,  97,  98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109,\n",
      "        110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123,\n",
      "        124, 125, 126, 127]) to machine 3\n",
      "Finished execution of layer 21\n",
      "Max diff:\n",
      "tensor([8.3267e-17], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127])  (len = 128)\n",
      "passing Cout = tensor([], dtype=torch.int64)  (len = 0)\n",
      "\n",
      "Executing module 22: layer2.0.bn2\n",
      "\tExecuting on machine 0\n",
      "current layer type: <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "\t\t-Splitting batch norm layer 22\n",
      "\t\t Output tensor shape : torch.Size([1, 128, 16, 16])\n",
      "\t\t sending C_out tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]) to machine 0\n",
      "\tExecuting on machine 1\n",
      "current layer type: <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "\t\t-Splitting batch norm layer 22\n",
      "\t\t Output tensor shape : torch.Size([1, 128, 16, 16])\n",
      "\t\t sending C_out tensor([32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49,\n",
      "        50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]) to machine 1\n",
      "\tExecuting on machine 2\n",
      "current layer type: <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "\t\t-Splitting batch norm layer 22\n",
      "\t\t Output tensor shape : torch.Size([1, 128, 16, 16])\n",
      "\t\t sending C_out tensor([64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81,\n",
      "        82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95]) to machine 2\n",
      "\tExecuting on machine 3\n",
      "current layer type: <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "\t\t-Splitting batch norm layer 22\n",
      "\t\t Output tensor shape : torch.Size([1, 128, 16, 16])\n",
      "\t\t sending C_out tensor([ 96,  97,  98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109,\n",
      "        110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123,\n",
      "        124, 125, 126, 127]) to machine 3\n",
      "Finished execution of layer 22\n",
      "Max diff:\n",
      "tensor([8.3267e-17], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127])  (len = 128)\n",
      "passing Cout = tensor([], dtype=torch.int64)  (len = 0)\n",
      "\n",
      "Executing module 23: layer2.0.shortcut.0\n",
      "\tExecuting on machine 0\n",
      "\t\t-Saving current input. Swapping for input saved from start of block\n",
      "current layer type: <class 'torch.nn.modules.conv.Conv2d'>\n",
      "\t\t-Splitting conv layer 23\n",
      "\t\t Output tensor shape : torch.Size([1, 128, 16, 16])\n",
      "\t\t sending C_out tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]) to machine 0\n",
      "\t\t sending C_out tensor([34, 35, 40, 41, 42, 43, 46, 52, 58, 63]) to machine 1\n",
      "\t\t sending C_out tensor([64, 76, 81, 92]) to machine 2\n",
      "\t\t sending C_out tensor([ 99, 104, 109, 119, 126]) to machine 3\n",
      "\tExecuting on machine 1\n",
      "\t\t-Saving current input. Swapping for input saved from start of block\n",
      "current layer type: <class 'torch.nn.modules.conv.Conv2d'>\n",
      "\t\t-Splitting conv layer 23\n",
      "\t\t Output tensor shape : torch.Size([1, 128, 16, 16])\n",
      "\t\t sending C_out tensor([ 2,  3,  4,  6, 11, 14, 24, 26, 27, 29]) to machine 0\n",
      "\t\t sending C_out tensor([32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49,\n",
      "        50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]) to machine 1\n",
      "\t\t sending C_out tensor([65, 84, 85, 92, 93]) to machine 2\n",
      "\t\t sending C_out tensor([ 98, 102, 110, 111, 113, 115, 116, 118, 120, 122, 124, 125]) to machine 3\n",
      "\tExecuting on machine 2\n",
      "\t\t-Saving current input. Swapping for input saved from start of block\n",
      "current layer type: <class 'torch.nn.modules.conv.Conv2d'>\n",
      "\t\t-Splitting conv layer 23\n",
      "\t\t Output tensor shape : torch.Size([1, 128, 16, 16])\n",
      "\t\t sending C_out tensor([ 0,  4,  9, 11, 16, 18, 23, 24, 27, 31]) to machine 0\n",
      "\t\t sending C_out tensor([32, 38, 48, 49, 51, 52, 55, 59, 63]) to machine 1\n",
      "\t\t sending C_out tensor([64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81,\n",
      "        82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95]) to machine 2\n",
      "\t\t sending C_out tensor([ 98,  99, 104, 108, 111, 112, 113, 121]) to machine 3\n",
      "\tExecuting on machine 3\n",
      "\t\t-Saving current input. Swapping for input saved from start of block\n",
      "current layer type: <class 'torch.nn.modules.conv.Conv2d'>\n",
      "\t\t-Splitting conv layer 23\n",
      "\t\t Output tensor shape : torch.Size([1, 128, 16, 16])\n",
      "\t\t sending C_out tensor([ 2,  7,  9, 12, 16, 17, 19, 21, 22, 23, 25, 27]) to machine 0\n",
      "\t\t sending C_out tensor([33, 36, 37, 41, 56, 58]) to machine 1\n",
      "\t\t sending C_out tensor([65, 66, 68, 69, 71, 74, 77, 79, 81, 83, 86, 88]) to machine 2\n",
      "\t\t sending C_out tensor([ 96,  97,  98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109,\n",
      "        110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123,\n",
      "        124, 125, 126, 127]) to machine 3\n",
      "Finished execution of layer 23\n",
      "Max diff:\n",
      "tensor([5.5511e-17], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127])  (len = 128)\n",
      "passing Cout = tensor([], dtype=torch.int64)  (len = 0)\n",
      "\n",
      "Executing module 24: layer2.0.shortcut.1\n",
      "\tExecuting on machine 0\n",
      "current layer type: <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "\t\t-Splitting batch norm layer 24\n",
      "\t\t Output tensor shape : torch.Size([1, 128, 16, 16])\n",
      "\t\t sending C_out tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]) to machine 0\n",
      "\tExecuting on machine 1\n",
      "current layer type: <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "\t\t-Splitting batch norm layer 24\n",
      "\t\t Output tensor shape : torch.Size([1, 128, 16, 16])\n",
      "\t\t sending C_out tensor([32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49,\n",
      "        50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]) to machine 1\n",
      "\tExecuting on machine 2\n",
      "current layer type: <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "\t\t-Splitting batch norm layer 24\n",
      "\t\t Output tensor shape : torch.Size([1, 128, 16, 16])\n",
      "\t\t sending C_out tensor([64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81,\n",
      "        82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95]) to machine 2\n",
      "\tExecuting on machine 3\n",
      "current layer type: <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "\t\t-Splitting batch norm layer 24\n",
      "\t\t Output tensor shape : torch.Size([1, 128, 16, 16])\n",
      "\t\t sending C_out tensor([ 96,  97,  98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109,\n",
      "        110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123,\n",
      "        124, 125, 126, 127]) to machine 3\n",
      "Finished execution of layer 24\n",
      "Max diff:\n",
      "tensor([5.5511e-17], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127])  (len = 128)\n",
      "passing Cout = tensor([], dtype=torch.int64)  (len = 0)\n",
      "\n",
      "Executing module 25: layer2.0.add\n",
      "\tExecuting on machine 0\n",
      "\t\t-adding residual\n",
      "\tExecuting on machine 1\n",
      "\t\t-adding residual\n",
      "\tExecuting on machine 2\n",
      "\t\t-adding residual\n",
      "\tExecuting on machine 3\n",
      "\t\t-adding residual\n",
      "Finished execution of layer 25\n",
      "Max diff:\n",
      "tensor([1.1102e-16], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127])  (len = 128)\n",
      "passing Cout = tensor([], dtype=torch.int64)  (len = 0)\n",
      "\n",
      "Executing module 26: layer2.0.relu_1\n",
      "\tExecuting on machine 0\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 1\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 2\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 3\n",
      "\t\t-Applying ReLU\n",
      "Finished execution of layer 26\n",
      "Max diff:\n",
      "tensor([1.1102e-16], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([  2,   5,   6,   7,   8,  10,  11,  13,  14,  15,  16,  17,  18,  21,\n",
      "         22,  23,  24,  26,  27,  28,  29,  30,  31,  32,  33,  34,  36,  37,\n",
      "         38,  39,  40,  41,  42,  45,  46,  47,  48,  49,  50,  51,  52,  53,\n",
      "         54,  55,  56,  57,  58,  59,  60,  61,  62,  65,  66,  67,  69,  70,\n",
      "         71,  72,  74,  75,  76,  77,  78,  79,  80,  81,  83,  84,  85,  86,\n",
      "         87,  88,  89,  90,  91,  92,  93,  94,  96,  97,  98,  99, 100, 102,\n",
      "        103, 104, 106, 108, 109, 110, 112, 113, 114, 115, 116, 117, 118, 119,\n",
      "        120, 121, 122, 123, 124, 126, 127])  (len = 105)\n",
      "passing Cout = tensor([], dtype=torch.int64)  (len = 0)\n",
      "\n",
      "Executing module 27: layer2.1.conv1\n",
      "\tExecuting on machine 0\n",
      "\t\t-Saving input for later...\n",
      "current layer type: <class 'torch.nn.modules.conv.Conv2d'>\n",
      "\t\t-Splitting conv layer 27\n",
      "\t\t Output tensor shape : torch.Size([1, 128, 16, 16])\n",
      "\t\t sending C_out tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]) to machine 0\n",
      "\tExecuting on machine 1\n",
      "\t\t-Saving input for later...\n",
      "current layer type: <class 'torch.nn.modules.conv.Conv2d'>\n",
      "\t\t-Splitting conv layer 27\n",
      "\t\t Output tensor shape : torch.Size([1, 128, 16, 16])\n",
      "\t\t sending C_out tensor([32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49,\n",
      "        50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]) to machine 1\n",
      "\t\t sending C_out tensor([122]) to machine 3\n",
      "\tExecuting on machine 2\n",
      "\t\t-Saving input for later...\n",
      "current layer type: <class 'torch.nn.modules.conv.Conv2d'>\n",
      "\t\t-Splitting conv layer 27\n",
      "\t\t Output tensor shape : torch.Size([1, 128, 16, 16])\n",
      "\t\t sending C_out tensor([64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81,\n",
      "        82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95]) to machine 2\n",
      "\tExecuting on machine 3\n",
      "\t\t-Saving input for later...\n",
      "current layer type: <class 'torch.nn.modules.conv.Conv2d'>\n",
      "\t\t-Splitting conv layer 27\n",
      "\t\t Output tensor shape : torch.Size([1, 128, 16, 16])\n",
      "\t\t sending C_out tensor([ 96,  97,  98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109,\n",
      "        110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123,\n",
      "        124, 125, 126, 127]) to machine 3\n",
      "Finished execution of layer 27\n",
      "Max diff:\n",
      "tensor([1.1102e-16], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127])  (len = 128)\n",
      "passing Cout = tensor([], dtype=torch.int64)  (len = 0)\n",
      "\n",
      "Executing module 28: layer2.1.bn1\n",
      "\tExecuting on machine 0\n",
      "current layer type: <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "\t\t-Splitting batch norm layer 28\n",
      "\t\t Output tensor shape : torch.Size([1, 128, 16, 16])\n",
      "\t\t sending C_out tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]) to machine 0\n",
      "\tExecuting on machine 1\n",
      "current layer type: <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "\t\t-Splitting batch norm layer 28\n",
      "\t\t Output tensor shape : torch.Size([1, 128, 16, 16])\n",
      "\t\t sending C_out tensor([32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49,\n",
      "        50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]) to machine 1\n",
      "\tExecuting on machine 2\n",
      "current layer type: <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "\t\t-Splitting batch norm layer 28\n",
      "\t\t Output tensor shape : torch.Size([1, 128, 16, 16])\n",
      "\t\t sending C_out tensor([64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81,\n",
      "        82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95]) to machine 2\n",
      "\tExecuting on machine 3\n",
      "current layer type: <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "\t\t-Splitting batch norm layer 28\n",
      "\t\t Output tensor shape : torch.Size([1, 128, 16, 16])\n",
      "\t\t sending C_out tensor([ 96,  97,  98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109,\n",
      "        110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123,\n",
      "        124, 125, 126, 127]) to machine 3\n",
      "Finished execution of layer 28\n",
      "Max diff:\n",
      "tensor([1.1102e-16], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127])  (len = 128)\n",
      "passing Cout = tensor([], dtype=torch.int64)  (len = 0)\n",
      "\n",
      "Executing module 29: layer2.1.relu\n",
      "\tExecuting on machine 0\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 1\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 2\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 3\n",
      "\t\t-Applying ReLU\n",
      "Finished execution of layer 29\n",
      "Max diff:\n",
      "tensor([2.7756e-17], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([  1,   3,   5,  10,  11,  15,  17,  18,  19,  20,  24,  26,  27,  28,\n",
      "         29,  30,  32,  33,  35,  36,  37,  38,  39,  40,  47,  48,  50,  51,\n",
      "         55,  57,  66,  67,  75,  90,  93,  95,  96,  99, 102, 103, 105, 107,\n",
      "        110, 111, 113, 114, 115, 116, 121, 122, 123, 124, 125, 127])  (len = 54)\n",
      "passing Cout = tensor([60])  (len = 1)\n",
      "\n",
      "Executing module 30: layer2.1.conv2\n",
      "\tExecuting on machine 0\n",
      "current layer type: <class 'torch.nn.modules.conv.Conv2d'>\n",
      "\t\t-Splitting conv layer 30\n",
      "\t\t Output tensor shape : torch.Size([1, 128, 16, 16])\n",
      "\t\t sending C_out tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]) to machine 0\n",
      "\tExecuting on machine 1\n",
      "current layer type: <class 'torch.nn.modules.conv.Conv2d'>\n",
      "\t\t-Splitting conv layer 30\n",
      "\t\t Output tensor shape : torch.Size([1, 128, 16, 16])\n",
      "\t\t sending C_out tensor([32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49,\n",
      "        50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]) to machine 1\n",
      "\tExecuting on machine 2\n",
      "current layer type: <class 'torch.nn.modules.conv.Conv2d'>\n",
      "\t\t-Splitting conv layer 30\n",
      "\t\t Output tensor shape : torch.Size([1, 128, 16, 16])\n",
      "\t\t sending C_out tensor([64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81,\n",
      "        82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95]) to machine 2\n",
      "\tExecuting on machine 3\n",
      "current layer type: <class 'torch.nn.modules.conv.Conv2d'>\n",
      "\t\t-Splitting conv layer 30\n",
      "\t\t Output tensor shape : torch.Size([1, 128, 16, 16])\n",
      "\t\t sending C_out tensor([ 96,  97,  98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109,\n",
      "        110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123,\n",
      "        124, 125, 126, 127]) to machine 3\n",
      "Finished execution of layer 30\n",
      "Max diff:\n",
      "tensor([2.7756e-17], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127])  (len = 128)\n",
      "passing Cout = tensor([], dtype=torch.int64)  (len = 0)\n",
      "\n",
      "Executing module 31: layer2.1.bn2\n",
      "\tExecuting on machine 0\n",
      "current layer type: <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "\t\t-Splitting batch norm layer 31\n",
      "\t\t Output tensor shape : torch.Size([1, 128, 16, 16])\n",
      "\t\t sending C_out tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]) to machine 0\n",
      "\tExecuting on machine 1\n",
      "current layer type: <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "\t\t-Splitting batch norm layer 31\n",
      "\t\t Output tensor shape : torch.Size([1, 128, 16, 16])\n",
      "\t\t sending C_out tensor([32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49,\n",
      "        50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]) to machine 1\n",
      "\tExecuting on machine 2\n",
      "current layer type: <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "\t\t-Splitting batch norm layer 31\n",
      "\t\t Output tensor shape : torch.Size([1, 128, 16, 16])\n",
      "\t\t sending C_out tensor([64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81,\n",
      "        82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95]) to machine 2\n",
      "\tExecuting on machine 3\n",
      "current layer type: <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "\t\t-Splitting batch norm layer 31\n",
      "\t\t Output tensor shape : torch.Size([1, 128, 16, 16])\n",
      "\t\t sending C_out tensor([ 96,  97,  98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109,\n",
      "        110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123,\n",
      "        124, 125, 126, 127]) to machine 3\n",
      "Finished execution of layer 31\n",
      "Max diff:\n",
      "tensor([2.7756e-17], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127])  (len = 128)\n",
      "passing Cout = tensor([], dtype=torch.int64)  (len = 0)\n",
      "\n",
      "Executing module 32: layer2.1.add\n",
      "\tExecuting on machine 0\n",
      "\t\t-adding residual\n",
      "\t\t-assuming shortcut had no layers\n",
      "\tExecuting on machine 1\n",
      "\t\t-adding residual\n",
      "\t\t-assuming shortcut had no layers\n",
      "\tExecuting on machine 2\n",
      "\t\t-adding residual\n",
      "\t\t-assuming shortcut had no layers\n",
      "\tExecuting on machine 3\n",
      "\t\t-adding residual\n",
      "\t\t-assuming shortcut had no layers\n",
      "Finished execution of layer 32\n",
      "Max diff:\n",
      "tensor([1.1102e-16], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127])  (len = 128)\n",
      "passing Cout = tensor([], dtype=torch.int64)  (len = 0)\n",
      "\n",
      "Executing module 33: layer2.1.relu_1\n",
      "\tExecuting on machine 0\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 1\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 2\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 3\n",
      "\t\t-Applying ReLU\n",
      "Finished execution of layer 33\n",
      "Max diff:\n",
      "tensor([1.1102e-16], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([  5,   6,   7,   8,  10,  11,  12,  13,  14,  15,  16,  17,  18,  21,\n",
      "         22,  23,  24,  26,  27,  28,  29,  30,  32,  33,  36,  37,  38,  39,\n",
      "         41,  42,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,  56,  57,\n",
      "         58,  59,  60,  61,  62,  64,  66,  67,  69,  71,  72,  74,  75,  76,\n",
      "         77,  78,  79,  80,  81,  83,  84,  85,  86,  87,  88,  89,  90,  91,\n",
      "         92,  93,  94,  96,  97,  98,  99, 100, 102, 103, 104, 105, 106, 108,\n",
      "        109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122,\n",
      "        123, 124, 125, 126, 127])  (len = 103)\n",
      "passing Cout = tensor([40])  (len = 1)\n",
      "\n",
      "Executing module 34: layer3.0.conv1\n",
      "\tExecuting on machine 0\n",
      "\t\t-Saving input for later...\n",
      "current layer type: <class 'torch.nn.modules.conv.Conv2d'>\n",
      "\t\t-Splitting conv layer 34\n",
      "\t\t Output tensor shape : torch.Size([1, 256, 8, 8])\n",
      "\t\t sending C_out tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
      "        54, 55, 56, 57, 58, 59, 60, 61, 62, 63]) to machine 0\n",
      "\tExecuting on machine 1\n",
      "\t\t-Saving input for later...\n",
      "current layer type: <class 'torch.nn.modules.conv.Conv2d'>\n",
      "\t\t-Splitting conv layer 34\n",
      "\t\t Output tensor shape : torch.Size([1, 256, 8, 8])\n",
      "\t\t sending C_out tensor([ 64,  65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
      "         78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,\n",
      "         92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104, 105,\n",
      "        106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119,\n",
      "        120, 121, 122, 123, 124, 125, 126, 127]) to machine 1\n",
      "\tExecuting on machine 2\n",
      "\t\t-Saving input for later...\n",
      "current layer type: <class 'torch.nn.modules.conv.Conv2d'>\n",
      "\t\t-Splitting conv layer 34\n",
      "\t\t Output tensor shape : torch.Size([1, 256, 8, 8])\n",
      "\t\t sending C_out tensor([128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141,\n",
      "        142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
      "        156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169,\n",
      "        170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183,\n",
      "        184, 185, 186, 187, 188, 189, 190, 191]) to machine 2\n",
      "\tExecuting on machine 3\n",
      "\t\t-Saving input for later...\n",
      "current layer type: <class 'torch.nn.modules.conv.Conv2d'>\n",
      "\t\t-Splitting conv layer 34\n",
      "\t\t Output tensor shape : torch.Size([1, 256, 8, 8])\n",
      "\t\t sending C_out tensor([192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205,\n",
      "        206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219,\n",
      "        220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
      "        234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247,\n",
      "        248, 249, 250, 251, 252, 253, 254, 255]) to machine 3\n",
      "Finished execution of layer 34\n",
      "Max diff:\n",
      "tensor([1.1102e-16], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
      "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
      "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
      "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
      "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
      "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
      "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
      "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
      "        252, 253, 254, 255])  (len = 256)\n",
      "passing Cout = tensor([], dtype=torch.int64)  (len = 0)\n",
      "\n",
      "Executing module 35: layer3.0.bn1\n",
      "\tExecuting on machine 0\n",
      "current layer type: <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "\t\t-Splitting batch norm layer 35\n",
      "\t\t Output tensor shape : torch.Size([1, 256, 8, 8])\n",
      "\t\t sending C_out tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
      "        54, 55, 56, 57, 58, 59, 60, 61, 62, 63]) to machine 0\n",
      "\tExecuting on machine 1\n",
      "current layer type: <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "\t\t-Splitting batch norm layer 35\n",
      "\t\t Output tensor shape : torch.Size([1, 256, 8, 8])\n",
      "\t\t sending C_out tensor([ 64,  65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
      "         78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,\n",
      "         92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104, 105,\n",
      "        106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119,\n",
      "        120, 121, 122, 123, 124, 125, 126, 127]) to machine 1\n",
      "\tExecuting on machine 2\n",
      "current layer type: <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "\t\t-Splitting batch norm layer 35\n",
      "\t\t Output tensor shape : torch.Size([1, 256, 8, 8])\n",
      "\t\t sending C_out tensor([128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141,\n",
      "        142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
      "        156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169,\n",
      "        170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183,\n",
      "        184, 185, 186, 187, 188, 189, 190, 191]) to machine 2\n",
      "\tExecuting on machine 3\n",
      "current layer type: <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "\t\t-Splitting batch norm layer 35\n",
      "\t\t Output tensor shape : torch.Size([1, 256, 8, 8])\n",
      "\t\t sending C_out tensor([192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205,\n",
      "        206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219,\n",
      "        220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
      "        234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247,\n",
      "        248, 249, 250, 251, 252, 253, 254, 255]) to machine 3\n",
      "Finished execution of layer 35\n",
      "Max diff:\n",
      "tensor([1.1102e-16], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
      "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
      "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
      "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
      "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
      "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
      "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
      "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
      "        252, 253, 254, 255])  (len = 256)\n",
      "passing Cout = tensor([], dtype=torch.int64)  (len = 0)\n",
      "\n",
      "Executing module 36: layer3.0.relu\n",
      "\tExecuting on machine 0\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 1\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 2\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 3\n",
      "\t\t-Applying ReLU\n",
      "Finished execution of layer 36\n",
      "Max diff:\n",
      "tensor([1.1102e-16], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([  2,   7,  10,  13,  16,  21,  23,  27,  28,  30,  33,  35,  40,  42,\n",
      "         43,  47,  50,  51,  52,  55,  56,  57,  58,  59,  62,  65,  68,  70,\n",
      "         82,  91,  92,  95,  96,  99, 101, 110, 113, 120, 122, 123, 124, 127,\n",
      "        142, 157, 171, 182, 184, 192, 193, 194, 198, 199, 200, 201, 202, 205,\n",
      "        206, 207, 210, 213, 214, 216, 217, 218, 219, 221, 224, 225, 227, 230,\n",
      "        236, 237, 239, 240, 241, 242, 244, 245, 247, 248, 250, 251, 253, 255])  (len = 84)\n",
      "passing Cout = tensor([100])  (len = 1)\n",
      "\n",
      "Executing module 37: layer3.0.conv2\n",
      "\tExecuting on machine 0\n",
      "current layer type: <class 'torch.nn.modules.conv.Conv2d'>\n",
      "\t\t-Splitting conv layer 37\n",
      "\t\t Output tensor shape : torch.Size([1, 256, 8, 8])\n",
      "\t\t sending C_out tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
      "        54, 55, 56, 57, 58, 59, 60, 61, 62, 63]) to machine 0\n",
      "\tExecuting on machine 1\n",
      "current layer type: <class 'torch.nn.modules.conv.Conv2d'>\n",
      "\t\t-Splitting conv layer 37\n",
      "\t\t Output tensor shape : torch.Size([1, 256, 8, 8])\n",
      "\t\t sending C_out tensor([ 64,  65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
      "         78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,\n",
      "         92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104, 105,\n",
      "        106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119,\n",
      "        120, 121, 122, 123, 124, 125, 126, 127]) to machine 1\n",
      "\tExecuting on machine 2\n",
      "current layer type: <class 'torch.nn.modules.conv.Conv2d'>\n",
      "\t\t-Splitting conv layer 37\n",
      "\t\t Output tensor shape : torch.Size([1, 256, 8, 8])\n",
      "\t\t sending C_out tensor([128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141,\n",
      "        142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
      "        156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169,\n",
      "        170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183,\n",
      "        184, 185, 186, 187, 188, 189, 190, 191]) to machine 2\n",
      "\tExecuting on machine 3\n",
      "current layer type: <class 'torch.nn.modules.conv.Conv2d'>\n",
      "\t\t-Splitting conv layer 37\n",
      "\t\t Output tensor shape : torch.Size([1, 256, 8, 8])\n",
      "\t\t sending C_out tensor([192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205,\n",
      "        206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219,\n",
      "        220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
      "        234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247,\n",
      "        248, 249, 250, 251, 252, 253, 254, 255]) to machine 3\n",
      "Finished execution of layer 37\n",
      "Max diff:\n",
      "tensor([2.2204e-16], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
      "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
      "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
      "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
      "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
      "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
      "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
      "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
      "        252, 253, 254, 255])  (len = 256)\n",
      "passing Cout = tensor([], dtype=torch.int64)  (len = 0)\n",
      "\n",
      "Executing module 38: layer3.0.bn2\n",
      "\tExecuting on machine 0\n",
      "current layer type: <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "\t\t-Splitting batch norm layer 38\n",
      "\t\t Output tensor shape : torch.Size([1, 256, 8, 8])\n",
      "\t\t sending C_out tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
      "        54, 55, 56, 57, 58, 59, 60, 61, 62, 63]) to machine 0\n",
      "\tExecuting on machine 1\n",
      "current layer type: <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "\t\t-Splitting batch norm layer 38\n",
      "\t\t Output tensor shape : torch.Size([1, 256, 8, 8])\n",
      "\t\t sending C_out tensor([ 64,  65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
      "         78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,\n",
      "         92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104, 105,\n",
      "        106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119,\n",
      "        120, 121, 122, 123, 124, 125, 126, 127]) to machine 1\n",
      "\tExecuting on machine 2\n",
      "current layer type: <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "\t\t-Splitting batch norm layer 38\n",
      "\t\t Output tensor shape : torch.Size([1, 256, 8, 8])\n",
      "\t\t sending C_out tensor([128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141,\n",
      "        142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
      "        156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169,\n",
      "        170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183,\n",
      "        184, 185, 186, 187, 188, 189, 190, 191]) to machine 2\n",
      "\tExecuting on machine 3\n",
      "current layer type: <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "\t\t-Splitting batch norm layer 38\n",
      "\t\t Output tensor shape : torch.Size([1, 256, 8, 8])\n",
      "\t\t sending C_out tensor([192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205,\n",
      "        206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219,\n",
      "        220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
      "        234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247,\n",
      "        248, 249, 250, 251, 252, 253, 254, 255]) to machine 3\n",
      "Finished execution of layer 38\n",
      "Max diff:\n",
      "tensor([2.2204e-16], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
      "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
      "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
      "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
      "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
      "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
      "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
      "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
      "        252, 253, 254, 255])  (len = 256)\n",
      "passing Cout = tensor([], dtype=torch.int64)  (len = 0)\n",
      "\n",
      "Executing module 39: layer3.0.shortcut.0\n",
      "\tExecuting on machine 0\n",
      "\t\t-Saving current input. Swapping for input saved from start of block\n",
      "current layer type: <class 'torch.nn.modules.conv.Conv2d'>\n",
      "\t\t-Splitting conv layer 39\n",
      "\t\t Output tensor shape : torch.Size([1, 256, 8, 8])\n",
      "\t\t sending C_out tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
      "        54, 55, 56, 57, 58, 59, 60, 61, 62, 63]) to machine 0\n",
      "\t\t sending C_out tensor([ 64,  66,  74,  75,  76,  77,  84,  85,  89,  92,  96,  99, 101, 104,\n",
      "        111, 120]) to machine 1\n",
      "\t\t sending C_out tensor([129, 131, 137, 144, 146, 147, 148, 149, 176, 180, 183, 190]) to machine 2\n",
      "\t\t sending C_out tensor([206, 210, 213, 214, 223, 224, 225, 231, 233, 234, 240]) to machine 3\n",
      "\tExecuting on machine 1\n",
      "\t\t-Saving current input. Swapping for input saved from start of block\n",
      "current layer type: <class 'torch.nn.modules.conv.Conv2d'>\n",
      "\t\t-Splitting conv layer 39\n",
      "\t\t Output tensor shape : torch.Size([1, 256, 8, 8])\n",
      "\t\t sending C_out tensor([ 8, 15, 16, 17, 18, 21, 25, 32, 34, 35, 40, 41, 44, 45, 48, 51, 52, 56,\n",
      "        59]) to machine 0\n",
      "\t\t sending C_out tensor([ 64,  65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
      "         78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,\n",
      "         92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104, 105,\n",
      "        106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119,\n",
      "        120, 121, 122, 123, 124, 125, 126, 127]) to machine 1\n",
      "\t\t sending C_out tensor([134, 138, 140, 142, 143, 147, 156, 160, 164, 165, 166, 167, 168, 170,\n",
      "        177, 180, 189]) to machine 2\n",
      "\t\t sending C_out tensor([203, 204, 209, 210, 214, 220, 228, 229, 239, 240, 241, 245, 246, 247,\n",
      "        253]) to machine 3\n",
      "\tExecuting on machine 2\n",
      "\t\t-Saving current input. Swapping for input saved from start of block\n",
      "current layer type: <class 'torch.nn.modules.conv.Conv2d'>\n",
      "\t\t-Splitting conv layer 39\n",
      "\t\t Output tensor shape : torch.Size([1, 256, 8, 8])\n",
      "\t\t sending C_out tensor([ 0,  3,  5,  9, 15, 21, 22, 24, 28, 46, 54, 58, 61]) to machine 0\n",
      "\t\t sending C_out tensor([ 68,  77,  83,  85,  88,  89,  93,  99, 103, 104, 105, 114, 119, 125]) to machine 1\n",
      "\t\t sending C_out tensor([128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141,\n",
      "        142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
      "        156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169,\n",
      "        170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183,\n",
      "        184, 185, 186, 187, 188, 189, 190, 191]) to machine 2\n",
      "\t\t sending C_out tensor([200, 203, 205, 209, 217, 219, 223, 225, 230, 233, 243, 244, 248, 253]) to machine 3\n",
      "\tExecuting on machine 3\n",
      "\t\t-Saving current input. Swapping for input saved from start of block\n",
      "current layer type: <class 'torch.nn.modules.conv.Conv2d'>\n",
      "\t\t-Splitting conv layer 39\n",
      "\t\t Output tensor shape : torch.Size([1, 256, 8, 8])\n",
      "\t\t sending C_out tensor([ 3, 10, 11, 18, 25, 26, 28, 30, 41, 42, 49, 51, 56, 63]) to machine 0\n",
      "\t\t sending C_out tensor([ 68,  71,  75,  79,  85,  88,  91,  92,  98, 102, 107, 110, 111, 112,\n",
      "        114, 118, 120]) to machine 1\n",
      "\t\t sending C_out tensor([131, 137, 143, 146, 151, 153, 154, 156, 162, 167, 169, 174, 177, 178,\n",
      "        179, 182, 184, 186, 190, 191]) to machine 2\n",
      "\t\t sending C_out tensor([192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205,\n",
      "        206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219,\n",
      "        220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
      "        234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247,\n",
      "        248, 249, 250, 251, 252, 253, 254, 255]) to machine 3\n",
      "Finished execution of layer 39\n",
      "Max diff:\n",
      "tensor([4.1633e-17], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
      "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
      "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
      "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
      "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
      "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
      "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
      "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
      "        252, 253, 254, 255])  (len = 256)\n",
      "passing Cout = tensor([], dtype=torch.int64)  (len = 0)\n",
      "\n",
      "Executing module 40: layer3.0.shortcut.1\n",
      "\tExecuting on machine 0\n",
      "current layer type: <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "\t\t-Splitting batch norm layer 40\n",
      "\t\t Output tensor shape : torch.Size([1, 256, 8, 8])\n",
      "\t\t sending C_out tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
      "        54, 55, 56, 57, 58, 59, 60, 61, 62, 63]) to machine 0\n",
      "\tExecuting on machine 1\n",
      "current layer type: <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "\t\t-Splitting batch norm layer 40\n",
      "\t\t Output tensor shape : torch.Size([1, 256, 8, 8])\n",
      "\t\t sending C_out tensor([ 64,  65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
      "         78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,\n",
      "         92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104, 105,\n",
      "        106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119,\n",
      "        120, 121, 122, 123, 124, 125, 126, 127]) to machine 1\n",
      "\tExecuting on machine 2\n",
      "current layer type: <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "\t\t-Splitting batch norm layer 40\n",
      "\t\t Output tensor shape : torch.Size([1, 256, 8, 8])\n",
      "\t\t sending C_out tensor([128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141,\n",
      "        142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
      "        156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169,\n",
      "        170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183,\n",
      "        184, 185, 186, 187, 188, 189, 190, 191]) to machine 2\n",
      "\tExecuting on machine 3\n",
      "current layer type: <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "\t\t-Splitting batch norm layer 40\n",
      "\t\t Output tensor shape : torch.Size([1, 256, 8, 8])\n",
      "\t\t sending C_out tensor([192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205,\n",
      "        206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219,\n",
      "        220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
      "        234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247,\n",
      "        248, 249, 250, 251, 252, 253, 254, 255]) to machine 3\n",
      "Finished execution of layer 40\n",
      "Max diff:\n",
      "tensor([4.1633e-17], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
      "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
      "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
      "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
      "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
      "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
      "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
      "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
      "        252, 253, 254, 255])  (len = 256)\n",
      "passing Cout = tensor([], dtype=torch.int64)  (len = 0)\n",
      "\n",
      "Executing module 41: layer3.0.add\n",
      "\tExecuting on machine 0\n",
      "\t\t-adding residual\n",
      "\tExecuting on machine 1\n",
      "\t\t-adding residual\n",
      "\tExecuting on machine 2\n",
      "\t\t-adding residual\n",
      "\tExecuting on machine 3\n",
      "\t\t-adding residual\n",
      "Finished execution of layer 41\n",
      "Max diff:\n",
      "tensor([2.2204e-16], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
      "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
      "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
      "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
      "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
      "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
      "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
      "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
      "        252, 253, 254, 255])  (len = 256)\n",
      "passing Cout = tensor([], dtype=torch.int64)  (len = 0)\n",
      "\n",
      "Executing module 42: layer3.0.relu_1\n",
      "\tExecuting on machine 0\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 1\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 2\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 3\n",
      "\t\t-Applying ReLU\n",
      "Finished execution of layer 42\n",
      "Max diff:\n",
      "tensor([1.1102e-16], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([  1,   2,   8,  10,  11,  18,  20,  24,  25,  28,  35,  36,  38,  40,\n",
      "         46,  48,  51,  52,  53,  54,  57,  60,  61,  62,  67,  69,  72,  73,\n",
      "         74,  76,  79,  81,  82,  83,  84,  86,  87,  88,  89,  93,  97,  99,\n",
      "        100, 101, 102, 104, 107, 111, 114, 116, 119, 120, 123, 126, 127, 135,\n",
      "        141, 162, 167, 170, 173, 177, 183, 189, 194, 195, 197, 198, 199, 200,\n",
      "        201, 202, 203, 204, 205, 206, 208, 210, 212, 213, 216, 217, 219, 220,\n",
      "        221, 224, 226, 227, 229, 230, 232, 233, 236, 240, 241, 243, 245, 246,\n",
      "        247, 248, 250, 251, 254])  (len = 103)\n",
      "passing Cout = tensor([130, 253])  (len = 2)\n",
      "\n",
      "Executing module 43: layer3.1.conv1\n",
      "\tExecuting on machine 0\n",
      "\t\t-Saving input for later...\n",
      "current layer type: <class 'torch.nn.modules.conv.Conv2d'>\n",
      "\t\t-Splitting conv layer 43\n",
      "\t\t Output tensor shape : torch.Size([1, 256, 8, 8])\n",
      "\t\t sending C_out tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
      "        54, 55, 56, 57, 58, 59, 60, 61, 62, 63]) to machine 0\n",
      "\tExecuting on machine 1\n",
      "\t\t-Saving input for later...\n",
      "current layer type: <class 'torch.nn.modules.conv.Conv2d'>\n",
      "\t\t-Splitting conv layer 43\n",
      "\t\t Output tensor shape : torch.Size([1, 256, 8, 8])\n",
      "\t\t sending C_out tensor([ 64,  65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
      "         78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,\n",
      "         92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104, 105,\n",
      "        106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119,\n",
      "        120, 121, 122, 123, 124, 125, 126, 127]) to machine 1\n",
      "\tExecuting on machine 2\n",
      "\t\t-Saving input for later...\n",
      "current layer type: <class 'torch.nn.modules.conv.Conv2d'>\n",
      "\t\t-Splitting conv layer 43\n",
      "\t\t Output tensor shape : torch.Size([1, 256, 8, 8])\n",
      "\t\t sending C_out tensor([128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141,\n",
      "        142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
      "        156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169,\n",
      "        170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183,\n",
      "        184, 185, 186, 187, 188, 189, 190, 191]) to machine 2\n",
      "\tExecuting on machine 3\n",
      "\t\t-Saving input for later...\n",
      "current layer type: <class 'torch.nn.modules.conv.Conv2d'>\n",
      "\t\t-Splitting conv layer 43\n",
      "\t\t Output tensor shape : torch.Size([1, 256, 8, 8])\n",
      "\t\t sending C_out tensor([192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205,\n",
      "        206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219,\n",
      "        220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
      "        234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247,\n",
      "        248, 249, 250, 251, 252, 253, 254, 255]) to machine 3\n",
      "Finished execution of layer 43\n",
      "Max diff:\n",
      "tensor([2.2204e-16], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
      "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
      "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
      "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
      "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
      "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
      "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
      "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
      "        252, 253, 254, 255])  (len = 256)\n",
      "passing Cout = tensor([], dtype=torch.int64)  (len = 0)\n",
      "\n",
      "Executing module 44: layer3.1.bn1\n",
      "\tExecuting on machine 0\n",
      "current layer type: <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "\t\t-Splitting batch norm layer 44\n",
      "\t\t Output tensor shape : torch.Size([1, 256, 8, 8])\n",
      "\t\t sending C_out tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
      "        54, 55, 56, 57, 58, 59, 60, 61, 62, 63]) to machine 0\n",
      "\tExecuting on machine 1\n",
      "current layer type: <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "\t\t-Splitting batch norm layer 44\n",
      "\t\t Output tensor shape : torch.Size([1, 256, 8, 8])\n",
      "\t\t sending C_out tensor([ 64,  65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
      "         78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,\n",
      "         92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104, 105,\n",
      "        106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119,\n",
      "        120, 121, 122, 123, 124, 125, 126, 127]) to machine 1\n",
      "\tExecuting on machine 2\n",
      "current layer type: <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "\t\t-Splitting batch norm layer 44\n",
      "\t\t Output tensor shape : torch.Size([1, 256, 8, 8])\n",
      "\t\t sending C_out tensor([128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141,\n",
      "        142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
      "        156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169,\n",
      "        170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183,\n",
      "        184, 185, 186, 187, 188, 189, 190, 191]) to machine 2\n",
      "\tExecuting on machine 3\n",
      "current layer type: <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "\t\t-Splitting batch norm layer 44\n",
      "\t\t Output tensor shape : torch.Size([1, 256, 8, 8])\n",
      "\t\t sending C_out tensor([192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205,\n",
      "        206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219,\n",
      "        220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
      "        234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247,\n",
      "        248, 249, 250, 251, 252, 253, 254, 255]) to machine 3\n",
      "Finished execution of layer 44\n",
      "Max diff:\n",
      "tensor([2.2204e-16], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
      "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
      "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
      "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
      "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
      "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
      "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
      "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
      "        252, 253, 254, 255])  (len = 256)\n",
      "passing Cout = tensor([], dtype=torch.int64)  (len = 0)\n",
      "\n",
      "Executing module 45: layer3.1.relu\n",
      "\tExecuting on machine 0\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 1\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 2\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 3\n",
      "\t\t-Applying ReLU\n",
      "Finished execution of layer 45\n",
      "Max diff:\n",
      "tensor([3.4694e-17], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([  0,   1,   2,   5,  20,  25,  26,  27,  30,  31,  33,  34,  38,  46,\n",
      "         52,  54,  59,  60,  61,  64,  66,  67,  72,  76,  77,  83,  93,  97,\n",
      "        101, 103, 105, 108, 110, 129, 131, 137, 138, 142, 148, 155, 156, 163,\n",
      "        174, 191, 193, 195, 197, 199, 200, 202, 204, 205, 210, 218, 221, 222,\n",
      "        223, 224, 227, 228, 232, 239, 240, 241, 242, 243, 245, 247, 248, 250,\n",
      "        252])  (len = 71)\n",
      "passing Cout = tensor([  6,  88, 125, 139, 166])  (len = 5)\n",
      "\n",
      "Executing module 46: layer3.1.conv2\n",
      "\tExecuting on machine 0\n",
      "current layer type: <class 'torch.nn.modules.conv.Conv2d'>\n",
      "\t\t-Splitting conv layer 46\n",
      "\t\t Output tensor shape : torch.Size([1, 256, 8, 8])\n",
      "\t\t sending C_out tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
      "        54, 55, 56, 57, 58, 59, 60, 61, 62, 63]) to machine 0\n",
      "\tExecuting on machine 1\n",
      "current layer type: <class 'torch.nn.modules.conv.Conv2d'>\n",
      "\t\t-Splitting conv layer 46\n",
      "\t\t Output tensor shape : torch.Size([1, 256, 8, 8])\n",
      "\t\t sending C_out tensor([ 64,  65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
      "         78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,\n",
      "         92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104, 105,\n",
      "        106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119,\n",
      "        120, 121, 122, 123, 124, 125, 126, 127]) to machine 1\n",
      "\tExecuting on machine 2\n",
      "current layer type: <class 'torch.nn.modules.conv.Conv2d'>\n",
      "\t\t-Splitting conv layer 46\n",
      "\t\t Output tensor shape : torch.Size([1, 256, 8, 8])\n",
      "\t\t sending C_out tensor([128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141,\n",
      "        142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
      "        156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169,\n",
      "        170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183,\n",
      "        184, 185, 186, 187, 188, 189, 190, 191]) to machine 2\n",
      "\tExecuting on machine 3\n",
      "current layer type: <class 'torch.nn.modules.conv.Conv2d'>\n",
      "\t\t-Splitting conv layer 46\n",
      "\t\t Output tensor shape : torch.Size([1, 256, 8, 8])\n",
      "\t\t sending C_out tensor([192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205,\n",
      "        206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219,\n",
      "        220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
      "        234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247,\n",
      "        248, 249, 250, 251, 252, 253, 254, 255]) to machine 3\n",
      "Finished execution of layer 46\n",
      "Max diff:\n",
      "tensor([2.0817e-17], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
      "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
      "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
      "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
      "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
      "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
      "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
      "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
      "        252, 253, 254, 255])  (len = 256)\n",
      "passing Cout = tensor([], dtype=torch.int64)  (len = 0)\n",
      "\n",
      "Executing module 47: layer3.1.bn2\n",
      "\tExecuting on machine 0\n",
      "current layer type: <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "\t\t-Splitting batch norm layer 47\n",
      "\t\t Output tensor shape : torch.Size([1, 256, 8, 8])\n",
      "\t\t sending C_out tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
      "        54, 55, 56, 57, 58, 59, 60, 61, 62, 63]) to machine 0\n",
      "\tExecuting on machine 1\n",
      "current layer type: <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "\t\t-Splitting batch norm layer 47\n",
      "\t\t Output tensor shape : torch.Size([1, 256, 8, 8])\n",
      "\t\t sending C_out tensor([ 64,  65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
      "         78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,\n",
      "         92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104, 105,\n",
      "        106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119,\n",
      "        120, 121, 122, 123, 124, 125, 126, 127]) to machine 1\n",
      "\tExecuting on machine 2\n",
      "current layer type: <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "\t\t-Splitting batch norm layer 47\n",
      "\t\t Output tensor shape : torch.Size([1, 256, 8, 8])\n",
      "\t\t sending C_out tensor([128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141,\n",
      "        142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
      "        156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169,\n",
      "        170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183,\n",
      "        184, 185, 186, 187, 188, 189, 190, 191]) to machine 2\n",
      "\tExecuting on machine 3\n",
      "current layer type: <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "\t\t-Splitting batch norm layer 47\n",
      "\t\t Output tensor shape : torch.Size([1, 256, 8, 8])\n",
      "\t\t sending C_out tensor([192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205,\n",
      "        206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219,\n",
      "        220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
      "        234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247,\n",
      "        248, 249, 250, 251, 252, 253, 254, 255]) to machine 3\n",
      "Finished execution of layer 47\n",
      "Max diff:\n",
      "tensor([2.7756e-17], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  68,  69,  70,\n",
      "         71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,  84,\n",
      "         85,  87,  88,  89,  90,  91,  93,  95,  96,  97,  98,  99, 100, 101,\n",
      "        102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115,\n",
      "        116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
      "        130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143,\n",
      "        144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157,\n",
      "        158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171,\n",
      "        172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185,\n",
      "        186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199,\n",
      "        200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213,\n",
      "        214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227,\n",
      "        228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241,\n",
      "        242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255])  (len = 252)\n",
      "passing Cout = tensor([67, 86, 92, 94])  (len = 4)\n",
      "\n",
      "Executing module 48: layer3.1.add\n",
      "\tExecuting on machine 0\n",
      "\t\t-adding residual\n",
      "\t\t-assuming shortcut had no layers\n",
      "\tExecuting on machine 1\n",
      "\t\t-adding residual\n",
      "\t\t-assuming shortcut had no layers\n",
      "\tExecuting on machine 2\n",
      "\t\t-adding residual\n",
      "\t\t-assuming shortcut had no layers\n",
      "\tExecuting on machine 3\n",
      "\t\t-adding residual\n",
      "\t\t-assuming shortcut had no layers\n",
      "Finished execution of layer 48\n",
      "Max diff:\n",
      "tensor([1.1102e-16], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  93,  95,  96,  97,  98,  99,\n",
      "        100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113,\n",
      "        114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127,\n",
      "        128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141,\n",
      "        142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
      "        156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169,\n",
      "        170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183,\n",
      "        184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197,\n",
      "        198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211,\n",
      "        212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225,\n",
      "        226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239,\n",
      "        240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253,\n",
      "        254, 255])  (len = 254)\n",
      "passing Cout = tensor([92, 94])  (len = 2)\n",
      "\n",
      "Executing module 49: layer3.1.relu_1\n",
      "\tExecuting on machine 0\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 1\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 2\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 3\n",
      "\t\t-Applying ReLU\n",
      "Finished execution of layer 49\n",
      "Max diff:\n",
      "tensor([1.1102e-16], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([  1,   2,   8,  11,  19,  20,  24,  25,  26,  27,  28,  34,  35,  36,\n",
      "         37,  38,  46,  48,  50,  51,  52,  53,  54,  57,  60,  61,  62,  69,\n",
      "         72,  73,  74,  76,  81,  82,  83,  84,  86,  87,  88,  89,  97,  99,\n",
      "        100, 101, 102, 107, 109, 111, 114, 116, 119, 120, 123, 126, 127, 141,\n",
      "        149, 150, 154, 167, 170, 173, 176, 177, 180, 183, 189, 192, 193, 194,\n",
      "        195, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
      "        210, 212, 213, 215, 216, 217, 219, 220, 221, 222, 224, 226, 228, 229,\n",
      "        230, 231, 232, 233, 234, 236, 237, 239, 240, 241, 242, 243, 245, 246,\n",
      "        247, 248, 249, 250, 251, 252, 253, 254, 255])  (len = 121)\n",
      "passing Cout = tensor([ 67,  79, 128, 168, 191])  (len = 5)\n",
      "\n",
      "Executing module 50: layer4.0.conv1\n",
      "\tExecuting on machine 0\n",
      "\t\t-Saving input for later...\n",
      "current layer type: <class 'torch.nn.modules.conv.Conv2d'>\n",
      "\t\t-Splitting conv layer 50\n",
      "\t\t Output tensor shape : torch.Size([1, 512, 4, 4])\n",
      "\t\t sending C_out tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127]) to machine 0\n",
      "\tExecuting on machine 1\n",
      "\t\t-Saving input for later...\n",
      "current layer type: <class 'torch.nn.modules.conv.Conv2d'>\n",
      "\t\t-Splitting conv layer 50\n",
      "\t\t Output tensor shape : torch.Size([1, 512, 4, 4])\n",
      "\t\t sending C_out tensor([128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141,\n",
      "        142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
      "        156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169,\n",
      "        170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183,\n",
      "        184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197,\n",
      "        198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211,\n",
      "        212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225,\n",
      "        226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239,\n",
      "        240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253,\n",
      "        254, 255]) to machine 1\n",
      "\tExecuting on machine 2\n",
      "\t\t-Saving input for later...\n",
      "current layer type: <class 'torch.nn.modules.conv.Conv2d'>\n",
      "\t\t-Splitting conv layer 50\n",
      "\t\t Output tensor shape : torch.Size([1, 512, 4, 4])\n",
      "\t\t sending C_out tensor([256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269,\n",
      "        270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283,\n",
      "        284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297,\n",
      "        298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311,\n",
      "        312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325,\n",
      "        326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339,\n",
      "        340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353,\n",
      "        354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367,\n",
      "        368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381,\n",
      "        382, 383]) to machine 2\n",
      "\tExecuting on machine 3\n",
      "\t\t-Saving input for later...\n",
      "current layer type: <class 'torch.nn.modules.conv.Conv2d'>\n",
      "\t\t-Splitting conv layer 50\n",
      "\t\t Output tensor shape : torch.Size([1, 512, 4, 4])\n",
      "\t\t sending C_out tensor([384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397,\n",
      "        398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411,\n",
      "        412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425,\n",
      "        426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439,\n",
      "        440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453,\n",
      "        454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467,\n",
      "        468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481,\n",
      "        482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495,\n",
      "        496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509,\n",
      "        510, 511]) to machine 3\n",
      "Finished execution of layer 50\n",
      "Max diff:\n",
      "tensor([4.4409e-16], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
      "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
      "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
      "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
      "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
      "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
      "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
      "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
      "        252, 253, 254, 255, 256, 257, 259, 260, 261, 262, 263, 264, 265, 266,\n",
      "        267, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281,\n",
      "        282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295,\n",
      "        296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309,\n",
      "        310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323,\n",
      "        324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337,\n",
      "        338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351,\n",
      "        352, 353, 354, 355, 356, 357, 358, 359, 361, 362, 363, 364, 365, 366,\n",
      "        367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380,\n",
      "        381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394,\n",
      "        395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408,\n",
      "        409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422,\n",
      "        423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436,\n",
      "        437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450,\n",
      "        451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464,\n",
      "        465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478,\n",
      "        479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492,\n",
      "        493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506,\n",
      "        507, 508, 509, 510, 511])  (len = 509)\n",
      "passing Cout = tensor([258, 268, 360])  (len = 3)\n",
      "\n",
      "Executing module 51: layer4.0.bn1\n",
      "\tExecuting on machine 0\n",
      "current layer type: <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "\t\t-Splitting batch norm layer 51\n",
      "\t\t Output tensor shape : torch.Size([1, 512, 4, 4])\n",
      "\t\t sending C_out tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127]) to machine 0\n",
      "\tExecuting on machine 1\n",
      "current layer type: <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "\t\t-Splitting batch norm layer 51\n",
      "\t\t Output tensor shape : torch.Size([1, 512, 4, 4])\n",
      "\t\t sending C_out tensor([128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141,\n",
      "        142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
      "        156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169,\n",
      "        170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183,\n",
      "        184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197,\n",
      "        198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211,\n",
      "        212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225,\n",
      "        226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239,\n",
      "        240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253,\n",
      "        254, 255]) to machine 1\n",
      "\tExecuting on machine 2\n",
      "current layer type: <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "\t\t-Splitting batch norm layer 51\n",
      "\t\t Output tensor shape : torch.Size([1, 512, 4, 4])\n",
      "\t\t sending C_out tensor([256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269,\n",
      "        270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283,\n",
      "        284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297,\n",
      "        298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311,\n",
      "        312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325,\n",
      "        326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339,\n",
      "        340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353,\n",
      "        354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367,\n",
      "        368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381,\n",
      "        382, 383]) to machine 2\n",
      "\tExecuting on machine 3\n",
      "current layer type: <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "\t\t-Splitting batch norm layer 51\n",
      "\t\t Output tensor shape : torch.Size([1, 512, 4, 4])\n",
      "\t\t sending C_out tensor([384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397,\n",
      "        398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411,\n",
      "        412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425,\n",
      "        426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439,\n",
      "        440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453,\n",
      "        454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467,\n",
      "        468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481,\n",
      "        482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495,\n",
      "        496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509,\n",
      "        510, 511]) to machine 3\n",
      "Finished execution of layer 51\n",
      "Max diff:\n",
      "tensor([4.4409e-16], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
      "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
      "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
      "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
      "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
      "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
      "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
      "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
      "        252, 253, 254, 255, 256, 257, 259, 260, 261, 262, 263, 264, 265, 266,\n",
      "        267, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281,\n",
      "        282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295,\n",
      "        296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309,\n",
      "        310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323,\n",
      "        324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337,\n",
      "        338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351,\n",
      "        352, 353, 354, 355, 356, 357, 358, 359, 361, 362, 363, 364, 365, 366,\n",
      "        367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380,\n",
      "        381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394,\n",
      "        395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408,\n",
      "        409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422,\n",
      "        423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436,\n",
      "        437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450,\n",
      "        451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464,\n",
      "        465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478,\n",
      "        479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492,\n",
      "        493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506,\n",
      "        507, 508, 509, 510, 511])  (len = 509)\n",
      "passing Cout = tensor([258, 268, 360])  (len = 3)\n",
      "\n",
      "Executing module 52: layer4.0.relu\n",
      "\tExecuting on machine 0\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 1\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 2\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 3\n",
      "\t\t-Applying ReLU\n",
      "Finished execution of layer 52\n",
      "Max diff:\n",
      "tensor([4.4409e-16], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([  0,   4,  12,  17,  23,  28,  32,  42,  44,  45,  49,  52,  70,  72,\n",
      "         73,  80,  81,  85,  90,  92,  93,  94,  97,  99, 102, 119, 122, 125,\n",
      "        130, 134, 137, 143, 146, 150, 153, 154, 155, 156, 162, 164, 172, 179,\n",
      "        182, 190, 194, 200, 203, 216, 230, 242, 253, 254, 255, 257, 259, 260,\n",
      "        267, 311, 316, 319, 322, 348, 364, 366, 368, 386, 388, 390, 391, 399,\n",
      "        401, 403, 406, 407, 409, 411, 412, 414, 415, 418, 421, 422, 424, 426,\n",
      "        429, 430, 431, 433, 435, 436, 438, 441, 443, 445, 447, 448, 449, 451,\n",
      "        453, 454, 457, 458, 459, 462, 466, 467, 468, 469, 477, 478, 479, 481,\n",
      "        482, 484, 485, 486, 487, 491, 494, 497, 499, 500, 503, 504, 505, 506,\n",
      "        509, 510])  (len = 128)\n",
      "passing Cout = tensor([ 89, 131, 170, 325])  (len = 4)\n",
      "\n",
      "Executing module 53: layer4.0.conv2\n",
      "\tExecuting on machine 0\n",
      "current layer type: <class 'torch.nn.modules.conv.Conv2d'>\n",
      "\t\t-Splitting conv layer 53\n",
      "\t\t Output tensor shape : torch.Size([1, 512, 4, 4])\n",
      "\t\t sending C_out tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127]) to machine 0\n",
      "\tExecuting on machine 1\n",
      "current layer type: <class 'torch.nn.modules.conv.Conv2d'>\n",
      "\t\t-Splitting conv layer 53\n",
      "\t\t Output tensor shape : torch.Size([1, 512, 4, 4])\n",
      "\t\t sending C_out tensor([128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141,\n",
      "        142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
      "        156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169,\n",
      "        170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183,\n",
      "        184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197,\n",
      "        198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211,\n",
      "        212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225,\n",
      "        226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239,\n",
      "        240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253,\n",
      "        254, 255]) to machine 1\n",
      "\tExecuting on machine 2\n",
      "current layer type: <class 'torch.nn.modules.conv.Conv2d'>\n",
      "\t\t-Splitting conv layer 53\n",
      "\t\t Output tensor shape : torch.Size([1, 512, 4, 4])\n",
      "\t\t sending C_out tensor([256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269,\n",
      "        270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283,\n",
      "        284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297,\n",
      "        298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311,\n",
      "        312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325,\n",
      "        326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339,\n",
      "        340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353,\n",
      "        354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367,\n",
      "        368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381,\n",
      "        382, 383]) to machine 2\n",
      "\tExecuting on machine 3\n",
      "current layer type: <class 'torch.nn.modules.conv.Conv2d'>\n",
      "\t\t-Splitting conv layer 53\n",
      "\t\t Output tensor shape : torch.Size([1, 512, 4, 4])\n",
      "\t\t sending C_out tensor([384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397,\n",
      "        398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411,\n",
      "        412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425,\n",
      "        426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439,\n",
      "        440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453,\n",
      "        454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467,\n",
      "        468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481,\n",
      "        482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495,\n",
      "        496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509,\n",
      "        510, 511]) to machine 3\n",
      "Finished execution of layer 53\n",
      "Max diff:\n",
      "tensor([4.4409e-16], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
      "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
      "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
      "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
      "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
      "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
      "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
      "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
      "        252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 263, 264, 266, 267,\n",
      "        268, 269, 270, 271, 272, 273, 274, 275, 278, 279, 280, 281, 282, 283,\n",
      "        284, 285, 286, 287, 288, 289, 290, 291, 292, 294, 295, 296, 297, 298,\n",
      "        299, 300, 301, 302, 303, 305, 306, 307, 309, 310, 311, 312, 313, 314,\n",
      "        316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329,\n",
      "        330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343,\n",
      "        344, 345, 346, 348, 349, 351, 352, 353, 354, 355, 356, 357, 358, 359,\n",
      "        360, 361, 362, 363, 364, 365, 367, 368, 369, 370, 371, 372, 373, 374,\n",
      "        375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388,\n",
      "        389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402,\n",
      "        403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416,\n",
      "        417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430,\n",
      "        431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444,\n",
      "        445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458,\n",
      "        459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472,\n",
      "        473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486,\n",
      "        487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500,\n",
      "        501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511])  (len = 501)\n",
      "passing Cout = tensor([262, 265, 276, 277, 293, 304, 308, 315, 347, 350, 366])  (len = 11)\n",
      "\n",
      "Executing module 54: layer4.0.bn2\n",
      "\tExecuting on machine 0\n",
      "current layer type: <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "\t\t-Splitting batch norm layer 54\n",
      "\t\t Output tensor shape : torch.Size([1, 512, 4, 4])\n",
      "\t\t sending C_out tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127]) to machine 0\n",
      "\tExecuting on machine 1\n",
      "current layer type: <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "\t\t-Splitting batch norm layer 54\n",
      "\t\t Output tensor shape : torch.Size([1, 512, 4, 4])\n",
      "\t\t sending C_out tensor([128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141,\n",
      "        142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
      "        156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169,\n",
      "        170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183,\n",
      "        184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197,\n",
      "        198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211,\n",
      "        212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225,\n",
      "        226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239,\n",
      "        240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253,\n",
      "        254, 255]) to machine 1\n",
      "\tExecuting on machine 2\n",
      "current layer type: <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "\t\t-Splitting batch norm layer 54\n",
      "\t\t Output tensor shape : torch.Size([1, 512, 4, 4])\n",
      "\t\t sending C_out tensor([256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269,\n",
      "        270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283,\n",
      "        284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297,\n",
      "        298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311,\n",
      "        312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325,\n",
      "        326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339,\n",
      "        340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353,\n",
      "        354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367,\n",
      "        368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381,\n",
      "        382, 383]) to machine 2\n",
      "\tExecuting on machine 3\n",
      "current layer type: <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "\t\t-Splitting batch norm layer 54\n",
      "\t\t Output tensor shape : torch.Size([1, 512, 4, 4])\n",
      "\t\t sending C_out tensor([384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397,\n",
      "        398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411,\n",
      "        412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425,\n",
      "        426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439,\n",
      "        440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453,\n",
      "        454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467,\n",
      "        468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481,\n",
      "        482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495,\n",
      "        496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509,\n",
      "        510, 511]) to machine 3\n",
      "Finished execution of layer 54\n",
      "Max diff:\n",
      "tensor([4.4409e-16], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  37,  38,  39,  41,  42,  45,\n",
      "         46,  47,  49,  50,  51,  52,  53,  54,  56,  57,  58,  59,  60,  61,\n",
      "         62,  64,  65,  66,  67,  68,  69,  70,  71,  72,  75,  76,  77,  78,\n",
      "         79,  80,  81,  82,  83,  84,  86,  87,  88,  90,  91,  92,  93,  94,\n",
      "         95,  96,  97,  98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108,\n",
      "        109, 110, 111, 112, 113, 115, 116, 117, 118, 119, 120, 121, 122, 124,\n",
      "        125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138,\n",
      "        139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152,\n",
      "        153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166,\n",
      "        167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180,\n",
      "        181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194,\n",
      "        195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208,\n",
      "        209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222,\n",
      "        223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236,\n",
      "        237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250,\n",
      "        251, 252, 253, 254, 255, 256, 257, 258, 259, 261, 263, 264, 266, 267,\n",
      "        268, 269, 270, 271, 272, 273, 274, 275, 278, 279, 280, 281, 282, 283,\n",
      "        284, 285, 286, 287, 288, 289, 290, 291, 292, 294, 295, 296, 297, 298,\n",
      "        299, 301, 302, 303, 305, 306, 307, 309, 310, 311, 312, 313, 314, 316,\n",
      "        317, 318, 319, 320, 321, 322, 324, 325, 326, 327, 328, 329, 330, 331,\n",
      "        332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345,\n",
      "        346, 348, 349, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361,\n",
      "        362, 363, 364, 365, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376,\n",
      "        377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390,\n",
      "        391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404,\n",
      "        405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418,\n",
      "        419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432,\n",
      "        433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446,\n",
      "        447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460,\n",
      "        461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474,\n",
      "        475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488,\n",
      "        489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502,\n",
      "        503, 504, 505, 506, 507, 508, 509, 510, 511])  (len = 485)\n",
      "passing Cout = tensor([ 36,  40,  43,  44,  48,  55,  63,  73,  74,  85,  89, 114, 123, 260,\n",
      "        262, 265, 276, 277, 293, 300, 304, 308, 315, 323, 347, 350, 366])  (len = 27)\n",
      "\n",
      "Executing module 55: layer4.0.shortcut.0\n",
      "\tExecuting on machine 0\n",
      "\t\t-Saving current input. Swapping for input saved from start of block\n",
      "current layer type: <class 'torch.nn.modules.conv.Conv2d'>\n",
      "\t\t-Splitting conv layer 55\n",
      "\t\t Output tensor shape : torch.Size([1, 512, 4, 4])\n",
      "\t\t sending C_out tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127]) to machine 0\n",
      "\t\t sending C_out tensor([133, 136, 140, 153, 183, 184, 186, 200, 201, 220, 226, 231, 241, 243]) to machine 1\n",
      "\t\t sending C_out tensor([276, 282, 284, 297, 298, 312, 319, 334, 344, 345, 346, 350, 362, 368,\n",
      "        370]) to machine 2\n",
      "\t\t sending C_out tensor([415, 418, 419, 423, 440, 508]) to machine 3\n",
      "\tExecuting on machine 1\n",
      "\t\t-Saving current input. Swapping for input saved from start of block\n",
      "current layer type: <class 'torch.nn.modules.conv.Conv2d'>\n",
      "\t\t-Splitting conv layer 55\n",
      "\t\t Output tensor shape : torch.Size([1, 512, 4, 4])\n",
      "\t\t sending C_out tensor([  1,   6,  12,  21,  24,  45,  48,  49,  55,  69,  70,  78,  79,  80,\n",
      "         82,  87,  88,  99, 101, 104, 110]) to machine 0\n",
      "\t\t sending C_out tensor([128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141,\n",
      "        142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
      "        156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169,\n",
      "        170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183,\n",
      "        184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197,\n",
      "        198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211,\n",
      "        212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225,\n",
      "        226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239,\n",
      "        240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253,\n",
      "        254, 255]) to machine 1\n",
      "\t\t sending C_out tensor([257, 267, 276, 277, 290, 304, 307, 314, 321, 330, 343, 365, 380]) to machine 2\n",
      "\t\t sending C_out tensor([405, 425, 438, 446, 469, 471, 477, 478, 481, 483, 500, 504]) to machine 3\n",
      "\tExecuting on machine 2\n",
      "\t\t-Saving current input. Swapping for input saved from start of block\n",
      "current layer type: <class 'torch.nn.modules.conv.Conv2d'>\n",
      "\t\t-Splitting conv layer 55\n",
      "\t\t Output tensor shape : torch.Size([1, 512, 4, 4])\n",
      "\t\t sending C_out tensor([ 26,  28,  50,  69,  77,  81,  83, 102, 124]) to machine 0\n",
      "\t\t sending C_out tensor([162, 168, 207, 222, 223, 247]) to machine 1\n",
      "\t\t sending C_out tensor([256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269,\n",
      "        270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283,\n",
      "        284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297,\n",
      "        298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311,\n",
      "        312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325,\n",
      "        326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339,\n",
      "        340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353,\n",
      "        354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367,\n",
      "        368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381,\n",
      "        382, 383]) to machine 2\n",
      "\t\t sending C_out tensor([386, 399, 408]) to machine 3\n",
      "\tExecuting on machine 3\n",
      "\t\t-Saving current input. Swapping for input saved from start of block\n",
      "current layer type: <class 'torch.nn.modules.conv.Conv2d'>\n",
      "\t\t-Splitting conv layer 55\n",
      "\t\t Output tensor shape : torch.Size([1, 512, 4, 4])\n",
      "\t\t sending C_out tensor([ 26,  28,  31,  38,  41,  45,  50,  51,  54,  58,  61,  62,  67,  71,\n",
      "         76,  80,  81,  82,  86,  89,  97, 103, 104, 106, 110, 112, 119, 124,\n",
      "        127]) to machine 0\n",
      "\t\t sending C_out tensor([128, 137, 151, 154, 197, 199, 206, 212, 215, 219, 223, 231, 232, 235,\n",
      "        238, 250]) to machine 1\n",
      "\t\t sending C_out tensor([263, 267, 271, 275, 303, 310, 311, 312, 313, 317, 320, 324, 326, 327,\n",
      "        333, 336, 348, 360, 364, 365, 367, 378]) to machine 2\n",
      "\t\t sending C_out tensor([384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397,\n",
      "        398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411,\n",
      "        412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425,\n",
      "        426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439,\n",
      "        440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453,\n",
      "        454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467,\n",
      "        468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481,\n",
      "        482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495,\n",
      "        496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509,\n",
      "        510, 511]) to machine 3\n",
      "Finished execution of layer 55\n",
      "Max diff:\n",
      "tensor([5.5511e-17], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
      "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
      "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
      "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
      "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
      "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
      "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
      "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
      "        252, 253, 254, 255, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266,\n",
      "        267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 280, 281,\n",
      "        282, 283, 284, 285, 286, 287, 288, 289, 290, 292, 293, 294, 295, 296,\n",
      "        297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310,\n",
      "        311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324,\n",
      "        325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338,\n",
      "        339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352,\n",
      "        353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366,\n",
      "        367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380,\n",
      "        381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394,\n",
      "        395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408,\n",
      "        409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422,\n",
      "        423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436,\n",
      "        437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450,\n",
      "        451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464,\n",
      "        465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478,\n",
      "        479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492,\n",
      "        493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506,\n",
      "        507, 508, 509, 510, 511])  (len = 509)\n",
      "passing Cout = tensor([256, 279, 291])  (len = 3)\n",
      "\n",
      "Executing module 56: layer4.0.shortcut.1\n",
      "\tExecuting on machine 0\n",
      "current layer type: <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "\t\t-Splitting batch norm layer 56\n",
      "\t\t Output tensor shape : torch.Size([1, 512, 4, 4])\n",
      "\t\t sending C_out tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127]) to machine 0\n",
      "\tExecuting on machine 1\n",
      "current layer type: <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "\t\t-Splitting batch norm layer 56\n",
      "\t\t Output tensor shape : torch.Size([1, 512, 4, 4])\n",
      "\t\t sending C_out tensor([128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141,\n",
      "        142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
      "        156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169,\n",
      "        170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183,\n",
      "        184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197,\n",
      "        198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211,\n",
      "        212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225,\n",
      "        226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239,\n",
      "        240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253,\n",
      "        254, 255]) to machine 1\n",
      "\tExecuting on machine 2\n",
      "current layer type: <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "\t\t-Splitting batch norm layer 56\n",
      "\t\t Output tensor shape : torch.Size([1, 512, 4, 4])\n",
      "\t\t sending C_out tensor([256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269,\n",
      "        270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283,\n",
      "        284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297,\n",
      "        298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311,\n",
      "        312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325,\n",
      "        326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339,\n",
      "        340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353,\n",
      "        354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367,\n",
      "        368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381,\n",
      "        382, 383]) to machine 2\n",
      "\tExecuting on machine 3\n",
      "current layer type: <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "\t\t-Splitting batch norm layer 56\n",
      "\t\t Output tensor shape : torch.Size([1, 512, 4, 4])\n",
      "\t\t sending C_out tensor([384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397,\n",
      "        398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411,\n",
      "        412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425,\n",
      "        426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439,\n",
      "        440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453,\n",
      "        454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467,\n",
      "        468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481,\n",
      "        482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495,\n",
      "        496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509,\n",
      "        510, 511]) to machine 3\n",
      "Finished execution of layer 56\n",
      "Max diff:\n",
      "tensor([8.3267e-17], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  12,  13,  14,\n",
      "         15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,  28,\n",
      "         29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,  42,\n",
      "         44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,  56,  57,\n",
      "         58,  59,  60,  62,  63,  64,  65,  66,  67,  68,  69,  70,  71,  72,\n",
      "         75,  76,  77,  78,  79,  80,  81,  84,  86,  87,  88,  90,  91,  92,\n",
      "         93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104, 105, 106,\n",
      "        107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 118, 119, 120, 121,\n",
      "        122, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136,\n",
      "        137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150,\n",
      "        151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164,\n",
      "        165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178,\n",
      "        179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192,\n",
      "        193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206,\n",
      "        207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220,\n",
      "        221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234,\n",
      "        235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248,\n",
      "        249, 250, 251, 252, 253, 254, 255, 257, 258, 259, 260, 261, 263, 264,\n",
      "        265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278,\n",
      "        280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 292, 293, 294,\n",
      "        295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308,\n",
      "        309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322,\n",
      "        323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336,\n",
      "        337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350,\n",
      "        351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364,\n",
      "        365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378,\n",
      "        379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392,\n",
      "        393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406,\n",
      "        407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420,\n",
      "        421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434,\n",
      "        435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448,\n",
      "        449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462,\n",
      "        463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476,\n",
      "        477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490,\n",
      "        491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504,\n",
      "        505, 506, 507, 508, 509, 510, 511])  (len = 497)\n",
      "passing Cout = tensor([ 11,  43,  61,  73,  74,  82,  83,  85,  89, 117, 123, 256, 262, 279,\n",
      "        291])  (len = 15)\n",
      "\n",
      "Executing module 57: layer4.0.add\n",
      "\tExecuting on machine 0\n",
      "\t\t-adding residual\n",
      "\tExecuting on machine 1\n",
      "\t\t-adding residual\n",
      "\tExecuting on machine 2\n",
      "\t\t-adding residual\n",
      "\tExecuting on machine 3\n",
      "\t\t-adding residual\n",
      "Finished execution of layer 57\n",
      "Max diff:\n",
      "tensor([4.4409e-16], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  12,  13,  14,\n",
      "         15,  16,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,  28,  29,\n",
      "         30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,  42,  44,\n",
      "         45,  46,  47,  48,  50,  51,  52,  53,  54,  55,  56,  57,  58,  59,\n",
      "         60,  61,  62,  64,  65,  66,  67,  68,  69,  70,  71,  72,  75,  76,\n",
      "         77,  78,  79,  80,  81,  84,  86,  87,  88,  90,  91,  92,  93,  94,\n",
      "         95,  96,  97,  98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108,\n",
      "        109, 110, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 124,\n",
      "        125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138,\n",
      "        139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152,\n",
      "        153, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
      "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 193, 194, 195, 196,\n",
      "        197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210,\n",
      "        211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224,\n",
      "        225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238,\n",
      "        239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252,\n",
      "        253, 254, 255, 256, 257, 258, 259, 260, 261, 263, 264, 266, 267, 268,\n",
      "        269, 270, 271, 272, 273, 274, 275, 276, 278, 279, 280, 281, 282, 283,\n",
      "        284, 285, 286, 287, 288, 289, 290, 291, 292, 294, 295, 296, 297, 298,\n",
      "        299, 301, 302, 303, 305, 306, 307, 309, 310, 311, 312, 313, 314, 315,\n",
      "        316, 317, 318, 319, 320, 321, 322, 324, 325, 326, 327, 328, 329, 330,\n",
      "        331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344,\n",
      "        345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358,\n",
      "        359, 360, 361, 362, 363, 364, 365, 367, 368, 369, 370, 371, 372, 373,\n",
      "        374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387,\n",
      "        388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401,\n",
      "        402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415,\n",
      "        416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429,\n",
      "        430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443,\n",
      "        444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457,\n",
      "        458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471,\n",
      "        472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485,\n",
      "        486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499,\n",
      "        500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511])  (len = 488)\n",
      "passing Cout = tensor([ 11,  17,  43,  49,  63,  73,  74,  82,  83,  85,  89, 111, 123, 154,\n",
      "        192, 262, 265, 277, 293, 300, 304, 308, 323, 366])  (len = 24)\n",
      "\n",
      "Executing module 58: layer4.0.relu_1\n",
      "\tExecuting on machine 0\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 1\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 2\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 3\n",
      "\t\t-Applying ReLU\n",
      "Finished execution of layer 58\n",
      "Max diff:\n",
      "tensor([4.4409e-16], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([  4,   8,  22,  44,  53,  55,  57,  59,  61,  68,  75,  87, 107, 109,\n",
      "        114, 117, 119, 120, 133, 134, 136, 137, 139, 141, 144, 148, 150, 155,\n",
      "        156, 158, 165, 169, 171, 172, 173, 177, 178, 179, 180, 181, 182, 183,\n",
      "        190, 191, 193, 195, 196, 199, 201, 203, 204, 205, 208, 212, 214, 215,\n",
      "        218, 219, 222, 228, 241, 244, 247, 248, 249, 250, 253, 255, 264, 267,\n",
      "        276, 280, 295, 313, 319, 324, 329, 331, 339, 345, 349, 354, 375, 379,\n",
      "        383, 384, 385, 387, 394, 395, 397, 398, 399, 401, 402, 404, 407, 408,\n",
      "        411, 419, 420, 421, 425, 427, 430, 432, 435, 436, 437, 443, 458, 461,\n",
      "        462, 466, 468, 469, 471, 473, 474, 477, 478, 481, 482, 485, 488, 490,\n",
      "        492, 493, 494, 495, 497, 500, 501, 504, 505, 510])  (len = 136)\n",
      "passing Cout = tensor([ 11,  17,  43,  49,  73,  74,  83,  85,  89, 123, 151, 161, 185, 192,\n",
      "        232, 262, 265, 323, 348, 431, 475])  (len = 21)\n",
      "\n",
      "Executing module 59: layer4.1.conv1\n",
      "\tExecuting on machine 0\n",
      "\t\t-Saving input for later...\n",
      "current layer type: <class 'torch.nn.modules.conv.Conv2d'>\n",
      "\t\t-Splitting conv layer 59\n",
      "\t\t Output tensor shape : torch.Size([1, 512, 4, 4])\n",
      "\t\t sending C_out tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127]) to machine 0\n",
      "\tExecuting on machine 1\n",
      "\t\t-Saving input for later...\n",
      "current layer type: <class 'torch.nn.modules.conv.Conv2d'>\n",
      "\t\t-Splitting conv layer 59\n",
      "\t\t Output tensor shape : torch.Size([1, 512, 4, 4])\n",
      "\t\t sending C_out tensor([128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141,\n",
      "        142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
      "        156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169,\n",
      "        170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183,\n",
      "        184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197,\n",
      "        198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211,\n",
      "        212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225,\n",
      "        226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239,\n",
      "        240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253,\n",
      "        254, 255]) to machine 1\n",
      "\tExecuting on machine 2\n",
      "\t\t-Saving input for later...\n",
      "current layer type: <class 'torch.nn.modules.conv.Conv2d'>\n",
      "\t\t-Splitting conv layer 59\n",
      "\t\t Output tensor shape : torch.Size([1, 512, 4, 4])\n",
      "\t\t sending C_out tensor([256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269,\n",
      "        270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283,\n",
      "        284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297,\n",
      "        298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311,\n",
      "        312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325,\n",
      "        326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339,\n",
      "        340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353,\n",
      "        354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367,\n",
      "        368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381,\n",
      "        382, 383]) to machine 2\n",
      "\tExecuting on machine 3\n",
      "\t\t-Saving input for later...\n",
      "current layer type: <class 'torch.nn.modules.conv.Conv2d'>\n",
      "\t\t-Splitting conv layer 59\n",
      "\t\t Output tensor shape : torch.Size([1, 512, 4, 4])\n",
      "\t\t sending C_out tensor([384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397,\n",
      "        398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411,\n",
      "        412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425,\n",
      "        426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439,\n",
      "        440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453,\n",
      "        454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467,\n",
      "        468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481,\n",
      "        482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495,\n",
      "        496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509,\n",
      "        510, 511]) to machine 3\n",
      "Finished execution of layer 59\n",
      "Max diff:\n",
      "tensor([8.8818e-16], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([  0,   1,   2,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,  14,\n",
      "         15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  28,  29,\n",
      "         32,  33,  34,  36,  37,  38,  39,  40,  41,  42,  43,  44,  45,  46,\n",
      "         48,  49,  50,  51,  52,  53,  54,  55,  56,  57,  59,  60,  62,  63,\n",
      "         64,  65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
      "         79,  81,  83,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,\n",
      "         97,  98,  99, 100, 103, 104, 105, 106, 107, 108, 109, 111, 112, 113,\n",
      "        115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128,\n",
      "        129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n",
      "        143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156,\n",
      "        157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170,\n",
      "        171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184,\n",
      "        185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198,\n",
      "        199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212,\n",
      "        213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226,\n",
      "        227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240,\n",
      "        241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254,\n",
      "        255, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 269, 270,\n",
      "        271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284,\n",
      "        285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298,\n",
      "        299, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313,\n",
      "        314, 316, 317, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329,\n",
      "        330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 342, 343, 344,\n",
      "        345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 356, 357, 358, 359,\n",
      "        361, 362, 363, 364, 366, 367, 368, 369, 371, 372, 373, 374, 375, 376,\n",
      "        377, 378, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391,\n",
      "        392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405,\n",
      "        406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419,\n",
      "        420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433,\n",
      "        434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447,\n",
      "        448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461,\n",
      "        462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475,\n",
      "        476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489,\n",
      "        490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503,\n",
      "        504, 505, 506, 507, 508, 509, 510, 511])  (len = 484)\n",
      "passing Cout = tensor([  3,  27,  30,  31,  35,  47,  58,  61,  78,  80,  82,  84,  96, 101,\n",
      "        102, 110, 114, 256, 268, 300, 315, 318, 341, 355, 360, 365, 370, 379])  (len = 28)\n",
      "\n",
      "Executing module 60: layer4.1.bn1\n",
      "\tExecuting on machine 0\n",
      "current layer type: <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "\t\t-Splitting batch norm layer 60\n",
      "\t\t Output tensor shape : torch.Size([1, 512, 4, 4])\n",
      "\t\t sending C_out tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127]) to machine 0\n",
      "\tExecuting on machine 1\n",
      "current layer type: <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "\t\t-Splitting batch norm layer 60\n",
      "\t\t Output tensor shape : torch.Size([1, 512, 4, 4])\n",
      "\t\t sending C_out tensor([128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141,\n",
      "        142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
      "        156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169,\n",
      "        170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183,\n",
      "        184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197,\n",
      "        198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211,\n",
      "        212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225,\n",
      "        226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239,\n",
      "        240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253,\n",
      "        254, 255]) to machine 1\n",
      "\tExecuting on machine 2\n",
      "current layer type: <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "\t\t-Splitting batch norm layer 60\n",
      "\t\t Output tensor shape : torch.Size([1, 512, 4, 4])\n",
      "\t\t sending C_out tensor([256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269,\n",
      "        270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283,\n",
      "        284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297,\n",
      "        298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311,\n",
      "        312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325,\n",
      "        326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339,\n",
      "        340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353,\n",
      "        354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367,\n",
      "        368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381,\n",
      "        382, 383]) to machine 2\n",
      "\tExecuting on machine 3\n",
      "current layer type: <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "\t\t-Splitting batch norm layer 60\n",
      "\t\t Output tensor shape : torch.Size([1, 512, 4, 4])\n",
      "\t\t sending C_out tensor([384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397,\n",
      "        398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411,\n",
      "        412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425,\n",
      "        426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439,\n",
      "        440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453,\n",
      "        454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467,\n",
      "        468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481,\n",
      "        482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495,\n",
      "        496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509,\n",
      "        510, 511]) to machine 3\n",
      "Finished execution of layer 60\n",
      "Max diff:\n",
      "tensor([8.8818e-16], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([  0,   1,   2,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,  14,\n",
      "         15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  28,  29,\n",
      "         32,  33,  34,  36,  37,  38,  39,  40,  41,  42,  43,  44,  45,  46,\n",
      "         48,  49,  50,  51,  52,  53,  54,  55,  56,  57,  59,  60,  62,  63,\n",
      "         64,  65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
      "         79,  81,  83,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,\n",
      "         97,  98,  99, 100, 103, 104, 105, 106, 107, 108, 109, 111, 112, 113,\n",
      "        115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128,\n",
      "        129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n",
      "        143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156,\n",
      "        157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170,\n",
      "        171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184,\n",
      "        185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198,\n",
      "        199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212,\n",
      "        213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226,\n",
      "        227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240,\n",
      "        241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254,\n",
      "        255, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 269, 270,\n",
      "        271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284,\n",
      "        285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298,\n",
      "        299, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313,\n",
      "        314, 316, 317, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329,\n",
      "        330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 342, 343, 344,\n",
      "        345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 356, 357, 358, 359,\n",
      "        361, 362, 363, 364, 366, 367, 368, 369, 371, 372, 373, 374, 375, 376,\n",
      "        377, 378, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391,\n",
      "        392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405,\n",
      "        406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419,\n",
      "        420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433,\n",
      "        434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447,\n",
      "        448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461,\n",
      "        462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475,\n",
      "        476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489,\n",
      "        490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503,\n",
      "        504, 505, 506, 507, 508, 509, 510, 511])  (len = 484)\n",
      "passing Cout = tensor([  3,  27,  30,  31,  35,  47,  58,  61,  78,  80,  82,  84,  96, 101,\n",
      "        102, 110, 114, 256, 268, 300, 315, 318, 341, 355, 360, 365, 370, 379])  (len = 28)\n",
      "\n",
      "Executing module 61: layer4.1.relu\n",
      "\tExecuting on machine 0\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 1\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 2\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 3\n",
      "\t\t-Applying ReLU\n",
      "Finished execution of layer 61\n",
      "Max diff:\n",
      "tensor([8.8818e-16], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([ 15,  28,  34,  57,  63,  94, 129, 146, 150, 166, 175, 188, 195, 198,\n",
      "        226, 241, 249, 265, 266, 271, 274, 275, 280, 289, 305, 307, 321, 325,\n",
      "        337, 342, 369, 372, 387, 389, 390, 391, 393, 395, 396, 398, 401, 402,\n",
      "        403, 406, 409, 413, 414, 419, 423, 430, 431, 432, 439, 443, 445, 447,\n",
      "        450, 454, 455, 456, 458, 459, 460, 462, 467, 468, 471, 472, 478, 483,\n",
      "        485, 488, 491, 492, 497, 498, 503, 504, 507, 509, 511])  (len = 81)\n",
      "passing Cout = tensor([ 84, 101, 110, 113, 114, 269, 300, 316, 376])  (len = 9)\n",
      "\n",
      "Executing module 62: layer4.1.conv2\n",
      "\tExecuting on machine 0\n",
      "current layer type: <class 'torch.nn.modules.conv.Conv2d'>\n",
      "\t\t-Splitting conv layer 62\n",
      "\t\t Output tensor shape : torch.Size([1, 512, 4, 4])\n",
      "\t\t sending C_out tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127]) to machine 0\n",
      "\tExecuting on machine 1\n",
      "current layer type: <class 'torch.nn.modules.conv.Conv2d'>\n",
      "\t\t-Splitting conv layer 62\n",
      "\t\t Output tensor shape : torch.Size([1, 512, 4, 4])\n",
      "\t\t sending C_out tensor([128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141,\n",
      "        142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
      "        156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169,\n",
      "        170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183,\n",
      "        184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197,\n",
      "        198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211,\n",
      "        212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225,\n",
      "        226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239,\n",
      "        240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253,\n",
      "        254, 255]) to machine 1\n",
      "\tExecuting on machine 2\n",
      "current layer type: <class 'torch.nn.modules.conv.Conv2d'>\n",
      "\t\t-Splitting conv layer 62\n",
      "\t\t Output tensor shape : torch.Size([1, 512, 4, 4])\n",
      "\t\t sending C_out tensor([256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269,\n",
      "        270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283,\n",
      "        284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297,\n",
      "        298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311,\n",
      "        312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325,\n",
      "        326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339,\n",
      "        340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353,\n",
      "        354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367,\n",
      "        368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381,\n",
      "        382, 383]) to machine 2\n",
      "\tExecuting on machine 3\n",
      "current layer type: <class 'torch.nn.modules.conv.Conv2d'>\n",
      "\t\t-Splitting conv layer 62\n",
      "\t\t Output tensor shape : torch.Size([1, 512, 4, 4])\n",
      "\t\t sending C_out tensor([384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397,\n",
      "        398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411,\n",
      "        412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425,\n",
      "        426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439,\n",
      "        440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453,\n",
      "        454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467,\n",
      "        468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481,\n",
      "        482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495,\n",
      "        496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509,\n",
      "        510, 511]) to machine 3\n",
      "Finished execution of layer 62\n",
      "Max diff:\n",
      "tensor([3.5527e-15], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([  0,   1,   2,   3,   5,   6,   8,   9,  10,  13,  16,  17,  18,  19,\n",
      "         22,  23,  24,  26,  27,  29,  30,  31,  36,  39,  40,  41,  43,  45,\n",
      "         46,  51,  52,  58,  60,  61,  62,  63,  65,  68,  73,  74,  76,  78,\n",
      "         79,  80,  81,  82,  85,  86,  87,  88,  89,  90,  91,  93,  94,  97,\n",
      "         98, 100, 101, 102, 103, 104, 105, 107, 111, 112, 113, 114, 115, 116,\n",
      "        117, 119, 120, 124, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137,\n",
      "        138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151,\n",
      "        152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165,\n",
      "        166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179,\n",
      "        180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193,\n",
      "        194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207,\n",
      "        208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221,\n",
      "        222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235,\n",
      "        236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249,\n",
      "        250, 251, 252, 253, 254, 255, 256, 258, 259, 260, 261, 262, 263, 264,\n",
      "        265, 266, 267, 268, 270, 271, 272, 273, 274, 275, 276, 277, 280, 281,\n",
      "        282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 295, 296,\n",
      "        297, 299, 300, 301, 302, 303, 304, 305, 307, 308, 309, 310, 311, 312,\n",
      "        313, 314, 315, 316, 317, 318, 319, 320, 322, 323, 324, 325, 326, 327,\n",
      "        328, 329, 330, 331, 332, 333, 334, 335, 336, 339, 340, 341, 342, 344,\n",
      "        345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358,\n",
      "        359, 360, 361, 362, 364, 365, 366, 367, 368, 369, 370, 371, 373, 374,\n",
      "        375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388,\n",
      "        389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402,\n",
      "        403, 404, 405, 406, 407, 408, 409, 410, 412, 413, 414, 415, 416, 417,\n",
      "        418, 419, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432,\n",
      "        433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446,\n",
      "        447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460,\n",
      "        461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474,\n",
      "        475, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489,\n",
      "        490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503,\n",
      "        504, 505, 506, 507, 508, 509, 510, 511])  (len = 442)\n",
      "passing Cout = tensor([  4,   7,  11,  12,  14,  15,  20,  21,  25,  28,  32,  33,  34,  35,\n",
      "         37,  38,  42,  44,  47,  48,  49,  50,  53,  54,  55,  56,  57,  59,\n",
      "         64,  66,  67,  69,  70,  71,  72,  75,  77,  83,  84,  92,  95,  96,\n",
      "         99, 106, 108, 109, 110, 118, 121, 122, 123, 125, 126, 127, 257, 269,\n",
      "        278, 279, 294, 298, 306, 321, 337, 338, 343, 363, 372, 411, 420, 476])  (len = 70)\n",
      "\n",
      "Executing module 63: layer4.1.bn2\n",
      "\tExecuting on machine 0\n",
      "current layer type: <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "\t\t-Splitting batch norm layer 63\n",
      "\t\t Output tensor shape : torch.Size([1, 512, 4, 4])\n",
      "\t\t sending C_out tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127]) to machine 0\n",
      "\tExecuting on machine 1\n",
      "current layer type: <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "\t\t-Splitting batch norm layer 63\n",
      "\t\t Output tensor shape : torch.Size([1, 512, 4, 4])\n",
      "\t\t sending C_out tensor([128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141,\n",
      "        142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
      "        156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169,\n",
      "        170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183,\n",
      "        184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197,\n",
      "        198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211,\n",
      "        212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225,\n",
      "        226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239,\n",
      "        240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253,\n",
      "        254, 255]) to machine 1\n",
      "\tExecuting on machine 2\n",
      "current layer type: <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "\t\t-Splitting batch norm layer 63\n",
      "\t\t Output tensor shape : torch.Size([1, 512, 4, 4])\n",
      "\t\t sending C_out tensor([256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269,\n",
      "        270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283,\n",
      "        284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297,\n",
      "        298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311,\n",
      "        312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325,\n",
      "        326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339,\n",
      "        340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353,\n",
      "        354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367,\n",
      "        368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381,\n",
      "        382, 383]) to machine 2\n",
      "\tExecuting on machine 3\n",
      "current layer type: <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "\t\t-Splitting batch norm layer 63\n",
      "\t\t Output tensor shape : torch.Size([1, 512, 4, 4])\n",
      "\t\t sending C_out tensor([384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397,\n",
      "        398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411,\n",
      "        412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425,\n",
      "        426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439,\n",
      "        440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453,\n",
      "        454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467,\n",
      "        468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481,\n",
      "        482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495,\n",
      "        496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509,\n",
      "        510, 511]) to machine 3\n",
      "Finished execution of layer 63\n",
      "Max diff:\n",
      "tensor([3.5527e-15], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([  0,   1,   2,   5,   6,   9,  10,  13,  16,  17,  18,  22,  23,  24,\n",
      "         26,  29,  30,  31,  36,  39,  40,  41,  46,  58,  60,  62,  63,  65,\n",
      "         68,  73,  76,  78,  82,  86,  88,  90,  91,  93,  94,  97, 100, 101,\n",
      "        102, 103, 104, 105, 107, 111, 112, 114, 115, 116, 120, 124, 128, 129,\n",
      "        130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143,\n",
      "        144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157,\n",
      "        158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171,\n",
      "        172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185,\n",
      "        186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199,\n",
      "        200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213,\n",
      "        214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227,\n",
      "        228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241,\n",
      "        242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255,\n",
      "        256, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 270, 271,\n",
      "        272, 273, 274, 275, 276, 277, 280, 281, 282, 283, 284, 285, 286, 287,\n",
      "        288, 289, 290, 291, 292, 293, 295, 296, 297, 299, 300, 301, 302, 303,\n",
      "        304, 305, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318,\n",
      "        319, 320, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333,\n",
      "        334, 335, 336, 339, 340, 341, 342, 344, 345, 346, 347, 348, 349, 350,\n",
      "        351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 364, 365,\n",
      "        366, 367, 368, 369, 370, 371, 373, 374, 375, 376, 377, 378, 379, 380,\n",
      "        381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394,\n",
      "        395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408,\n",
      "        409, 410, 412, 413, 414, 415, 416, 417, 418, 419, 421, 422, 423, 424,\n",
      "        425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438,\n",
      "        440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453,\n",
      "        454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467,\n",
      "        468, 469, 470, 471, 472, 473, 474, 475, 477, 478, 479, 480, 481, 482,\n",
      "        483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496,\n",
      "        497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510,\n",
      "        511])  (len = 421)\n",
      "passing Cout = tensor([  3,   4,   7,   8,  11,  12,  14,  15,  19,  20,  21,  25,  27,  28,\n",
      "         32,  33,  34,  35,  37,  38,  42,  43,  44,  45,  47,  48,  49,  50,\n",
      "         51,  52,  53,  54,  55,  56,  57,  59,  61,  64,  66,  67,  69,  70,\n",
      "         71,  72,  74,  75,  77,  79,  80,  81,  83,  84,  85,  87,  89,  92,\n",
      "         95,  96,  98,  99, 106, 108, 109, 110, 113, 117, 118, 119, 121, 122,\n",
      "        123, 125, 126, 127, 257, 269, 278, 279, 294, 298, 306, 321, 337, 338,\n",
      "        343, 363, 372, 411, 420, 439, 476])  (len = 91)\n",
      "\n",
      "Executing module 64: layer4.1.add\n",
      "\tExecuting on machine 0\n",
      "\t\t-adding residual\n",
      "\t\t-assuming shortcut had no layers\n",
      "\tExecuting on machine 1\n",
      "\t\t-adding residual\n",
      "\t\t-assuming shortcut had no layers\n",
      "\tExecuting on machine 2\n",
      "\t\t-adding residual\n",
      "\t\t-assuming shortcut had no layers\n",
      "\tExecuting on machine 3\n",
      "\t\t-adding residual\n",
      "\t\t-assuming shortcut had no layers\n",
      "Finished execution of layer 64\n",
      "Max diff:\n",
      "tensor([3.5527e-15], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([  0,   1,   2,   4,   5,   6,   9,  10,  13,  16,  17,  18,  22,  23,\n",
      "         24,  26,  29,  30,  31,  36,  39,  40,  41,  46,  53,  58,  59,  60,\n",
      "         61,  62,  63,  65,  68,  75,  76,  78,  82,  86,  87,  88,  90,  91,\n",
      "         93,  94,  97, 100, 101, 102, 103, 104, 105, 107, 111, 112, 114, 115,\n",
      "        116, 117, 119, 120, 124, 128, 129, 130, 131, 132, 133, 134, 135, 136,\n",
      "        137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150,\n",
      "        151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164,\n",
      "        165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178,\n",
      "        179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192,\n",
      "        193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206,\n",
      "        207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220,\n",
      "        221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234,\n",
      "        235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248,\n",
      "        249, 250, 251, 252, 253, 254, 255, 256, 258, 259, 260, 261, 262, 263,\n",
      "        264, 265, 266, 267, 268, 270, 271, 272, 273, 274, 275, 276, 277, 280,\n",
      "        281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 295,\n",
      "        296, 297, 299, 300, 301, 302, 303, 304, 305, 307, 308, 309, 310, 311,\n",
      "        312, 313, 314, 315, 316, 317, 318, 319, 320, 322, 323, 324, 325, 326,\n",
      "        327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 339, 340, 341, 342,\n",
      "        344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357,\n",
      "        358, 359, 360, 361, 362, 364, 365, 366, 367, 368, 369, 370, 371, 373,\n",
      "        374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387,\n",
      "        388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 399, 400, 401, 402,\n",
      "        403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416,\n",
      "        417, 418, 419, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431,\n",
      "        432, 433, 434, 435, 436, 437, 438, 440, 441, 442, 443, 444, 445, 446,\n",
      "        447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460,\n",
      "        461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474,\n",
      "        475, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489,\n",
      "        490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503,\n",
      "        504, 505, 506, 507, 508, 509, 510, 511])  (len = 428)\n",
      "passing Cout = tensor([  3,   7,   8,  11,  12,  14,  15,  19,  20,  21,  25,  27,  28,  32,\n",
      "         33,  34,  35,  37,  38,  42,  43,  44,  45,  47,  48,  49,  50,  51,\n",
      "         52,  54,  55,  56,  57,  64,  66,  67,  69,  70,  71,  72,  73,  74,\n",
      "         77,  79,  80,  81,  83,  84,  85,  89,  92,  95,  96,  98,  99, 106,\n",
      "        108, 109, 110, 113, 118, 121, 122, 123, 125, 126, 127, 257, 269, 278,\n",
      "        279, 294, 298, 306, 321, 337, 338, 343, 363, 372, 398, 420, 439, 476])  (len = 84)\n",
      "\n",
      "Executing module 65: layer4.1.relu_1\n",
      "\tExecuting on machine 0\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 1\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 2\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 3\n",
      "\t\t-Applying ReLU\n",
      "Finished execution of layer 65\n",
      "Max diff:\n",
      "tensor([3.5527e-15], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([  2,   4,  17,  22,  53,  59,  61,  68,  75,  87, 101, 107, 111, 114,\n",
      "        117, 119, 120, 130, 131, 132, 133, 134, 136, 137, 139, 141, 144, 148,\n",
      "        150, 155, 158, 165, 169, 171, 172, 175, 177, 178, 179, 180, 181, 182,\n",
      "        183, 192, 193, 194, 195, 196, 199, 201, 203, 205, 207, 208, 209, 212,\n",
      "        214, 215, 218, 219, 222, 227, 228, 239, 244, 247, 249, 250, 253, 255,\n",
      "        262, 264, 267, 268, 270, 275, 276, 277, 280, 285, 289, 290, 291, 297,\n",
      "        299, 300, 302, 308, 309, 313, 320, 322, 323, 324, 326, 329, 330, 332,\n",
      "        339, 340, 341, 342, 347, 349, 350, 351, 354, 358, 361, 367, 374, 375,\n",
      "        379, 382, 383, 384, 385, 387, 394, 395, 397, 399, 401, 402, 404, 407,\n",
      "        408, 409, 411, 419, 421, 422, 424, 425, 427, 428, 430, 432, 435, 436,\n",
      "        437, 438, 443, 445, 446, 447, 448, 451, 453, 454, 458, 461, 462, 464,\n",
      "        466, 468, 469, 474, 475, 477, 478, 481, 482, 485, 487, 490, 492, 494,\n",
      "        495, 497, 499, 500, 503, 504, 505, 508])  (len = 176)\n",
      "passing Cout = tensor([  8,  11,  43,  44,  49,  55,  57,  66,  67,  73,  74,  83,  85,  89,\n",
      "         95,  99, 109, 123, 124, 127, 161, 248, 257, 292, 294, 298, 312, 321,\n",
      "        337, 343, 356, 398, 400, 420, 439])  (len = 35)\n",
      "\n",
      "Executing module 66: avg_pool2d\n",
      "\tExecuting on machine 0\n",
      "\t\t-average pooling\n",
      "\tExecuting on machine 1\n",
      "\t\t-average pooling\n",
      "\tExecuting on machine 2\n",
      "\t\t-average pooling\n",
      "\tExecuting on machine 3\n",
      "\t\t-average pooling\n",
      "Finished execution of layer 66\n",
      "Max diff:\n",
      "tensor([8.8818e-16], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([107, 119, 130, 131, 132, 134, 148, 155, 165, 172, 175, 177, 179, 182,\n",
      "        183, 194, 201, 203, 208, 209, 219, 222, 227, 228, 239, 249, 250, 264,\n",
      "        277, 290, 309, 322, 342, 347, 350, 351, 374, 385, 401, 409, 422, 430,\n",
      "        432, 445, 447, 451, 454, 464, 475, 487, 490, 499, 503])  (len = 53)\n",
      "passing Cout = tensor([  2,   4,   8,  11,  17,  22,  43,  44,  49,  53,  55,  57,  59,  61,\n",
      "         66,  67,  68,  73,  74,  75,  83,  85,  87,  89,  95,  99, 101, 109,\n",
      "        111, 114, 117, 120, 123, 124, 127, 133, 136, 137, 139, 141, 144, 150,\n",
      "        158, 161, 169, 171, 178, 180, 181, 192, 193, 195, 196, 199, 205, 207,\n",
      "        212, 214, 215, 218, 244, 247, 248, 253, 255, 257, 262, 267, 268, 270,\n",
      "        275, 276, 280, 285, 289, 291, 292, 294, 297, 298, 299, 300, 302, 308,\n",
      "        312, 313, 320, 321, 323, 324, 326, 329, 330, 332, 337, 339, 340, 341,\n",
      "        343, 349, 354, 356, 358, 361, 367, 375, 379, 382, 383, 384, 387, 394,\n",
      "        395, 397, 398, 399, 400, 402, 404, 407, 408, 411, 419, 420, 421, 424,\n",
      "        425, 427, 428, 435, 436, 437, 438, 439, 443, 446, 448, 453, 458, 461,\n",
      "        462, 466, 468, 469, 474, 477, 478, 481, 482, 485, 492, 494, 495, 497,\n",
      "        500, 504, 505, 508])  (len = 158)\n",
      "\n",
      "Executing module 67: size\n",
      "\tExecuting on machine 0\n",
      "\t\t-skipping\n",
      "\tExecuting on machine 1\n",
      "\t\t-skipping\n",
      "\tExecuting on machine 2\n",
      "\t\t-skipping\n",
      "\tExecuting on machine 3\n",
      "\t\t-skipping\n",
      "Finished execution of layer 67\n",
      "Horizontal output is <class 'int'>. Skipping comparison\n",
      "\n",
      "Executing module 68: view\n",
      "\tExecuting on machine 0\n",
      "\t\t-reshaping (view)\n",
      "\tExecuting on machine 1\n",
      "\t\t-reshaping (view)\n",
      "\tExecuting on machine 2\n",
      "\t\t-reshaping (view)\n",
      "\tExecuting on machine 3\n",
      "\t\t-reshaping (view)\n",
      "Finished execution of layer 68\n",
      "Max diff:\n",
      "tensor([8.8818e-16], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([107, 119, 130, 131, 132, 134, 148, 155, 165, 172, 175, 177, 179, 182,\n",
      "        183, 194, 201, 203, 208, 209, 219, 222, 227, 228, 239, 249, 250, 264,\n",
      "        277, 290, 309, 322, 342, 347, 350, 351, 374, 385, 401, 409, 422, 430,\n",
      "        432, 445, 447, 451, 454, 464, 475, 487, 490, 499, 503])  (len = 53)\n",
      "passing Cout = tensor([  2,   4,   8,  11,  17,  22,  43,  44,  49,  53,  55,  57,  59,  61,\n",
      "         66,  67,  68,  73,  74,  75,  83,  85,  87,  89,  95,  99, 101, 109,\n",
      "        111, 114, 117, 120, 123, 124, 127, 133, 136, 137, 139, 141, 144, 150,\n",
      "        158, 161, 169, 171, 178, 180, 181, 192, 193, 195, 196, 199, 205, 207,\n",
      "        212, 214, 215, 218, 244, 247, 248, 253, 255, 257, 262, 267, 268, 270,\n",
      "        275, 276, 280, 285, 289, 291, 292, 294, 297, 298, 299, 300, 302, 308,\n",
      "        312, 313, 320, 321, 323, 324, 326, 329, 330, 332, 337, 339, 340, 341,\n",
      "        343, 349, 354, 356, 358, 361, 367, 375, 379, 382, 383, 384, 387, 394,\n",
      "        395, 397, 398, 399, 400, 402, 404, 407, 408, 411, 419, 420, 421, 424,\n",
      "        425, 427, 428, 435, 436, 437, 438, 439, 443, 446, 448, 453, 458, 461,\n",
      "        462, 466, 468, 469, 474, 477, 478, 481, 482, 485, 492, 494, 495, 497,\n",
      "        500, 504, 505, 508])  (len = 158)\n",
      "\n",
      "Executing module 69: linear1\n",
      "\tExecuting on machine 0\n",
      "current layer type: <class 'torch.nn.modules.linear.Linear'>\n",
      "\t\t-Splitting linear layer 69\n",
      "\t\t Output tensor shape : torch.Size([1, 256])\n",
      "\t\t sending C_out tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
      "        54, 55, 56, 57, 58, 59, 60, 61, 62, 63]) to machine 0\n",
      "\t\t sending C_out tensor([ 65,  66,  68,  70,  71,  72,  74,  77,  78,  79,  80,  81,  83,  85,\n",
      "         86,  87,  88,  89,  91,  92,  93,  94,  95,  96,  97,  99, 101, 104,\n",
      "        105, 106, 108, 109, 111, 112, 113, 114, 115, 116, 117, 119, 122, 123,\n",
      "        125, 126, 127]) to machine 1\n",
      "\t\t sending C_out tensor([128, 130, 131, 132, 134, 135, 136, 137, 138, 140, 141, 142, 144, 145,\n",
      "        147, 149, 150, 151, 152, 155, 156, 157, 158, 159, 160, 161, 162, 164,\n",
      "        165, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 180, 181,\n",
      "        182, 183, 184, 185, 187, 188, 189, 190]) to machine 2\n",
      "\t\t sending C_out tensor([195, 196, 197, 198, 200, 201, 202, 203, 204, 207, 208, 209, 210, 214,\n",
      "        215, 216, 217, 218, 219, 220, 222, 223, 226, 228, 229, 230, 232, 233,\n",
      "        234, 235, 237, 238, 239, 244, 245, 247, 248, 249, 250, 251, 252, 253,\n",
      "        254]) to machine 3\n",
      "\tExecuting on machine 1\n",
      "current layer type: <class 'torch.nn.modules.linear.Linear'>\n",
      "\t\t-Splitting linear layer 69\n",
      "\t\t Output tensor shape : torch.Size([1, 256])\n",
      "\t\t sending C_out tensor([ 0,  1,  2,  3,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19,\n",
      "        20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 38, 39,\n",
      "        41, 42, 43, 44, 45, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 60,\n",
      "        61, 62, 63]) to machine 0\n",
      "\t\t sending C_out tensor([ 64,  65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
      "         78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,\n",
      "         92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104, 105,\n",
      "        106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119,\n",
      "        120, 121, 122, 123, 124, 125, 126, 127]) to machine 1\n",
      "\t\t sending C_out tensor([128, 129, 130, 131, 132, 133, 135, 136, 137, 138, 139, 141, 142, 143,\n",
      "        144, 145, 146, 147, 149, 150, 151, 154, 155, 156, 157, 158, 159, 160,\n",
      "        161, 162, 163, 165, 166, 167, 168, 169, 170, 171, 172, 173, 175, 177,\n",
      "        178, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191]) to machine 2\n",
      "\t\t sending C_out tensor([194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 205, 207, 208, 210,\n",
      "        211, 212, 213, 215, 216, 218, 219, 221, 222, 223, 224, 225, 226, 227,\n",
      "        228, 229, 231, 233, 235, 236, 237, 240, 241, 242, 243, 244, 245, 246,\n",
      "        248, 249, 250, 252, 253, 254, 255]) to machine 3\n",
      "\tExecuting on machine 2\n",
      "current layer type: <class 'torch.nn.modules.linear.Linear'>\n",
      "\t\t-Splitting linear layer 69\n",
      "\t\t Output tensor shape : torch.Size([1, 256])\n",
      "\t\t sending C_out tensor([ 0,  2,  4,  5,  6,  8,  9, 10, 12, 13, 15, 16, 17, 18, 19, 20, 21, 22,\n",
      "        23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n",
      "        41, 42, 44, 45, 46, 47, 48, 49, 50, 52, 53, 54, 55, 56, 58, 59, 60, 61,\n",
      "        62, 63]) to machine 0\n",
      "\t\t sending C_out tensor([ 64,  65,  66,  67,  69,  70,  71,  73,  74,  75,  76,  77,  79,  80,\n",
      "         81,  82,  83,  84,  87,  88,  89,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 102, 104, 105, 106, 107, 108, 109, 110, 111, 113, 114,\n",
      "        116, 117, 118, 120, 121, 124, 125, 126, 127]) to machine 1\n",
      "\t\t sending C_out tensor([128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141,\n",
      "        142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
      "        156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169,\n",
      "        170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183,\n",
      "        184, 185, 186, 187, 188, 189, 190, 191]) to machine 2\n",
      "\t\t sending C_out tensor([192, 193, 194, 196, 197, 198, 199, 201, 202, 203, 204, 205, 206, 208,\n",
      "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 224, 225,\n",
      "        226, 228, 229, 230, 231, 233, 235, 236, 237, 240, 242, 243, 245, 247,\n",
      "        248, 249, 251, 252, 253, 254, 255]) to machine 3\n",
      "\tExecuting on machine 3\n",
      "current layer type: <class 'torch.nn.modules.linear.Linear'>\n",
      "\t\t-Splitting linear layer 69\n",
      "\t\t Output tensor shape : torch.Size([1, 256])\n",
      "\t\t sending C_out tensor([ 1,  2,  3,  4,  5,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 19, 20,\n",
      "        21, 22, 23, 24, 25, 27, 28, 29, 32, 33, 34, 36, 37, 38, 39, 40, 41, 42,\n",
      "        43, 45, 46, 47, 48, 49, 50, 51, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62,\n",
      "        63]) to machine 0\n",
      "\t\t sending C_out tensor([ 64,  65,  66,  67,  69,  70,  71,  72,  73,  74,  75,  78,  79,  80,\n",
      "         81,  82,  83,  84,  85,  86,  87,  88,  89,  91,  92,  93,  94,  96,\n",
      "         97,  98,  99, 100, 101, 102, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 114, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 127]) to machine 1\n",
      "\t\t sending C_out tensor([128, 129, 130, 131, 132, 133, 134, 135, 136, 138, 139, 140, 141, 142,\n",
      "        143, 144, 145, 146, 147, 149, 151, 152, 153, 154, 155, 156, 157, 158,\n",
      "        159, 161, 162, 163, 164, 165, 167, 168, 169, 170, 171, 172, 173, 174,\n",
      "        175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 190, 191]) to machine 2\n",
      "\t\t sending C_out tensor([192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205,\n",
      "        206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219,\n",
      "        220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
      "        234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247,\n",
      "        248, 249, 250, 251, 252, 253, 254, 255]) to machine 3\n",
      "Finished execution of layer 69\n",
      "Max diff:\n",
      "tensor([1.7764e-15], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([  3,   6,   9,  10,  11,  12,  14,  15,  17,  18,  20,  21,  23,  24,\n",
      "         25,  27,  31,  32,  36,  41,  43,  44,  45,  47,  48,  49,  50,  52,\n",
      "         54,  55,  57,  58,  60,  61,  63,  66,  67,  68,  69,  70,  71,  73,\n",
      "         75,  77,  78,  79,  80,  82,  83,  86,  87,  90,  95,  96,  98,  99,\n",
      "        101, 102, 104, 106, 109, 111, 112, 113, 117, 118, 119, 120, 121, 122,\n",
      "        123, 124, 125, 130, 134, 135, 140, 143, 144, 145, 147, 148, 151, 153,\n",
      "        156, 157, 158, 160, 162, 165, 172, 173, 174, 176, 178, 180, 181, 182,\n",
      "        186, 191, 193, 195, 197, 199, 201, 204, 206, 207, 211, 212, 213, 215,\n",
      "        216, 217, 218, 220, 223, 224, 226, 227, 228, 229, 231, 232, 234, 236,\n",
      "        237, 238, 240, 241, 244, 246, 247, 248, 250, 251, 255])  (len = 137)\n",
      "passing Cout = tensor([  0,   1,   2,   4,   5,   7,   8,  13,  16,  19,  22,  26,  28,  29,\n",
      "         30,  33,  34,  35,  37,  38,  39,  40,  42,  46,  51,  53,  56,  59,\n",
      "         62,  64,  65,  72,  74,  76,  81,  84,  85,  88,  89,  91,  92,  93,\n",
      "         94,  97, 100, 103, 105, 107, 108, 110, 114, 115, 116, 126, 127, 128,\n",
      "        129, 131, 132, 133, 136, 137, 138, 139, 141, 142, 146, 149, 150, 152,\n",
      "        154, 155, 159, 161, 163, 164, 166, 167, 168, 169, 170, 171, 175, 177,\n",
      "        179, 183, 184, 185, 187, 188, 189, 190, 192, 194, 196, 198, 200, 202,\n",
      "        203, 205, 208, 209, 210, 214, 219, 221, 222, 225, 230, 233, 235, 239,\n",
      "        242, 243, 245, 249, 252, 253, 254])  (len = 119)\n",
      "\n",
      "Executing module 70: relu_1\n",
      "\tExecuting on machine 0\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 1\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 2\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 3\n",
      "\t\t-Applying ReLU\n",
      "Finished execution of layer 70\n",
      "Max diff:\n",
      "tensor([8.8818e-16], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([  6,  15,  20,  21,  27,  31,  32,  36,  41,  43,  45,  48,  50,  54,\n",
      "         61,  73,  75,  79,  96,  99, 101, 102, 109, 111, 112, 113, 120, 122,\n",
      "        123, 124, 147, 176, 182, 193, 195, 197, 204, 206, 211, 212, 215, 217,\n",
      "        227, 228, 231, 236, 244, 246, 248, 251])  (len = 50)\n",
      "passing Cout = tensor([  0,   2,   4,   5,  33,  40,  53,  59,  74,  81,  88,  89,  92,  93,\n",
      "        103, 105, 107, 114, 115, 126, 128, 139, 141, 150, 161, 164, 166, 168,\n",
      "        171, 175, 185, 187, 190, 192, 196, 198, 200, 202, 210, 214, 230, 242,\n",
      "        252, 254])  (len = 44)\n",
      "\n",
      "Executing module 71: linear2\n",
      "\tExecuting on machine 0\n",
      "current layer type: <class 'torch.nn.modules.linear.Linear'>\n",
      "\t\t-Splitting linear layer 71\n",
      "\t\t Output tensor shape : torch.Size([1, 128])\n",
      "\t\t sending C_out tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]) to machine 0\n",
      "\t\t sending C_out tensor([32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "        51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 63]) to machine 1\n",
      "\t\t sending C_out tensor([64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81,\n",
      "        82, 83, 84, 85, 86, 87, 88, 89, 91, 93, 94, 95]) to machine 2\n",
      "\t\t sending C_out tensor([ 96,  97,  98, 100, 101, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127]) to machine 3\n",
      "\tExecuting on machine 1\n",
      "current layer type: <class 'torch.nn.modules.linear.Linear'>\n",
      "\t\t-Splitting linear layer 71\n",
      "\t\t Output tensor shape : torch.Size([1, 128])\n",
      "\t\t sending C_out tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8, 10, 11, 12, 13, 14, 15, 16, 18, 19,\n",
      "        20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]) to machine 0\n",
      "\t\t sending C_out tensor([32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49,\n",
      "        50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]) to machine 1\n",
      "\t\t sending C_out tensor([64, 66, 67, 68, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83,\n",
      "        84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95]) to machine 2\n",
      "\t\t sending C_out tensor([ 96,  97,  98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 110,\n",
      "        111, 112, 113, 114, 115, 117, 118, 119, 120, 121, 122, 124, 125, 126]) to machine 3\n",
      "\tExecuting on machine 2\n",
      "current layer type: <class 'torch.nn.modules.linear.Linear'>\n",
      "\t\t-Splitting linear layer 71\n",
      "\t\t Output tensor shape : torch.Size([1, 128])\n",
      "\t\t sending C_out tensor([ 0,  1,  3,  4,  5,  6,  8,  9, 10, 13, 14, 15, 16, 18, 20, 21, 22, 23,\n",
      "        24, 25, 26, 27, 28, 29, 30]) to machine 0\n",
      "\t\t sending C_out tensor([32, 34, 35, 36, 37, 38, 40, 41, 43, 44, 46, 47, 48, 49, 50, 51, 52, 53,\n",
      "        54, 55, 56, 58, 59, 61]) to machine 1\n",
      "\t\t sending C_out tensor([64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81,\n",
      "        82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95]) to machine 2\n",
      "\t\t sending C_out tensor([ 96,  97,  98,  99, 100, 102, 103, 104, 105, 106, 107, 110, 112, 114,\n",
      "        115, 116, 118, 119, 120, 121, 122, 123, 124, 126]) to machine 3\n",
      "\tExecuting on machine 3\n",
      "current layer type: <class 'torch.nn.modules.linear.Linear'>\n",
      "\t\t-Splitting linear layer 71\n",
      "\t\t Output tensor shape : torch.Size([1, 128])\n",
      "\t\t sending C_out tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 16, 17, 18,\n",
      "        19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 30, 31]) to machine 0\n",
      "\t\t sending C_out tensor([32, 33, 34, 35, 36, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "        51, 52, 53, 54, 55, 56, 58, 59, 60, 61, 62, 63]) to machine 1\n",
      "\t\t sending C_out tensor([64, 65, 66, 67, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 80, 81, 82, 83,\n",
      "        84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95]) to machine 2\n",
      "\t\t sending C_out tensor([ 96,  97,  98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109,\n",
      "        110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123,\n",
      "        124, 125, 126, 127]) to machine 3\n",
      "Finished execution of layer 71\n",
      "Max diff:\n",
      "tensor([8.8818e-16], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([  0,   3,   4,   5,   7,   8,   9,  10,  11,  12,  13,  15,  16,  19,\n",
      "         20,  21,  22,  23,  24,  25,  28,  30,  31,  32,  33,  34,  35,  38,\n",
      "         39,  40,  42,  43,  44,  45,  47,  48,  49,  50,  51,  52,  53,  54,\n",
      "         55,  56,  57,  62,  63,  64,  65,  66,  68,  69,  73,  74,  75,  76,\n",
      "         77,  78,  80,  85,  87,  88,  91,  92,  93,  95,  96,  98,  99, 100,\n",
      "        101, 102, 105, 106, 107, 108, 109, 110, 113, 116, 117, 118, 119, 122,\n",
      "        123, 125, 126, 127])  (len = 88)\n",
      "passing Cout = tensor([  1,   2,   6,  14,  17,  18,  26,  27,  29,  36,  37,  41,  46,  58,\n",
      "         59,  60,  61,  67,  70,  71,  72,  79,  81,  82,  83,  84,  86,  89,\n",
      "         90,  94,  97, 103, 104, 111, 112, 114, 115, 120, 121, 124])  (len = 40)\n",
      "\n",
      "Executing module 72: relu_2\n",
      "\tExecuting on machine 0\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 1\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 2\n",
      "\t\t-Applying ReLU\n",
      "\tExecuting on machine 3\n",
      "\t\t-Applying ReLU\n",
      "Finished execution of layer 72\n",
      "Max diff:\n",
      "tensor([8.8818e-16], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([  4,   5,   7,   9,  10,  11,  15,  20,  22,  23,  25,  28,  32,  38,\n",
      "         39,  40,  43,  48,  50,  51,  54,  56,  63,  64,  65,  73,  76,  92,\n",
      "         93, 106, 107, 108, 110, 113, 116, 117])  (len = 36)\n",
      "passing Cout = tensor([  6,  14,  17,  27,  29,  60,  61,  67,  70,  71,  81,  82,  83,  86,\n",
      "         94, 112, 124])  (len = 17)\n",
      "\n",
      "Executing module 73: linear\n",
      "\tExecuting on machine 0\n",
      "current layer type: <class 'torch.nn.modules.linear.Linear'>\n",
      "\t\t-Splitting linear layer 73\n",
      "\t\t Output tensor shape : torch.Size([1, 10])\n",
      "\t\t sending C_out tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) to machine 0\n",
      "\tExecuting on machine 1\n",
      "current layer type: <class 'torch.nn.modules.linear.Linear'>\n",
      "\t\t-Splitting linear layer 73\n",
      "\t\t Output tensor shape : torch.Size([1, 10])\n",
      "\t\t sending C_out tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) to machine 0\n",
      "\tExecuting on machine 2\n",
      "current layer type: <class 'torch.nn.modules.linear.Linear'>\n",
      "\t\t-Splitting linear layer 73\n",
      "\t\t Output tensor shape : torch.Size([1, 10])\n",
      "\t\t sending C_out tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) to machine 0\n",
      "\tExecuting on machine 3\n",
      "current layer type: <class 'torch.nn.modules.linear.Linear'>\n",
      "\t\t-Splitting linear layer 73\n",
      "\t\t Output tensor shape : torch.Size([1, 10])\n",
      "\t\t sending C_out tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) to machine 0\n",
      "Finished execution of layer 73\n",
      "\n",
      "\n",
      "############################# FINAL EXECUTION TIME 3.350024461746216 [seconds] #############################\n",
      "\n",
      "\n",
      "Max diff:\n",
      "tensor([6.6613e-16], dtype=torch.float64)\n",
      "\n",
      "\n",
      "failing Cout = tensor([0, 4, 6, 8, 9])  (len = 5)\n",
      "passing Cout = tensor([1, 2, 3, 5, 7])  (len = 5)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "''' Prep Inout for Mock Run version 2 (fx)'''\n",
    "\n",
    "# TODO: reduce size of communicated tensors to only what is necessary \n",
    "# TODO: also check bias for nonzero\n",
    "# TODO: come up with more general scheme to handle residual layers\n",
    "\n",
    "# channel_id == INPUTS\n",
    "# filter_id  == OUTPUTS\n",
    "\n",
    "# try greater precision\n",
    "#torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "#torch.set_default_dtype(torch.float64)\n",
    "\n",
    "# setup input \n",
    "N_batch = 1\n",
    "torch.manual_seed(0)\n",
    "\n",
    "if setup == 'resnet18':\n",
    "    input_tensor = torch.rand(N_batch, 3, 32, 32, dtype=torch.float64, device=torch.device(configs['device'])) # 1k images, 3 channels, 32x32 image (cifar10)\n",
    "    input_tensor = tuple([input_tensor])\n",
    "    input_tensor = torch.cat(input_tensor, dim=1)\n",
    "elif setup == 'escnet':\n",
    "    input_tensor = torch.rand(N_batch, 3, 266, 320, dtype=torch.float64, device=torch.device(configs['device']))\n",
    "    # input_tensor = torch.cat([input_tensor]*5, dim=1)\n",
    "    input_tensor = tuple([input_tensor]*5)\n",
    "    input_tensor = torch.cat(input_tensor, dim=1)\n",
    "elif setup == 'flashnet':\n",
    "    input_tensor = torch.rand(N_batch, 2, 10, 10, dtype=torch.float64, device=torch.device(configs['device']))\n",
    "elif setup == 'wideresnet':\n",
    "    input_tensor = torch.rand(N_batch, 3, 32, 32, dtype=torch.float64, device=torch.device(configs['device']))\n",
    "    input_tensor = tuple([input_tensor])\n",
    "    input_tensor = torch.cat(input_tensor, dim=1)\n",
    "\n",
    "\n",
    "# if configs['data_code'] == 'flash':\n",
    "#     input_shape = [(2, 1),\n",
    "#                 #    (90, 160, 3),\n",
    "#                     (360, 640, 3),\n",
    "#                     (20, 20, 20),]\n",
    "# elif configs['data_code'] == 'esc':\n",
    "#     input_shape = [(3, 266, 320) for _ in range(5)]\n",
    "# else:\n",
    "#     input_shape = [(3, 32, 32)]\n",
    "\n",
    "# input_np = (np.random.uniform(0, 1, (1,)+x) for x in input_shape)\n",
    "# input_tensor = tuple(Variable(torch.FloatTensor(x), requires_grad=False).to(configs[\"device\"]) for x in input_np)\n",
    "# # input_tensor = torch.cat(input_tensor, dim=1)\n",
    "\n",
    "# print(input_tensor.shape)\n",
    "# print(input_tensor[0].shape)\n",
    "# print(len(input_tensor))\n",
    "\n",
    "# print(f'input_tensor: {input_tensor}')\n",
    "\n",
    "model = model.type(torch.float64)\n",
    "\n",
    "# make SplitManagers for split model execution \n",
    "configs['dtype'] = 'float64'\n",
    "split_managers = [SplitManager]*num_machines\n",
    "for i in range(num_machines):\n",
    "    split_managers[i] = SplitManager(configs, i, num_machines, input_tensor)\n",
    "\n",
    "# broadcast input_tensor to different machines\n",
    "# TODO: find a better datastructure for this\n",
    "#input = np.empty((num_machines, num_machines), dtype=torch.Tensor)\n",
    "input = [None]*num_machines\n",
    "input = [input[:] for i in range(num_machines)]\n",
    "for imach in range(num_machines):\n",
    "    input[imach][imach] = input_tensor\n",
    "\n",
    "# print(f'input: {input}')\n",
    "\n",
    "residual_block_start, residual_connection_start, residual_block_end = get_residual_block_indexes(model)\n",
    "\n",
    "print(f'residual_block_start: {residual_block_start}')\n",
    "print(f'residual_connection_start: {residual_connection_start}')\n",
    "print(f'residual_block_end: {residual_block_end}')\n",
    "\n",
    "# put models into eval mode and on device\n",
    "model.eval()\n",
    "model.to(configs['device'])\n",
    "\n",
    "# set index to execute vertical splitting up to \n",
    "exec_to_layer = total_layers_fx-1\n",
    "\n",
    "# get true output at each layer\n",
    "# WARNING: bn layers are initialized differently between model in this notebook and model in split manager. The moving mean and variance fields do not match HOWEVER the weights and biases are the same\n",
    "#horz_output, size_LUT = get_output_at_each_layer(model, input_tensor) \n",
    "horz_output, size_LUT = get_output_at_each_layer(split_managers[0].model, input_tensor)\n",
    "\n",
    "BREAK_LOOP = 0 # break loop when output differs\n",
    "\n",
    "'''\n",
    "    mock run through inference using split models \n",
    "'''\n",
    "\n",
    "# timing\n",
    "split_execution_start_time = time.time()\n",
    "layer_completion_time_stamp = {}\n",
    "layer_execution_duration = {}\n",
    "\n",
    "# make inference \n",
    "with torch.no_grad():\n",
    "    residual_input = {} # use this to keep track of inputs stored in machine memory for residule layers\n",
    "\n",
    "    # iterate through layers 1 module at a time \n",
    "    for imodule in range(exec_to_layer+1):#range(num_total_modules): # 16 <=> layer_1 block \n",
    "\n",
    "        # initialize output for ilayer\n",
    "        #output = np.empty((num_machines, num_machines), dtype=torch.Tensor) # square list indexed as: output[destination/RX machine][origin/TX machine]\n",
    "        # TODO: find a better datastructure for this \n",
    "\n",
    "        output = [None]*num_machines\n",
    "        output = [output[:] for i in range(num_machines)]\n",
    "\n",
    "        # print(f'output: {output}')\n",
    "\n",
    "        # DEBUG\n",
    "        full_input = combine_all_inputs(input, num_machines)\n",
    "\n",
    "        print(f'Executing module {imodule}: {layer_names_fx[imodule]}')\n",
    "\n",
    "        # iterate through each machine (done in parallel later)\n",
    "        for imach in range(num_machines):\n",
    "            print(f'\\tExecuting on machine {imach}')\n",
    "            # print(f'curr_input: {curr_input}')\n",
    "            # print(f'input: {input}')\n",
    "            # collect communication inputs if necessary \n",
    "            if not imodule == 0 and ('conv' in layer_names_fx[imodule-1] or 'linear' in layer_names_fx[imodule-1] or 'shortcut.1' in layer_names_fx[imodule]): # TODO: this is very hacky, needs to be generalized. The issue is ID'ing conv layers in shortcut blocks\n",
    "                curr_input = combine_inputs(input, num_machines, imach)\n",
    "                # print(f'ilk if e girdi, curr_input: {curr_input}')\n",
    "            else:\n",
    "                curr_input = input[imach][imach]\n",
    "                # print(f'ikinci elif e girdi, curr_input: {curr_input}')\n",
    "\n",
    "            # print(f'len curr_input: {len(curr_input)}')\n",
    "            \n",
    "            out_tensor, do_comms = split_managers[imach].execute_split_layer(curr_input, imodule)\n",
    "            # print(f'out_tensor: {out_tensor}')\n",
    "            if not do_comms:\n",
    "                # update output to current machine and continue\n",
    "                if torch.is_tensor(out_tensor):\n",
    "                    # sometimes out_tensor is None\n",
    "                    # input is sent to all machines for 1st layer execution even though not all machines need to compute \n",
    "                    # Output from machine is None in this case TODO: fix where inputs are sent \n",
    "                    output[imach][imach] = out_tensor\n",
    "                    # print(f'output: {output}')\n",
    "                continue\n",
    "            # print(f'out_tensor: {out_tensor}')\n",
    "            # END SplitManager execute split_layer\n",
    "\n",
    "            print(f'\\t\\t Output tensor shape : {out_tensor.shape}')\n",
    "\n",
    "            # debug\n",
    "            nonzero_out_tensor = torch.unique(torch.nonzero(out_tensor, as_tuple=True)[1])\n",
    "\n",
    "            # look at which C_out need to be computed and sent\n",
    "            #nonzero_Cout = torch.unique(torch.nonzero(split_layer.weight, as_tuple=True)[0]) # find nonzero dimensions in output channels\n",
    "            nonzero_Cout = get_nonzero_channels(out_tensor)\n",
    "            # print(f'\\t\\t\\t nonzero_Cout: {nonzero_Cout}')\n",
    "\n",
    "            # prep communications by populating output\n",
    "            out_channel_array = torch.arange(out_tensor.shape[1])\n",
    "            for rx_mach in range(num_machines):\n",
    "                # only add to output if communication is necessary \n",
    "\n",
    "                # Get output channels for current rx machine? TODO: consider removing, this just maps C_out's to machine\n",
    "                #output_channels = torch.tensor(configs['partition'][][rx_mach],\n",
    "                #        device=torch.device(configs['device']))\n",
    "                output_channels = torch.tensor(split_managers[imach].output_channel_map[rx_mach],\n",
    "                        device=torch.device(configs['device']))\n",
    "                \n",
    "                # print(f'\\t\\t\\t output_channels: {output_channels}')\n",
    "\n",
    "                # TODO: is there a faster way to do this? Consider putting larger array 1st... just not sure which one that'd be\n",
    "                nonzero_out_channels = nonzero_Cout[torch.isin(nonzero_Cout, output_channels)]\n",
    "                if nonzero_out_channels.nelement() > 0:\n",
    "                        communication_mask = torch.isin(out_channel_array, nonzero_out_channels)\n",
    "\n",
    "                        # TODO: this is inefficient, redo. Probbably need to send a tensor and some info what output channels are being sent\n",
    "                        tmp_out = torch.zeros(out_tensor.shape, dtype= split_managers[imach].dtype) \n",
    "                        if imodule == total_layers_fx-1 or 'hidden' in layer_names_fx[imodule] or 'linear' in layer_names_fx[imodule]:\n",
    "                                tmp_out[:,communication_mask] = out_tensor[:,communication_mask]\n",
    "                        else:\n",
    "                                tmp_out[:,communication_mask,:,:] = out_tensor[:,communication_mask,:,:]\n",
    "                        output[rx_mach][imach] = tmp_out\n",
    "\n",
    "                        # debug\n",
    "                        print(f'\\t\\t sending C_out {nonzero_out_channels} to machine {rx_mach}')\n",
    "\n",
    "        # send to next layer  \n",
    "        input = output\n",
    "        print(f'Finished execution of layer {imodule}')\n",
    "\n",
    "        # update timing\n",
    "        layer_completion_time_stamp[layer_names_fx[imodule]] = time.time()\n",
    "        if imodule > 0:\n",
    "            layer_execution_duration[layer_names_fx[imodule]] = layer_completion_time_stamp[layer_names_fx[imodule]] - layer_completion_time_stamp[layer_names_fx[imodule-1]] \n",
    "        else:\n",
    "            layer_execution_duration[layer_names_fx[imodule]] = layer_completion_time_stamp[layer_names_fx[imodule]] - split_execution_start_time\n",
    "\n",
    "        # check output at end of each layer to see if fit matches \n",
    "        tmp_output = input \n",
    "        # print(f'tmp_output = {tmp_output}')\n",
    "        need_to_init  = True\n",
    "        for rx_mach in range(num_machines):\n",
    "                for tx_mach in range(num_machines):\n",
    "                        if not tmp_output[rx_mach][tx_mach] == None:\n",
    "                                if need_to_init:\n",
    "                                        vert_output = tmp_output[rx_mach][tx_mach]\n",
    "                                        need_to_init = False\n",
    "                                else:\n",
    "                                        # TODO: += causes assignment issues, switched to x = x+y which might be more more inefficent memory wise ... \n",
    "                                        vert_output = vert_output + tmp_output[rx_mach][tx_mach] \n",
    "                                        #nz_channels = get_nonzero_channels(vert_output)\n",
    "                                        #print(f'({rx_mach},{tx_mach}) {nz_channels}')\n",
    "        \n",
    "        if imodule == total_layers_fx-1:\n",
    "            # apply bias\n",
    "            # TODO: assumes Linear layer is final layer and bias can be handled as final step \n",
    "            vert_output = vert_output + get_current_module(model, imodule).bias\n",
    "            \n",
    "            # final execution time\n",
    "            tot_split_execution_time = time.time() - split_execution_start_time\n",
    "            print(f'\\n\\n############################# FINAL EXECUTION TIME {tot_split_execution_time} [seconds] #############################\\n\\n')\n",
    "\n",
    "        truth_output = horz_output[layer_names_fx[imodule]]\n",
    "        # if 'x' == layer_names_fx[imodule] or '_x' == layer_names_fx[imodule] or 'getitem' == layer_names_fx[imodule] or 'getitem_1' == layer_names_fx[imodule] or 'getitem_2' == layer_names_fx[imodule] or 'getitem_3' == layer_names_fx[imodule] or 'getitem_4' == layer_names_fx[imodule] or 'cat' == layer_names_fx[imodule]:\n",
    "        if 'x' == layer_names_fx[imodule] or '_x' in layer_names_fx[imodule]:    \n",
    "            print(f'Input layer. Skipping comparison')\n",
    "        elif torch.is_tensor(truth_output):\n",
    "            max_diff, max_by_Cout = compare_outputs(vert_output, truth_output)\n",
    "            if max_diff > 0.1:\n",
    "                pass\n",
    "                # BREAK_LOOP = 1 \n",
    "        else:\n",
    "            print(f'Horizontal output is {type(truth_output)}. Skipping comparison')\n",
    "        print()\n",
    "\n",
    "        if BREAK_LOOP:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "#     Conv1 layer test -- should we expect some difference, or exactly 0 in the output when we split the model?\n",
    "# '''\n",
    "\n",
    "# # DIFFERENCE SHOULD BE 0 NOT 1E-7\n",
    "\n",
    "# N_in = 1\n",
    "# split_1 = nn.Conv2d(N_in,\n",
    "#             model.conv1.weight.shape[0], # TODO does this need to be an int? (currently tensor)\n",
    "#             kernel_size= model.conv1.kernel_size,\n",
    "#             stride=model.conv1.stride,\n",
    "#             padding=model.conv1.padding, \n",
    "#             bias=False) # TODO: add bias during input collecting step on next layer \n",
    "# split_1.weight = torch.nn.Parameter(model.conv1.weight.index_select(1, torch.tensor([0])))  \n",
    "# out_split1 = split_1(input_tensor.index_select(1, torch.tensor([0])))\n",
    "\n",
    "# split_2 = split_1\n",
    "# split_2.weight = torch.nn.Parameter(model.conv1.weight.index_select(1, torch.tensor([1])))  \n",
    "# out_split2 = split_2(input_tensor.index_select(1, torch.tensor([1])))\n",
    "\n",
    "# split_3 = split_1\n",
    "# split_3.weight = torch.nn.Parameter(model.conv1.weight.index_select(1, torch.tensor([2])))  \n",
    "# out_split3 = split_3(input_tensor.index_select(1, torch.tensor([2])))\n",
    "\n",
    "# split_out = torch.add(torch.add(out_split1, out_split2), out_split3)\n",
    "# full_out = model.conv1(input_tensor)\n",
    "\n",
    "# diff_output = torch.abs(full_out - split_out)\n",
    "# max_diff = torch.max(diff_output)\n",
    "# max_diff.sci_mode = True\n",
    "# print(max_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''  \n",
    "#     Inspect I/O of single layer\n",
    "# '''\n",
    "\n",
    "# t = torch.ones((1,2,2,2), dtype=torch.float32) # (batch, in channel, H, W)\n",
    "# w = torch.ones((1,2,2,2), dtype=torch.float32) # (out channels, in channels, H, W)\n",
    "# w[0,0,0,0] = 1e-10\n",
    "# w[0,0,0,1] = 1e-10\n",
    "# w[0,1,0,0] = 1e-10\n",
    "\n",
    "# full_conv = torch.nn.Conv2d(2,1,kernel_size=(2,2), bias=False, stride=(1),dtype=torch.float32)\n",
    "# full_conv.weight = torch.nn.Parameter(w)\n",
    "# conv1 = torch.nn.Conv2d(1,1,kernel_size=(2,2), bias=False, stride=1, dtype=torch.float32)\n",
    "# conv1.weight =torch.nn.Parameter( w[:,0:1,:,:])\n",
    "# conv2 = torch.nn.Conv2d(1,1,kernel_size=(2,2), bias=False, stride=1,dtype=torch.float32)\n",
    "# conv2.weight = torch.nn.Parameter(w[:,1:2,:,:])\n",
    "\n",
    "# full_conv.eval()\n",
    "# conv1.eval()\n",
    "# conv2.eval()\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     full_out = full_conv(t)\n",
    "#     split_out =  conv2(t[0,1:2,:,:]) + conv1(t[:,0:1,:,:])\n",
    "\n",
    "# diff = torch.abs(full_out - split_out)\n",
    "\n",
    "# torch.nonzero(diff)\n",
    "# #print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test bn1\n",
    "#estimate = -bn1.running_mean[1]/torch.sqrt(bn1.running_var[1] + bn1.eps)*bn1.weight[1] + bn1.bias[1]\n",
    "#estimate_split = -split_layer.running_mean[1]/torch.sqrt(split_layer.running_var[1] + split_layer.eps)*split_layer.weight[1] + split_layer.bias[1]\n",
    "\n",
    "# running estimates are different \n",
    "#bn1.running_mean[1] - split_layer.running_mean[1]\n",
    "#bn1.running_var[1] - split_layer.running_var[1] \n",
    "#bn1.weight[1] - split_layer.weight[1]\n",
    "#bn1.eps - split_layer.eps\n",
    "#bn1.bias[1] - split_layer.bias[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cap_nb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
